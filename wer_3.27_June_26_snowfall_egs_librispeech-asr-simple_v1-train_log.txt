config parsed
Added key: store_based_barrier_key:1 to store for rank: 0
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
about to create model
================================================================================
Model parameters summary:
================================================================================
* encoder_embed.conv.0.weight:                                              4608
* encoder_embed.conv.0.bias:                                                 512
* encoder_embed.conv.2.weight:                                           2359296
* encoder_embed.conv.2.bias:                                                 512
* encoder_embed.out.weight:                                              4980736
* encoder_embed.out.bias:                                                    512
* encoder.layers.0.self_attn.pos_bias_u:                                     512
* encoder.layers.0.self_attn.pos_bias_v:                                     512
* encoder.layers.0.self_attn.in_proj.weight:                              786432
* encoder.layers.0.self_attn.in_proj.bias:                                  1536
* encoder.layers.0.self_attn.out_proj.weight:                             262144
* encoder.layers.0.self_attn.out_proj.bias:                                  512
* encoder.layers.0.self_attn.linear_pos.weight:                           262144
* encoder.layers.0.feed_forward.0.weight:                                1048576
* encoder.layers.0.feed_forward.0.bias:                                     2048
* encoder.layers.0.feed_forward.3.weight:                                1048576
* encoder.layers.0.feed_forward.3.bias:                                      512
* encoder.layers.0.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.0.feed_forward_macaron.0.bias:                             2048
* encoder.layers.0.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.0.feed_forward_macaron.3.bias:                              512
* encoder.layers.0.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.0.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.0.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.0.conv_module.depthwise_conv.bias:                          512
* encoder.layers.0.conv_module.norm.weight:                                  512
* encoder.layers.0.conv_module.norm.bias:                                    512
* encoder.layers.0.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.0.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.0.norm_ff_macaron.weight:                                   512
* encoder.layers.0.norm_ff_macaron.bias:                                     512
* encoder.layers.0.norm_ff.weight:                                           512
* encoder.layers.0.norm_ff.bias:                                             512
* encoder.layers.0.norm_mha.weight:                                          512
* encoder.layers.0.norm_mha.bias:                                            512
* encoder.layers.0.norm_conv.weight:                                         512
* encoder.layers.0.norm_conv.bias:                                           512
* encoder.layers.0.norm_final.weight:                                        512
* encoder.layers.0.norm_final.bias:                                          512
* encoder.layers.1.self_attn.pos_bias_u:                                     512
* encoder.layers.1.self_attn.pos_bias_v:                                     512
* encoder.layers.1.self_attn.in_proj.weight:                              786432
* encoder.layers.1.self_attn.in_proj.bias:                                  1536
* encoder.layers.1.self_attn.out_proj.weight:                             262144
* encoder.layers.1.self_attn.out_proj.bias:                                  512
* encoder.layers.1.self_attn.linear_pos.weight:                           262144
* encoder.layers.1.feed_forward.0.weight:                                1048576
* encoder.layers.1.feed_forward.0.bias:                                     2048
* encoder.layers.1.feed_forward.3.weight:                                1048576
* encoder.layers.1.feed_forward.3.bias:                                      512
* encoder.layers.1.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.1.feed_forward_macaron.0.bias:                             2048
* encoder.layers.1.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.1.feed_forward_macaron.3.bias:                              512
* encoder.layers.1.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.1.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.1.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.1.conv_module.depthwise_conv.bias:                          512
* encoder.layers.1.conv_module.norm.weight:                                  512
* encoder.layers.1.conv_module.norm.bias:                                    512
* encoder.layers.1.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.1.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.1.norm_ff_macaron.weight:                                   512
* encoder.layers.1.norm_ff_macaron.bias:                                     512
* encoder.layers.1.norm_ff.weight:                                           512
* encoder.layers.1.norm_ff.bias:                                             512
* encoder.layers.1.norm_mha.weight:                                          512
* encoder.layers.1.norm_mha.bias:                                            512
* encoder.layers.1.norm_conv.weight:                                         512
* encoder.layers.1.norm_conv.bias:                                           512
* encoder.layers.1.norm_final.weight:                                        512
* encoder.layers.1.norm_final.bias:                                          512
* encoder.layers.2.self_attn.pos_bias_u:                                     512
* encoder.layers.2.self_attn.pos_bias_v:                                     512
* encoder.layers.2.self_attn.in_proj.weight:                              786432
* encoder.layers.2.self_attn.in_proj.bias:                                  1536
* encoder.layers.2.self_attn.out_proj.weight:                             262144
* encoder.layers.2.self_attn.out_proj.bias:                                  512
* encoder.layers.2.self_attn.linear_pos.weight:                           262144
* encoder.layers.2.feed_forward.0.weight:                                1048576
* encoder.layers.2.feed_forward.0.bias:                                     2048
* encoder.layers.2.feed_forward.3.weight:                                1048576
* encoder.layers.2.feed_forward.3.bias:                                      512
* encoder.layers.2.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.2.feed_forward_macaron.0.bias:                             2048
* encoder.layers.2.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.2.feed_forward_macaron.3.bias:                              512
* encoder.layers.2.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.2.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.2.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.2.conv_module.depthwise_conv.bias:                          512
* encoder.layers.2.conv_module.norm.weight:                                  512
* encoder.layers.2.conv_module.norm.bias:                                    512
* encoder.layers.2.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.2.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.2.norm_ff_macaron.weight:                                   512
* encoder.layers.2.norm_ff_macaron.bias:                                     512
* encoder.layers.2.norm_ff.weight:                                           512
* encoder.layers.2.norm_ff.bias:                                             512
* encoder.layers.2.norm_mha.weight:                                          512
* encoder.layers.2.norm_mha.bias:                                            512
* encoder.layers.2.norm_conv.weight:                                         512
* encoder.layers.2.norm_conv.bias:                                           512
* encoder.layers.2.norm_final.weight:                                        512
* encoder.layers.2.norm_final.bias:                                          512
* encoder.layers.3.self_attn.pos_bias_u:                                     512
* encoder.layers.3.self_attn.pos_bias_v:                                     512
* encoder.layers.3.self_attn.in_proj.weight:                              786432
* encoder.layers.3.self_attn.in_proj.bias:                                  1536
* encoder.layers.3.self_attn.out_proj.weight:                             262144
* encoder.layers.3.self_attn.out_proj.bias:                                  512
* encoder.layers.3.self_attn.linear_pos.weight:                           262144
* encoder.layers.3.feed_forward.0.weight:                                1048576
* encoder.layers.3.feed_forward.0.bias:                                     2048
* encoder.layers.3.feed_forward.3.weight:                                1048576
* encoder.layers.3.feed_forward.3.bias:                                      512
* encoder.layers.3.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.3.feed_forward_macaron.0.bias:                             2048
* encoder.layers.3.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.3.feed_forward_macaron.3.bias:                              512
* encoder.layers.3.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.3.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.3.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.3.conv_module.depthwise_conv.bias:                          512
* encoder.layers.3.conv_module.norm.weight:                                  512
* encoder.layers.3.conv_module.norm.bias:                                    512
* encoder.layers.3.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.3.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.3.norm_ff_macaron.weight:                                   512
* encoder.layers.3.norm_ff_macaron.bias:                                     512
* encoder.layers.3.norm_ff.weight:                                           512
* encoder.layers.3.norm_ff.bias:                                             512
* encoder.layers.3.norm_mha.weight:                                          512
* encoder.layers.3.norm_mha.bias:                                            512
* encoder.layers.3.norm_conv.weight:                                         512
* encoder.layers.3.norm_conv.bias:                                           512
* encoder.layers.3.norm_final.weight:                                        512
* encoder.layers.3.norm_final.bias:                                          512
* encoder.layers.4.self_attn.pos_bias_u:                                     512
* encoder.layers.4.self_attn.pos_bias_v:                                     512
* encoder.layers.4.self_attn.in_proj.weight:                              786432
* encoder.layers.4.self_attn.in_proj.bias:                                  1536
* encoder.layers.4.self_attn.out_proj.weight:                             262144
* encoder.layers.4.self_attn.out_proj.bias:                                  512
* encoder.layers.4.self_attn.linear_pos.weight:                           262144
* encoder.layers.4.feed_forward.0.weight:                                1048576
* encoder.layers.4.feed_forward.0.bias:                                     2048
* encoder.layers.4.feed_forward.3.weight:                                1048576
* encoder.layers.4.feed_forward.3.bias:                                      512
* encoder.layers.4.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.4.feed_forward_macaron.0.bias:                             2048
* encoder.layers.4.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.4.feed_forward_macaron.3.bias:                              512
* encoder.layers.4.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.4.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.4.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.4.conv_module.depthwise_conv.bias:                          512
* encoder.layers.4.conv_module.norm.weight:                                  512
* encoder.layers.4.conv_module.norm.bias:                                    512
* encoder.layers.4.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.4.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.4.norm_ff_macaron.weight:                                   512
* encoder.layers.4.norm_ff_macaron.bias:                                     512
* encoder.layers.4.norm_ff.weight:                                           512
* encoder.layers.4.norm_ff.bias:                                             512
* encoder.layers.4.norm_mha.weight:                                          512
* encoder.layers.4.norm_mha.bias:                                            512
* encoder.layers.4.norm_conv.weight:                                         512
* encoder.layers.4.norm_conv.bias:                                           512
* encoder.layers.4.norm_final.weight:                                        512
* encoder.layers.4.norm_final.bias:                                          512
* encoder.layers.5.self_attn.pos_bias_u:                                     512
* encoder.layers.5.self_attn.pos_bias_v:                                     512
* encoder.layers.5.self_attn.in_proj.weight:                              786432
* encoder.layers.5.self_attn.in_proj.bias:                                  1536
* encoder.layers.5.self_attn.out_proj.weight:                             262144
* encoder.layers.5.self_attn.out_proj.bias:                                  512
* encoder.layers.5.self_attn.linear_pos.weight:                           262144
* encoder.layers.5.feed_forward.0.weight:                                1048576
* encoder.layers.5.feed_forward.0.bias:                                     2048
* encoder.layers.5.feed_forward.3.weight:                                1048576
* encoder.layers.5.feed_forward.3.bias:                                      512
* encoder.layers.5.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.5.feed_forward_macaron.0.bias:                             2048
* encoder.layers.5.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.5.feed_forward_macaron.3.bias:                              512
* encoder.layers.5.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.5.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.5.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.5.conv_module.depthwise_conv.bias:                          512
* encoder.layers.5.conv_module.norm.weight:                                  512
* encoder.layers.5.conv_module.norm.bias:                                    512
* encoder.layers.5.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.5.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.5.norm_ff_macaron.weight:                                   512
* encoder.layers.5.norm_ff_macaron.bias:                                     512
* encoder.layers.5.norm_ff.weight:                                           512
* encoder.layers.5.norm_ff.bias:                                             512
* encoder.layers.5.norm_mha.weight:                                          512
* encoder.layers.5.norm_mha.bias:                                            512
* encoder.layers.5.norm_conv.weight:                                         512
* encoder.layers.5.norm_conv.bias:                                           512
* encoder.layers.5.norm_final.weight:                                        512
* encoder.layers.5.norm_final.bias:                                          512
* encoder.layers.6.self_attn.pos_bias_u:                                     512
* encoder.layers.6.self_attn.pos_bias_v:                                     512
* encoder.layers.6.self_attn.in_proj.weight:                              786432
* encoder.layers.6.self_attn.in_proj.bias:                                  1536
* encoder.layers.6.self_attn.out_proj.weight:                             262144
* encoder.layers.6.self_attn.out_proj.bias:                                  512
* encoder.layers.6.self_attn.linear_pos.weight:                           262144
* encoder.layers.6.feed_forward.0.weight:                                1048576
* encoder.layers.6.feed_forward.0.bias:                                     2048
* encoder.layers.6.feed_forward.3.weight:                                1048576
* encoder.layers.6.feed_forward.3.bias:                                      512
* encoder.layers.6.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.6.feed_forward_macaron.0.bias:                             2048
* encoder.layers.6.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.6.feed_forward_macaron.3.bias:                              512
* encoder.layers.6.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.6.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.6.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.6.conv_module.depthwise_conv.bias:                          512
* encoder.layers.6.conv_module.norm.weight:                                  512
* encoder.layers.6.conv_module.norm.bias:                                    512
* encoder.layers.6.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.6.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.6.norm_ff_macaron.weight:                                   512
* encoder.layers.6.norm_ff_macaron.bias:                                     512
* encoder.layers.6.norm_ff.weight:                                           512
* encoder.layers.6.norm_ff.bias:                                             512
* encoder.layers.6.norm_mha.weight:                                          512
* encoder.layers.6.norm_mha.bias:                                            512
* encoder.layers.6.norm_conv.weight:                                         512
* encoder.layers.6.norm_conv.bias:                                           512
* encoder.layers.6.norm_final.weight:                                        512
* encoder.layers.6.norm_final.bias:                                          512
* encoder.layers.7.self_attn.pos_bias_u:                                     512
* encoder.layers.7.self_attn.pos_bias_v:                                     512
* encoder.layers.7.self_attn.in_proj.weight:                              786432
* encoder.layers.7.self_attn.in_proj.bias:                                  1536
* encoder.layers.7.self_attn.out_proj.weight:                             262144
* encoder.layers.7.self_attn.out_proj.bias:                                  512
* encoder.layers.7.self_attn.linear_pos.weight:                           262144
* encoder.layers.7.feed_forward.0.weight:                                1048576
* encoder.layers.7.feed_forward.0.bias:                                     2048
* encoder.layers.7.feed_forward.3.weight:                                1048576
* encoder.layers.7.feed_forward.3.bias:                                      512
* encoder.layers.7.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.7.feed_forward_macaron.0.bias:                             2048
* encoder.layers.7.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.7.feed_forward_macaron.3.bias:                              512
* encoder.layers.7.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.7.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.7.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.7.conv_module.depthwise_conv.bias:                          512
* encoder.layers.7.conv_module.norm.weight:                                  512
* encoder.layers.7.conv_module.norm.bias:                                    512
* encoder.layers.7.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.7.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.7.norm_ff_macaron.weight:                                   512
* encoder.layers.7.norm_ff_macaron.bias:                                     512
* encoder.layers.7.norm_ff.weight:                                           512
* encoder.layers.7.norm_ff.bias:                                             512
* encoder.layers.7.norm_mha.weight:                                          512
* encoder.layers.7.norm_mha.bias:                                            512
* encoder.layers.7.norm_conv.weight:                                         512
* encoder.layers.7.norm_conv.bias:                                           512
* encoder.layers.7.norm_final.weight:                                        512
* encoder.layers.7.norm_final.bias:                                          512
* encoder.layers.8.self_attn.pos_bias_u:                                     512
* encoder.layers.8.self_attn.pos_bias_v:                                     512
* encoder.layers.8.self_attn.in_proj.weight:                              786432
* encoder.layers.8.self_attn.in_proj.bias:                                  1536
* encoder.layers.8.self_attn.out_proj.weight:                             262144
* encoder.layers.8.self_attn.out_proj.bias:                                  512
* encoder.layers.8.self_attn.linear_pos.weight:                           262144
* encoder.layers.8.feed_forward.0.weight:                                1048576
* encoder.layers.8.feed_forward.0.bias:                                     2048
* encoder.layers.8.feed_forward.3.weight:                                1048576
* encoder.layers.8.feed_forward.3.bias:                                      512
* encoder.layers.8.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.8.feed_forward_macaron.0.bias:                             2048
* encoder.layers.8.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.8.feed_forward_macaron.3.bias:                              512
* encoder.layers.8.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.8.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.8.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.8.conv_module.depthwise_conv.bias:                          512
* encoder.layers.8.conv_module.norm.weight:                                  512
* encoder.layers.8.conv_module.norm.bias:                                    512
* encoder.layers.8.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.8.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.8.norm_ff_macaron.weight:                                   512
* encoder.layers.8.norm_ff_macaron.bias:                                     512
* encoder.layers.8.norm_ff.weight:                                           512
* encoder.layers.8.norm_ff.bias:                                             512
* encoder.layers.8.norm_mha.weight:                                          512
* encoder.layers.8.norm_mha.bias:                                            512Added key: store_based_barrier_key:1 to store for rank: 4
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 6
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 7
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 2
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 1
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 3
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader
Added key: store_based_barrier_key:1 to store for rank: 5
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using BucketingSampler.
About to create train dataloader
About to get dev cuts
About to get dev cuts
About to create dev dataset
About to create dev dataloader

* encoder.layers.8.norm_conv.weight:                                         512
* encoder.layers.8.norm_conv.bias:                                           512
* encoder.layers.8.norm_final.weight:                                        512
* encoder.layers.8.norm_final.bias:                                          512
* encoder.layers.9.self_attn.pos_bias_u:                                     512
* encoder.layers.9.self_attn.pos_bias_v:                                     512
* encoder.layers.9.self_attn.in_proj.weight:                              786432
* encoder.layers.9.self_attn.in_proj.bias:                                  1536
* encoder.layers.9.self_attn.out_proj.weight:                             262144
* encoder.layers.9.self_attn.out_proj.bias:                                  512
* encoder.layers.9.self_attn.linear_pos.weight:                           262144
* encoder.layers.9.feed_forward.0.weight:                                1048576
* encoder.layers.9.feed_forward.0.bias:                                     2048
* encoder.layers.9.feed_forward.3.weight:                                1048576
* encoder.layers.9.feed_forward.3.bias:                                      512
* encoder.layers.9.feed_forward_macaron.0.weight:                        1048576
* encoder.layers.9.feed_forward_macaron.0.bias:                             2048
* encoder.layers.9.feed_forward_macaron.3.weight:                        1048576
* encoder.layers.9.feed_forward_macaron.3.bias:                              512
* encoder.layers.9.conv_module.pointwise_conv1.weight:                    524288
* encoder.layers.9.conv_module.pointwise_conv1.bias:                        1024
* encoder.layers.9.conv_module.depthwise_conv.weight:                      15872
* encoder.layers.9.conv_module.depthwise_conv.bias:                          512
* encoder.layers.9.conv_module.norm.weight:                                  512
* encoder.layers.9.conv_module.norm.bias:                                    512
* encoder.layers.9.conv_module.pointwise_conv2.weight:                    262144
* encoder.layers.9.conv_module.pointwise_conv2.bias:                         512
* encoder.layers.9.norm_ff_macaron.weight:                                   512
* encoder.layers.9.norm_ff_macaron.bias:                                     512
* encoder.layers.9.norm_ff.weight:                                           512
* encoder.layers.9.norm_ff.bias:                                             512
* encoder.layers.9.norm_mha.weight:                                          512
* encoder.layers.9.norm_mha.bias:                                            512
* encoder.layers.9.norm_conv.weight:                                         512
* encoder.layers.9.norm_conv.bias:                                           512
* encoder.layers.9.norm_final.weight:                                        512
* encoder.layers.9.norm_final.bias:                                          512
* encoder.layers.10.self_attn.pos_bias_u:                                    512
* encoder.layers.10.self_attn.pos_bias_v:                                    512
* encoder.layers.10.self_attn.in_proj.weight:                             786432
* encoder.layers.10.self_attn.in_proj.bias:                                 1536
* encoder.layers.10.self_attn.out_proj.weight:                            262144
* encoder.layers.10.self_attn.out_proj.bias:                                 512
* encoder.layers.10.self_attn.linear_pos.weight:                          262144
* encoder.layers.10.feed_forward.0.weight:                               1048576
* encoder.layers.10.feed_forward.0.bias:                                    2048
* encoder.layers.10.feed_forward.3.weight:                               1048576
* encoder.layers.10.feed_forward.3.bias:                                     512
* encoder.layers.10.feed_forward_macaron.0.weight:                       1048576
* encoder.layers.10.feed_forward_macaron.0.bias:                            2048
* encoder.layers.10.feed_forward_macaron.3.weight:                       1048576
* encoder.layers.10.feed_forward_macaron.3.bias:                             512
* encoder.layers.10.conv_module.pointwise_conv1.weight:                   524288
* encoder.layers.10.conv_module.pointwise_conv1.bias:                       1024
* encoder.layers.10.conv_module.depthwise_conv.weight:                     15872
* encoder.layers.10.conv_module.depthwise_conv.bias:                         512
* encoder.layers.10.conv_module.norm.weight:                                 512
* encoder.layers.10.conv_module.norm.bias:                                   512
* encoder.layers.10.conv_module.pointwise_conv2.weight:                   262144
* encoder.layers.10.conv_module.pointwise_conv2.bias:                        512
* encoder.layers.10.norm_ff_macaron.weight:                                  512
* encoder.layers.10.norm_ff_macaron.bias:                                    512
* encoder.layers.10.norm_ff.weight:                                          512
* encoder.layers.10.norm_ff.bias:                                            512
* encoder.layers.10.norm_mha.weight:                                         512
* encoder.layers.10.norm_mha.bias:                                           512
* encoder.layers.10.norm_conv.weight:                                        512
* encoder.layers.10.norm_conv.bias:                                          512
* encoder.layers.10.norm_final.weight:                                       512
* encoder.layers.10.norm_final.bias:                                         512
* encoder.layers.11.self_attn.pos_bias_u:                                    512
* encoder.layers.11.self_attn.pos_bias_v:                                    512
* encoder.layers.11.self_attn.in_proj.weight:                             786432
* encoder.layers.11.self_attn.in_proj.bias:                                 1536
* encoder.layers.11.self_attn.out_proj.weight:                            262144
* encoder.layers.11.self_attn.out_proj.bias:                                 512
* encoder.layers.11.self_attn.linear_pos.weight:                          262144
* encoder.layers.11.feed_forward.0.weight:                               1048576
* encoder.layers.11.feed_forward.0.bias:                                    2048
* encoder.layers.11.feed_forward.3.weight:                               1048576
* encoder.layers.11.feed_forward.3.bias:                                     512
* encoder.layers.11.feed_forward_macaron.0.weight:                       1048576
* encoder.layers.11.feed_forward_macaron.0.bias:                            2048
* encoder.layers.11.feed_forward_macaron.3.weight:                       1048576
* encoder.layers.11.feed_forward_macaron.3.bias:                             512
* encoder.layers.11.conv_module.pointwise_conv1.weight:                   524288
* encoder.layers.11.conv_module.pointwise_conv1.bias:                       1024
* encoder.layers.11.conv_module.depthwise_conv.weight:                     15872
* encoder.layers.11.conv_module.depthwise_conv.bias:                         512
* encoder.layers.11.conv_module.norm.weight:                                 512
* encoder.layers.11.conv_module.norm.bias:                                   512
* encoder.layers.11.conv_module.pointwise_conv2.weight:                   262144
* encoder.layers.11.conv_module.pointwise_conv2.bias:                        512
* encoder.layers.11.norm_ff_macaron.weight:                                  512
* encoder.layers.11.norm_ff_macaron.bias:                                    512
* encoder.layers.11.norm_ff.weight:                                          512
* encoder.layers.11.norm_ff.bias:                                            512
* encoder.layers.11.norm_mha.weight:                                         512
* encoder.layers.11.norm_mha.bias:                                           512
* encoder.layers.11.norm_conv.weight:                                        512
* encoder.layers.11.norm_conv.bias:                                          512
* encoder.layers.11.norm_final.weight:                                       512
* encoder.layers.11.norm_final.bias:                                         512
* encoder_output_layer.1.weight:                                         2560000
* encoder_output_layer.1.bias:                                              5000
* decoder_embed.weight:                                                  2560000
* decoder.layers.0.self_attn.in_proj_weight:                              786432
* decoder.layers.0.self_attn.in_proj_bias:                                  1536
* decoder.layers.0.self_attn.out_proj.weight:                             262144
* decoder.layers.0.self_attn.out_proj.bias:                                  512
* decoder.layers.0.src_attn.in_proj_weight:                               786432
* decoder.layers.0.src_attn.in_proj_bias:                                   1536
* decoder.layers.0.src_attn.out_proj.weight:                              262144
* decoder.layers.0.src_attn.out_proj.bias:                                   512
* decoder.layers.0.linear1.weight:                                       1048576
* decoder.layers.0.linear1.bias:                                            2048
* decoder.layers.0.linear2.weight:                                       1048576
* decoder.layers.0.linear2.bias:                                             512
* decoder.layers.0.norm1.weight:                                             512
* decoder.layers.0.norm1.bias:                                               512
* decoder.layers.0.norm2.weight:                                             512
* decoder.layers.0.norm2.bias:                                               512
* decoder.layers.0.norm3.weight:                                             512
* decoder.layers.0.norm3.bias:                                               512
* decoder.layers.1.self_attn.in_proj_weight:                              786432
* decoder.layers.1.self_attn.in_proj_bias:                                  1536
* decoder.layers.1.self_attn.out_proj.weight:                             262144
* decoder.layers.1.self_attn.out_proj.bias:                                  512
* decoder.layers.1.src_attn.in_proj_weight:                               786432
* decoder.layers.1.src_attn.in_proj_bias:                                   1536
* decoder.layers.1.src_attn.out_proj.weight:                              262144
* decoder.layers.1.src_attn.out_proj.bias:                                   512
* decoder.layers.1.linear1.weight:                                       1048576
* decoder.layers.1.linear1.bias:                                            2048
* decoder.layers.1.linear2.weight:                                       1048576
* decoder.layers.1.linear2.bias:                                             512
* decoder.layers.1.norm1.weight:                                             512
* decoder.layers.1.norm1.bias:                                               512
* decoder.layers.1.norm2.weight:                                             512
* decoder.layers.1.norm2.bias:                                               512
* decoder.layers.1.norm3.weight:                                             512
* decoder.layers.1.norm3.bias:                                               512
* decoder.layers.2.self_attn.in_proj_weight:                              786432
* decoder.layers.2.self_attn.in_proj_bias:                                  1536
* decoder.layers.2.self_attn.out_proj.weight:                             262144
* decoder.layers.2.self_attn.out_proj.bias:                                  512
* decoder.layers.2.src_attn.in_proj_weight:                               786432
* decoder.layers.2.src_attn.in_proj_bias:                                   1536
* decoder.layers.2.src_attn.out_proj.weight:                              262144
* decoder.layers.2.src_attn.out_proj.bias:                                   512
* decoder.layers.2.linear1.weight:                                       1048576
* decoder.layers.2.linear1.bias:                                            2048
* decoder.layers.2.linear2.weight:                                       1048576
* decoder.layers.2.linear2.bias:                                             512
* decoder.layers.2.norm1.weight:                                             512
* decoder.layers.2.norm1.bias:                                               512
* decoder.layers.2.norm2.weight:                                             512
* decoder.layers.2.norm2.bias:                                               512
* decoder.layers.2.norm3.weight:                                             512
* decoder.layers.2.norm3.bias:                                               512
* decoder.layers.3.self_attn.in_proj_weight:                              786432
* decoder.layers.3.self_attn.in_proj_bias:                                  1536
* decoder.layers.3.self_attn.out_proj.weight:                             262144
* decoder.layers.3.self_attn.out_proj.bias:                                  512
* decoder.layers.3.src_attn.in_proj_weight:                               786432
* decoder.layers.3.src_attn.in_proj_bias:                                   1536
* decoder.layers.3.src_attn.out_proj.weight:                              262144
* decoder.layers.3.src_attn.out_proj.bias:                                   512
* decoder.layers.3.linear1.weight:                                       1048576
* decoder.layers.3.linear1.bias:                                            2048
* decoder.layers.3.linear2.weight:                                       1048576
* decoder.layers.3.linear2.bias:                                             512
* decoder.layers.3.norm1.weight:                                             512
* decoder.layers.3.norm1.bias:                                               512
* decoder.layers.3.norm2.weight:                                             512
* decoder.layers.3.norm2.bias:                                               512
* decoder.layers.3.norm3.weight:                                             512
* decoder.layers.3.norm3.bias:                                               512
* decoder.layers.4.self_attn.in_proj_weight:                              786432
* decoder.layers.4.self_attn.in_proj_bias:                                  1536
* decoder.layers.4.self_attn.out_proj.weight:                             262144
* decoder.layers.4.self_attn.out_proj.bias:                                  512
* decoder.layers.4.src_attn.in_proj_weight:                               786432
* decoder.layers.4.src_attn.in_proj_bias:                                   1536
* decoder.layers.4.src_attn.out_proj.weight:                              262144
* decoder.layers.4.src_attn.out_proj.bias:                                   512
* decoder.layers.4.linear1.weight:                                       1048576
* decoder.layers.4.linear1.bias:                                            2048
* decoder.layers.4.linear2.weight:                                       1048576
* decoder.layers.4.linear2.bias:                                             512
* decoder.layers.4.norm1.weight:                                             512
* decoder.layers.4.norm1.bias:                                               512
* decoder.layers.4.norm2.weight:                                             512
* decoder.layers.4.norm2.bias:                                               512
* decoder.layers.4.norm3.weight:                                             512
* decoder.layers.4.norm3.bias:                                               512
* decoder.layers.5.self_attn.in_proj_weight:                              786432
* decoder.layers.5.self_attn.in_proj_bias:                                  1536
* decoder.layers.5.self_attn.out_proj.weight:                             262144
* decoder.layers.5.self_attn.out_proj.bias:                                  512
* decoder.layers.5.src_attn.in_proj_weight:                               786432
* decoder.layers.5.src_attn.in_proj_bias:                                   1536
* decoder.layers.5.src_attn.out_proj.weight:                              262144
* decoder.layers.5.src_attn.out_proj.bias:                                   512
* decoder.layers.5.linear1.weight:                                       1048576
* decoder.layers.5.linear1.bias:                                            2048
* decoder.layers.5.linear2.weight:                                       1048576
* decoder.layers.5.linear2.bias:                                             512
* decoder.layers.5.norm1.weight:                                             512
* decoder.layers.5.norm1.bias:                                               512
* decoder.layers.5.norm2.weight:                                             512
* decoder.layers.5.norm2.bias:                                               512
* decoder.layers.5.norm3.weight:                                             512
* decoder.layers.5.norm3.bias:                                               512
* decoder.norm.weight:                                                       512
* decoder.norm.bias:                                                         512
* decoder_output_layer.weight:                                           2560000
* decoder_output_layer.bias:                                                5000
* after_norm.weight:                                                         512
* after_norm.bias:                                                           512
================================================================================
Total: 116146960
================================================================================
epoch 1, learning rate 0
1epoch:train:0-0batch: loss=1.19e+03, loss_att=378.6, loss_ctc=3.083e+03
Reducer buckets have been rebuilt in this iteration.
1epoch:train:0-100batch: loss=511.3, loss_att=232.7, loss_ctc=1.161e+03
1epoch:train:100-200batch: loss=266.7, loss_att=257.9, loss_ctc=287.0
1epoch:train:200-300batch: loss=260.9, loss_att=253.6, loss_ctc=277.9
1epoch:train:300-400batch: loss=195.7, loss_att=187.4, loss_ctc=215.1
1epoch:train:400-500batch: loss=203.8, loss_att=194.9, loss_ctc=224.6
1epoch:train:500-600batch: loss=198.4, loss_att=189.3, loss_ctc=219.7
1epoch:train:600-700batch: loss=186.6, loss_att=177.4, loss_ctc=208.0
1epoch:train:700-800batch: loss=191.5, loss_att=181.5, loss_ctc=214.7
1epoch:train:800-900batch: loss=187.1, loss_att=176.8, loss_ctc=210.9
1epoch:train:900-1000batch: loss=195.1, loss_att=184.0, loss_ctc=220.9
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Validation average objf: -135.030257 over 5505.0 utts
1epoch:train:1000-1100batch: loss=201.1, loss_att=189.7, loss_ctc=227.7
1epoch:train:1100-1200batch: loss=172.5, loss_att=162.9, loss_ctc=195.0
1epoch:train:1200-1300batch: loss=200.2, loss_att=189.6, loss_ctc=225.1
1epoch:train:1300-1400batch: loss=195.8, loss_att=186.4, loss_ctc=217.5
1epoch:train:1400-1500batch: loss=199.7, loss_att=191.8, loss_ctc=218.4
1epoch:train:1500-1600batch: loss=178.4, loss_att=174.0, loss_ctc=188.4
1epoch:train:1600-1700batch: loss=188.4, loss_att=186.6, loss_ctc=192.7
1epoch:train:1700-1800batch: loss=191.0, loss_att=191.5, loss_ctc=189.9
1epoch:train:1800-1900batch: loss=175.5, loss_att=177.0, loss_ctc=172.0
1epoch:train:1900-2000batch: loss=196.4, loss_att=200.0, loss_ctc=188.0
Validation average objf: -110.757312 over 5505.0 utts
1epoch:train:2000-2100batch: loss=191.2, loss_att=197.3, loss_ctc=176.9
1epoch:train:2100-2200batch: loss=188.7, loss_att=195.7, loss_ctc=172.3
1epoch:train:2200-2300batch: loss=191.6, loss_att=200.2, loss_ctc=171.6
1epoch:train:2300-2400batch: loss=185.6, loss_att=195.7, loss_ctc=162.0
1epoch:train:2400-2500batch: loss=179.8, loss_att=189.7, loss_ctc=156.8
1epoch:train:2500-2600batch: loss=170.8, loss_att=181.8, loss_ctc=145.1
1epoch:train:2600-2700batch: loss=177.0, loss_att=189.2, loss_ctc=148.4
1epoch:train:2700-2800batch: loss=186.0, loss_att=200.5, loss_ctc=152.4
1epoch:train:2800-2900batch: loss=178.3, loss_att=193.3, loss_ctc=143.4
1epoch:train:2900-3000batch: loss=179.5, loss_att=195.1, loss_ctc=143.0
Validation average objf: -96.484378 over 5505.0 utts
1epoch:train:3000-3100batch: loss=183.8, loss_att=201.0, loss_ctc=143.6
1epoch:train:3100-3200batch: loss=176.2, loss_att=193.5, loss_ctc=135.8
1epoch:train:3200-3300batch: loss=191.9, loss_att=211.5, loss_ctc=146.0
1epoch:train:3300-3400batch: loss=186.7, loss_att=207.6, loss_ctc=138.0
1epoch:train:3400-3500batch: loss=182.9, loss_att=203.0, loss_ctc=136.2
1epoch:train:3500-3600batch: loss=179.6, loss_att=200.3, loss_ctc=131.3
1epoch:train:3600-3700batch: loss=170.3, loss_att=190.1, loss_ctc=124.1
1epoch:train:3700-3800batch: loss=166.4, loss_att=184.5, loss_ctc=124.3
1epoch:train:3800-3900batch: loss=160.1, loss_att=177.1, loss_ctc=120.5
1epoch:train:3900-4000batch: loss=150.3, loss_att=165.5, loss_ctc=114.9
Validation average objf: -74.520941 over 5505.0 utts
1epoch:train:4000-4100batch: loss=145.8, loss_att=158.6, loss_ctc=116.0
1epoch:train:4100-4200batch: loss=140.7, loss_att=152.4, loss_ctc=113.5
1epoch:train:4200-4300batch: loss=129.7, loss_att=138.9, loss_ctc=108.5
1epoch:train:4300-4400batch: loss=133.6, loss_att=142.2, loss_ctc=113.5
1epoch:train:4400-4500batch: loss=129.1, loss_att=136.2, loss_ctc=112.4
1epoch:train:4500-4600batch: loss=116.8, loss_att=122.2, loss_ctc=104.0
1epoch:train:4600-4700batch: loss=116.1, loss_att=121.6, loss_ctc=103.2
1epoch:train:4700-4800batch: loss=110.3, loss_att=114.2, loss_ctc=101.3
1epoch:train:4800-4900batch: loss=104.6, loss_att=107.6, loss_ctc=97.63
1epoch:train:4900-5000batch: loss=105.7, loss_att=107.9, loss_ctc=100.5
Validation average objf: -59.203298 over 5505.0 utts
1epoch:train:5000-5100batch: loss=101.0, loss_att=102.3, loss_ctc=97.94
1epoch:train:5100-5200batch: loss=98.86, loss_att=99.66, loss_ctc=97.0
1epoch:train:5200-5300batch: loss=95.76, loss_att=96.13, loss_ctc=94.87
1epoch:train:5300-5400batch: loss=94.72, loss_att=94.58, loss_ctc=95.03
1epoch:train:5400-5500batch: loss=91.0, loss_att=90.17, loss_ctc=92.94
1epoch:train:5500-5600batch: loss=88.39, loss_att=87.5, loss_ctc=90.46
1epoch:train:5600-5700batch: loss=87.53, loss_att=85.97, loss_ctc=91.17
1epoch:train:5700-5800batch: loss=87.54, loss_att=85.9, loss_ctc=91.35
1epoch:train:5800-5900batch: loss=83.19, loss_att=81.06, loss_ctc=88.16
1epoch:train:5900-6000batch: loss=83.45, loss_att=81.18, loss_ctc=88.74
Validation average objf: -35.236331 over 5505.0 utts
1epoch:train:6000-6100batch: loss=81.78, loss_att=78.98, loss_ctc=88.32
1epoch:train:6100-6200batch: loss=80.58, loss_att=77.76, loss_ctc=87.18
1epoch:train:6200-6300batch: loss=77.33, loss_att=74.11, loss_ctc=84.84
1epoch:train:6300-6400batch: loss=75.9, loss_att=72.85, loss_ctc=83.04
1epoch:train:6400-6500batch: loss=74.06, loss_att=70.99, loss_ctc=81.23
1epoch:train:6500-6600batch: loss=72.25, loss_att=68.89, loss_ctc=80.08
1epoch:train:6600-6700batch: loss=70.2, loss_att=66.97, loss_ctc=77.75
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/best_model.pt: epoch=1, learning_rate=0, objf=-11.18231639619789, valid_objf=-35.23633060853769
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/best-epoch-info
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-1.pt: epoch=1, learning_rate=0, objf=-11.18231639619789, valid_objf=-35.23633060853769
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-1-info
epoch 2, learning rate 0.000373882710552387
2epoch:train:0-0batch: loss=37.61, loss_att=36.4, loss_ctc=40.41
2epoch:train:0-100batch: loss=48.45, loss_att=45.76, loss_ctc=54.73
2epoch:train:100-200batch: loss=43.69, loss_att=40.89, loss_ctc=50.24
2epoch:train:200-300batch: loss=52.21, loss_att=49.03, loss_ctc=59.63
2epoch:train:300-400batch: loss=48.28, loss_att=45.07, loss_ctc=55.79
2epoch:train:400-500batch: loss=46.85, loss_att=43.74, loss_ctc=54.09
2epoch:train:500-600batch: loss=45.01, loss_att=41.79, loss_ctc=52.52
2epoch:train:600-700batch: loss=45.45, loss_att=41.99, loss_ctc=53.51
2epoch:train:700-800batch: loss=44.92, loss_att=41.51, loss_ctc=52.87
2epoch:train:800-900batch: loss=42.13, loss_att=38.98, loss_ctc=49.49
2epoch:train:900-1000batch: loss=46.13, loss_att=42.61, loss_ctc=54.36
Validation average objf: -22.075219 over 5505.0 utts
2epoch:train:1000-1100batch: loss=48.45, loss_att=44.72, loss_ctc=57.15
2epoch:train:1100-1200batch: loss=45.69, loss_att=42.12, loss_ctc=54.02
2epoch:train:1200-1300batch: loss=46.52, loss_att=42.78, loss_ctc=55.26
2epoch:train:1300-1400batch: loss=50.69, loss_att=46.55, loss_ctc=60.34
2epoch:train:1400-1500batch: loss=44.41, loss_att=40.66, loss_ctc=53.15
2epoch:train:1500-1600batch: loss=45.93, loss_att=42.2, loss_ctc=54.64
2epoch:train:1600-1700batch: loss=45.27, loss_att=41.44, loss_ctc=54.21
2epoch:train:1700-1800batch: loss=41.59, loss_att=38.05, loss_ctc=49.85
2epoch:train:1800-1900batch: loss=41.1, loss_att=37.23, loss_ctc=50.12
2epoch:train:1900-2000batch: loss=45.02, loss_att=41.25, loss_ctc=53.83
Validation average objf: -19.171187 over 5505.0 utts
2epoch:train:2000-2100batch: loss=42.14, loss_att=38.22, loss_ctc=51.27
2epoch:train:2100-2200batch: loss=45.68, loss_att=41.5, loss_ctc=55.45
2epoch:train:2200-2300batch: loss=43.79, loss_att=39.9, loss_ctc=52.85
2epoch:train:2300-2400batch: loss=47.07, loss_att=42.63, loss_ctc=57.41
2epoch:train:2400-2500batch: loss=39.26, loss_att=35.51, loss_ctc=48.0
2epoch:train:2500-2600batch: loss=40.08, loss_att=36.3, loss_ctc=48.91
2epoch:train:2600-2700batch: loss=41.62, loss_att=37.56, loss_ctc=51.09
2epoch:train:2700-2800batch: loss=43.41, loss_att=39.01, loss_ctc=53.66
2epoch:train:2800-2900batch: loss=43.01, loss_att=38.85, loss_ctc=52.73
2epoch:train:2900-3000batch: loss=43.11, loss_att=38.87, loss_ctc=53.0
Validation average objf: -16.642857 over 5505.0 utts
2epoch:train:3000-3100batch: loss=43.51, loss_att=39.07, loss_ctc=53.86
2epoch:train:3100-3200batch: loss=42.81, loss_att=38.48, loss_ctc=52.89
2epoch:train:3200-3300batch: loss=43.86, loss_att=39.51, loss_ctc=54.0
2epoch:train:3300-3400batch: loss=43.1, loss_att=38.66, loss_ctc=53.47
2epoch:train:3400-3500batch: loss=43.39, loss_att=38.96, loss_ctc=53.73
2epoch:train:3500-3600batch: loss=41.98, loss_att=37.73, loss_ctc=51.91
2epoch:train:3600-3700batch: loss=40.77, loss_att=36.41, loss_ctc=50.94
2epoch:train:3700-3800batch: loss=42.71, loss_att=38.07, loss_ctc=53.54
2epoch:train:3800-3900batch: loss=41.19, loss_att=36.96, loss_ctc=51.06
2epoch:train:3900-4000batch: loss=40.02, loss_att=35.76, loss_ctc=49.95
Validation average objf: -16.055330 over 5505.0 utts
2epoch:train:4000-4100batch: loss=41.8, loss_att=37.36, loss_ctc=52.16
2epoch:train:4100-4200batch: loss=41.31, loss_att=36.91, loss_ctc=51.56
2epoch:train:4200-4300batch: loss=42.19, loss_att=37.74, loss_ctc=52.59
2epoch:train:4300-4400batch: loss=40.87, loss_att=36.44, loss_ctc=51.19
2epoch:train:4400-4500batch: loss=40.99, loss_att=36.46, loss_ctc=51.56
2epoch:train:4500-4600batch: loss=41.73, loss_att=37.18, loss_ctc=52.36
2epoch:train:4600-4700batch: loss=39.95, loss_att=35.44, loss_ctc=50.47
2epoch:train:4700-4800batch: loss=40.79, loss_att=36.28, loss_ctc=51.33
2epoch:train:4800-4900batch: loss=40.42, loss_att=35.97, loss_ctc=50.8
2epoch:train:4900-5000batch: loss=40.59, loss_att=36.18, loss_ctc=50.86
Validation average objf: -15.020107 over 5505.0 utts
2epoch:train:5000-5100batch: loss=39.18, loss_att=34.79, loss_ctc=49.41
2epoch:train:5100-5200batch: loss=41.05, loss_att=36.43, loss_ctc=51.85
2epoch:train:5200-5300batch: loss=40.49, loss_att=35.93, loss_ctc=51.12
2epoch:train:5300-5400batch: loss=39.49, loss_att=35.17, loss_ctc=49.56
2epoch:train:5400-5500batch: loss=39.73, loss_att=35.22, loss_ctc=50.27
2epoch:train:5500-5600batch: loss=41.23, loss_att=36.66, loss_ctc=51.9
2epoch:train:5600-5700batch: loss=40.51, loss_att=35.81, loss_ctc=51.48
2epoch:train:5700-5800batch: loss=40.59, loss_att=36.01, loss_ctc=51.29
2epoch:train:5800-5900batch: loss=40.73, loss_att=36.03, loss_ctc=51.69
2epoch:train:5900-6000batch: loss=38.98, loss_att=34.52, loss_ctc=49.41
Validation average objf: -16.849378 over 5505.0 utts
2epoch:train:6000-6100batch: loss=40.08, loss_att=35.32, loss_ctc=51.19
2epoch:train:6100-6200batch: loss=40.49, loss_att=35.72, loss_ctc=51.62
2epoch:train:6200-6300batch: loss=39.88, loss_att=35.24, loss_ctc=50.73
2epoch:train:6300-6400batch: loss=38.84, loss_att=34.37, loss_ctc=49.27
2epoch:train:6400-6500batch: loss=37.97, loss_att=33.47, loss_ctc=48.47
2epoch:train:6500-6600batch: loss=38.29, loss_att=33.79, loss_ctc=48.77
2epoch:train:6600-6700batch: loss=38.18, loss_att=33.76, loss_ctc=48.49
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-2.pt: epoch=2, learning_rate=0.000373882710552387, objf=-2.910845783741319, valid_objf=-16.84937840599455
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-2-info
epoch 3, learning rate 0.000747765421104774
3epoch:train:0-0batch: loss=33.62, loss_att=30.13, loss_ctc=41.75
3epoch:train:0-100batch: loss=27.84, loss_att=24.49, loss_ctc=35.65
3epoch:train:100-200batch: loss=29.97, loss_att=26.42, loss_ctc=38.25
3epoch:train:200-300batch: loss=30.89, loss_att=27.13, loss_ctc=39.68
3epoch:train:300-400batch: loss=27.46, loss_att=24.12, loss_ctc=35.27
3epoch:train:400-500batch: loss=30.03, loss_att=26.32, loss_ctc=38.7
3epoch:train:500-600batch: loss=29.1, loss_att=25.49, loss_ctc=37.52
3epoch:train:600-700batch: loss=27.64, loss_att=24.19, loss_ctc=35.69
3epoch:train:700-800batch: loss=27.66, loss_att=24.18, loss_ctc=35.78
3epoch:train:800-900batch: loss=28.99, loss_att=25.39, loss_ctc=37.39
3epoch:train:900-1000batch: loss=30.02, loss_att=26.17, loss_ctc=39.0
Validation average objf: -12.474136 over 5505.0 utts
3epoch:train:1000-1100batch: loss=27.79, loss_att=24.43, loss_ctc=35.64
3epoch:train:1100-1200batch: loss=29.01, loss_att=25.43, loss_ctc=37.35
3epoch:train:1200-1300batch: loss=28.92, loss_att=25.21, loss_ctc=37.57
3epoch:train:1300-1400batch: loss=27.86, loss_att=24.42, loss_ctc=35.9
3epoch:train:1400-1500batch: loss=30.67, loss_att=26.77, loss_ctc=39.79
3epoch:train:1500-1600batch: loss=30.89, loss_att=26.99, loss_ctc=39.98
3epoch:train:1600-1700batch: loss=29.86, loss_att=26.13, loss_ctc=38.56
3epoch:train:1700-1800batch: loss=31.36, loss_att=27.3, loss_ctc=40.82
3epoch:train:1800-1900batch: loss=30.59, loss_att=26.73, loss_ctc=39.58
3epoch:train:1900-2000batch: loss=30.66, loss_att=26.77, loss_ctc=39.74
Validation average objf: -12.197414 over 5505.0 utts
3epoch:train:2000-2100batch: loss=28.54, loss_att=24.91, loss_ctc=36.99
3epoch:train:2100-2200batch: loss=30.9, loss_att=27.05, loss_ctc=39.89
3epoch:train:2200-2300batch: loss=29.93, loss_att=26.22, loss_ctc=38.58
3epoch:train:2300-2400batch: loss=32.19, loss_att=28.09, loss_ctc=41.77
3epoch:train:2400-2500batch: loss=31.46, loss_att=27.41, loss_ctc=40.92
3epoch:train:2500-2600batch: loss=32.38, loss_att=28.13, loss_ctc=42.29
3epoch:train:2600-2700batch: loss=32.34, loss_att=28.03, loss_ctc=42.4
3epoch:train:2700-2800batch: loss=31.41, loss_att=27.31, loss_ctc=40.97
3epoch:train:2800-2900batch: loss=29.39, loss_att=25.54, loss_ctc=38.37
3epoch:train:2900-3000batch: loss=32.66, loss_att=28.42, loss_ctc=42.57
Validation average objf: -11.703735 over 5505.0 utts
3epoch:train:3000-3100batch: loss=30.44, loss_att=26.45, loss_ctc=39.76
3epoch:train:3100-3200batch: loss=31.23, loss_att=27.15, loss_ctc=40.73
3epoch:train:3200-3300batch: loss=30.4, loss_att=26.49, loss_ctc=39.53
3epoch:train:3300-3400batch: loss=31.11, loss_att=27.14, loss_ctc=40.38
3epoch:train:3400-3500batch: loss=32.37, loss_att=28.01, loss_ctc=42.54
3epoch:train:3500-3600batch: loss=31.0, loss_att=26.92, loss_ctc=40.51
3epoch:train:3600-3700batch: loss=30.63, loss_att=26.55, loss_ctc=40.16
3epoch:train:3700-3800batch: loss=31.62, loss_att=27.42, loss_ctc=41.4
3epoch:train:3800-3900batch: loss=31.96, loss_att=27.78, loss_ctc=41.72
3epoch:train:3900-4000batch: loss=31.52, loss_att=27.29, loss_ctc=41.39
Validation average objf: -11.479030 over 5505.0 utts
3epoch:train:4000-4100batch: loss=31.67, loss_att=27.51, loss_ctc=41.38
3epoch:train:4100-4200batch: loss=32.23, loss_att=27.96, loss_ctc=42.2
3epoch:train:4200-4300batch: loss=29.59, loss_att=25.76, loss_ctc=38.52
3epoch:train:4300-4400batch: loss=30.3, loss_att=26.32, loss_ctc=39.6
3epoch:train:4400-4500batch: loss=29.64, loss_att=25.64, loss_ctc=38.97
3epoch:train:4500-4600batch: loss=31.73, loss_att=27.45, loss_ctc=41.71
3epoch:train:4600-4700batch: loss=31.02, loss_att=26.87, loss_ctc=40.71
3epoch:train:4700-4800batch: loss=31.18, loss_att=26.9, loss_ctc=41.17
3epoch:train:4800-4900batch: loss=31.59, loss_att=27.35, loss_ctc=41.48
3epoch:train:4900-5000batch: loss=31.68, loss_att=27.46, loss_ctc=41.54
Validation average objf: -11.336823 over 5505.0 utts
3epoch:train:5000-5100batch: loss=31.94, loss_att=27.7, loss_ctc=41.83
3epoch:train:5100-5200batch: loss=30.98, loss_att=26.81, loss_ctc=40.7
3epoch:train:5200-5300batch: loss=31.62, loss_att=27.35, loss_ctc=41.57
3epoch:train:5300-5400batch: loss=31.42, loss_att=27.21, loss_ctc=41.23
3epoch:train:5400-5500batch: loss=31.28, loss_att=27.18, loss_ctc=40.86
3epoch:train:5500-5600batch: loss=30.84, loss_att=26.83, loss_ctc=40.21
3epoch:train:5600-5700batch: loss=32.49, loss_att=28.16, loss_ctc=42.6
3epoch:train:5700-5800batch: loss=29.81, loss_att=25.84, loss_ctc=39.06
3epoch:train:5800-5900batch: loss=31.26, loss_att=26.95, loss_ctc=41.31
3epoch:train:5900-6000batch: loss=31.48, loss_att=27.15, loss_ctc=41.57
Validation average objf: -10.833479 over 5505.0 utts
3epoch:train:6000-6100batch: loss=31.91, loss_att=27.66, loss_ctc=41.8
3epoch:train:6100-6200batch: loss=30.91, loss_att=26.67, loss_ctc=40.81
3epoch:train:6200-6300batch: loss=30.46, loss_att=26.28, loss_ctc=40.24
3epoch:train:6300-6400batch: loss=30.57, loss_att=26.36, loss_ctc=40.4
3epoch:train:6400-6500batch: loss=31.79, loss_att=27.48, loss_ctc=41.86
3epoch:train:6500-6600batch: loss=31.05, loss_att=26.86, loss_ctc=40.83
3epoch:train:6600-6700batch: loss=31.24, loss_att=27.06, loss_ctc=41.02
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-3.pt: epoch=3, learning_rate=0.000747765421104774, objf=-2.062782034632109, valid_objf=-10.833478797683924
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-3-info
epoch 4, learning rate 0.0011217033743744413
4epoch:train:0-0batch: loss=21.1, loss_att=18.97, loss_ctc=26.06
4epoch:train:0-100batch: loss=25.43, loss_att=21.82, loss_ctc=33.86
4epoch:train:100-200batch: loss=24.94, loss_att=21.44, loss_ctc=33.11
4epoch:train:200-300batch: loss=24.32, loss_att=21.0, loss_ctc=32.07
4epoch:train:300-400batch: loss=22.8, loss_att=19.64, loss_ctc=30.15
4epoch:train:400-500batch: loss=22.32, loss_att=19.23, loss_ctc=29.52
4epoch:train:500-600batch: loss=22.35, loss_att=19.23, loss_ctc=29.64
4epoch:train:600-700batch: loss=24.15, loss_att=20.85, loss_ctc=31.87
4epoch:train:700-800batch: loss=21.04, loss_att=18.09, loss_ctc=27.93
4epoch:train:800-900batch: loss=22.19, loss_att=19.03, loss_ctc=29.56
4epoch:train:900-1000batch: loss=22.64, loss_att=19.46, loss_ctc=30.04
Validation average objf: -10.480409 over 5505.0 utts
4epoch:train:1000-1100batch: loss=26.48, loss_att=22.7, loss_ctc=35.3
4epoch:train:1100-1200batch: loss=24.63, loss_att=21.09, loss_ctc=32.89
4epoch:train:1200-1300batch: loss=27.08, loss_att=23.22, loss_ctc=36.1
4epoch:train:1300-1400batch: loss=26.18, loss_att=22.45, loss_ctc=34.9
4epoch:train:1400-1500batch: loss=23.82, loss_att=20.45, loss_ctc=31.66
4epoch:train:1500-1600batch: loss=25.61, loss_att=21.89, loss_ctc=34.3
4epoch:train:1600-1700batch: loss=25.45, loss_att=21.83, loss_ctc=33.91
4epoch:train:1700-1800batch: loss=26.51, loss_att=22.72, loss_ctc=35.35
4epoch:train:1800-1900batch: loss=26.08, loss_att=22.36, loss_ctc=34.78
4epoch:train:1900-2000batch: loss=26.08, loss_att=22.4, loss_ctc=34.67
Validation average objf: -10.134316 over 5505.0 utts
4epoch:train:2000-2100batch: loss=25.53, loss_att=21.87, loss_ctc=34.09
4epoch:train:2100-2200batch: loss=25.89, loss_att=22.24, loss_ctc=34.4
4epoch:train:2200-2300batch: loss=24.81, loss_att=21.34, loss_ctc=32.91
4epoch:train:2300-2400batch: loss=25.27, loss_att=21.55, loss_ctc=33.95
4epoch:train:2400-2500batch: loss=26.1, loss_att=22.31, loss_ctc=34.95
4epoch:train:2500-2600batch: loss=25.72, loss_att=21.99, loss_ctc=34.42
4epoch:train:2600-2700batch: loss=25.88, loss_att=22.11, loss_ctc=34.68
4epoch:train:2700-2800batch: loss=26.81, loss_att=22.96, loss_ctc=35.8
4epoch:train:2800-2900batch: loss=26.72, loss_att=22.9, loss_ctc=35.65
4epoch:train:2900-3000batch: loss=26.89, loss_att=23.1, loss_ctc=35.73
Validation average objf: -9.940954 over 5505.0 utts
4epoch:train:3000-3100batch: loss=25.82, loss_att=22.01, loss_ctc=34.7
4epoch:train:3100-3200batch: loss=25.79, loss_att=22.06, loss_ctc=34.48
4epoch:train:3200-3300batch: loss=25.94, loss_att=22.15, loss_ctc=34.77
4epoch:train:3300-3400batch: loss=26.76, loss_att=22.84, loss_ctc=35.91
4epoch:train:3400-3500batch: loss=26.07, loss_att=22.35, loss_ctc=34.73
4epoch:train:3500-3600batch: loss=27.66, loss_att=23.68, loss_ctc=36.93
4epoch:train:3600-3700batch: loss=26.32, loss_att=22.5, loss_ctc=35.25
4epoch:train:3700-3800batch: loss=27.48, loss_att=23.38, loss_ctc=37.06
4epoch:train:3800-3900batch: loss=27.47, loss_att=23.51, loss_ctc=36.7
4epoch:train:3900-4000batch: loss=26.83, loss_att=23.01, loss_ctc=35.73
Validation average objf: -9.745496 over 5505.0 utts
4epoch:train:4000-4100batch: loss=27.72, loss_att=23.62, loss_ctc=37.29
4epoch:train:4100-4200batch: loss=26.31, loss_att=22.53, loss_ctc=35.13
4epoch:train:4200-4300batch: loss=26.3, loss_att=22.49, loss_ctc=35.18
4epoch:train:4300-4400batch: loss=26.36, loss_att=22.53, loss_ctc=35.3
4epoch:train:4400-4500batch: loss=26.91, loss_att=22.96, loss_ctc=36.12
4epoch:train:4500-4600batch: loss=25.92, loss_att=22.22, loss_ctc=34.58
4epoch:train:4600-4700batch: loss=26.94, loss_att=23.05, loss_ctc=36.0
4epoch:train:4700-4800batch: loss=26.66, loss_att=22.84, loss_ctc=35.57
4epoch:train:4800-4900batch: loss=27.62, loss_att=23.53, loss_ctc=37.18
4epoch:train:4900-5000batch: loss=25.61, loss_att=21.87, loss_ctc=34.35
Validation average objf: -9.719855 over 5505.0 utts
4epoch:train:5000-5100batch: loss=27.19, loss_att=23.2, loss_ctc=36.51
4epoch:train:5100-5200batch: loss=27.32, loss_att=23.37, loss_ctc=36.54
4epoch:train:5200-5300batch: loss=27.17, loss_att=23.11, loss_ctc=36.65
4epoch:train:5300-5400batch: loss=27.31, loss_att=23.15, loss_ctc=37.03
4epoch:train:5400-5500batch: loss=26.28, loss_att=22.34, loss_ctc=35.48
4epoch:train:5500-5600batch: loss=27.04, loss_att=23.05, loss_ctc=36.33
4epoch:train:5600-5700batch: loss=27.0, loss_att=23.15, loss_ctc=35.99
4epoch:train:5700-5800batch: loss=28.21, loss_att=24.05, loss_ctc=37.91
4epoch:train:5800-5900batch: loss=26.96, loss_att=22.99, loss_ctc=36.2
4epoch:train:5900-6000batch: loss=27.37, loss_att=23.37, loss_ctc=36.71
Validation average objf: -9.477894 over 5505.0 utts
4epoch:train:6000-6100batch: loss=27.26, loss_att=23.25, loss_ctc=36.61
4epoch:train:6100-6200batch: loss=28.47, loss_att=24.27, loss_ctc=38.29
4epoch:train:6200-6300batch: loss=27.89, loss_att=23.79, loss_ctc=37.46
4epoch:train:6300-6400batch: loss=26.82, loss_att=22.74, loss_ctc=36.34
4epoch:train:6400-6500batch: loss=27.0, loss_att=23.0, loss_ctc=36.32
4epoch:train:6500-6600batch: loss=26.23, loss_att=22.4, loss_ctc=35.15
4epoch:train:6600-6700batch: loss=27.42, loss_att=23.36, loss_ctc=36.91
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-4.pt: epoch=4, learning_rate=0.0011217033743744413, objf=-1.7430176914810755, valid_objf=-9.47789438578565
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-4-info
epoch 5, learning rate 0.0014956413276441083
5epoch:train:0-0batch: loss=23.0, loss_att=19.91, loss_ctc=30.21
5epoch:train:0-100batch: loss=22.41, loss_att=19.1, loss_ctc=30.12
5epoch:train:100-200batch: loss=22.73, loss_att=19.3, loss_ctc=30.74
5epoch:train:200-300batch: loss=21.69, loss_att=18.48, loss_ctc=29.2
5epoch:train:300-400batch: loss=21.36, loss_att=18.11, loss_ctc=28.95
5epoch:train:400-500batch: loss=22.13, loss_att=18.89, loss_ctc=29.69
5epoch:train:500-600batch: loss=21.31, loss_att=18.05, loss_ctc=28.9
5epoch:train:600-700batch: loss=21.57, loss_att=18.35, loss_ctc=29.07
5epoch:train:700-800batch: loss=22.12, loss_att=18.84, loss_ctc=29.77
5epoch:train:800-900batch: loss=20.71, loss_att=17.54, loss_ctc=28.09
5epoch:train:900-1000batch: loss=22.62, loss_att=19.2, loss_ctc=30.6
Validation average objf: -9.339709 over 5505.0 utts
5epoch:train:1000-1100batch: loss=23.25, loss_att=19.73, loss_ctc=31.48
5epoch:train:1100-1200batch: loss=21.9, loss_att=18.59, loss_ctc=29.64
5epoch:train:1200-1300batch: loss=21.43, loss_att=18.13, loss_ctc=29.14
5epoch:train:1300-1400batch: loss=21.84, loss_att=18.45, loss_ctc=29.75
5epoch:train:1400-1500batch: loss=21.95, loss_att=18.55, loss_ctc=29.89
5epoch:train:1500-1600batch: loss=23.22, loss_att=19.63, loss_ctc=31.59
5epoch:train:1600-1700batch: loss=21.08, loss_att=17.87, loss_ctc=28.57
5epoch:train:1700-1800batch: loss=22.25, loss_att=18.87, loss_ctc=30.11
5epoch:train:1800-1900batch: loss=22.18, loss_att=18.77, loss_ctc=30.14
5epoch:train:1900-2000batch: loss=22.64, loss_att=19.12, loss_ctc=30.86
Validation average objf: -9.081958 over 5505.0 utts
5epoch:train:2000-2100batch: loss=23.12, loss_att=19.51, loss_ctc=31.56
5epoch:train:2100-2200batch: loss=22.9, loss_att=19.42, loss_ctc=31.01
5epoch:train:2200-2300batch: loss=22.76, loss_att=19.24, loss_ctc=30.98
5epoch:train:2300-2400batch: loss=22.6, loss_att=19.16, loss_ctc=30.64
5epoch:train:2400-2500batch: loss=23.53, loss_att=19.93, loss_ctc=31.92
5epoch:train:2500-2600batch: loss=23.14, loss_att=19.64, loss_ctc=31.29
5epoch:train:2600-2700batch: loss=23.27, loss_att=19.68, loss_ctc=31.65
5epoch:train:2700-2800batch: loss=24.15, loss_att=20.48, loss_ctc=32.71
5epoch:train:2800-2900batch: loss=23.54, loss_att=19.86, loss_ctc=32.12
5epoch:train:2900-3000batch: loss=22.77, loss_att=19.18, loss_ctc=31.15
Validation average objf: -9.001672 over 5505.0 utts
5epoch:train:3000-3100batch: loss=23.15, loss_att=19.7, loss_ctc=31.23
5epoch:train:3100-3200batch: loss=25.21, loss_att=21.33, loss_ctc=34.27
5epoch:train:3200-3300batch: loss=24.78, loss_att=20.94, loss_ctc=33.75
5epoch:train:3300-3400batch: loss=23.92, loss_att=20.24, loss_ctc=32.49
5epoch:train:3400-3500batch: loss=24.39, loss_att=20.64, loss_ctc=33.16
5epoch:train:3500-3600batch: loss=25.43, loss_att=21.57, loss_ctc=34.43
5epoch:train:3600-3700batch: loss=24.66, loss_att=20.84, loss_ctc=33.57
5epoch:train:3700-3800batch: loss=23.86, loss_att=20.08, loss_ctc=32.68
5epoch:train:3800-3900batch: loss=25.02, loss_att=21.05, loss_ctc=34.28
5epoch:train:3900-4000batch: loss=23.59, loss_att=19.87, loss_ctc=32.27
Validation average objf: -8.881179 over 5505.0 utts
5epoch:train:4000-4100batch: loss=22.74, loss_att=19.32, loss_ctc=30.74
5epoch:train:4100-4200batch: loss=23.92, loss_att=20.16, loss_ctc=32.69
5epoch:train:4200-4300batch: loss=24.06, loss_att=20.36, loss_ctc=32.69
5epoch:train:4300-4400batch: loss=25.14, loss_att=21.23, loss_ctc=34.27
5epoch:train:4400-4500batch: loss=24.3, loss_att=20.48, loss_ctc=33.21
5epoch:train:4500-4600batch: loss=24.02, loss_att=20.3, loss_ctc=32.69
5epoch:train:4600-4700batch: loss=24.29, loss_att=20.56, loss_ctc=33.0
5epoch:train:4700-4800batch: loss=25.01, loss_att=21.07, loss_ctc=34.21
5epoch:train:4800-4900batch: loss=25.38, loss_att=21.55, loss_ctc=34.31
5epoch:train:4900-5000batch: loss=25.48, loss_att=21.5, loss_ctc=34.76
Validation average objf: -8.778023 over 5505.0 utts
5epoch:train:5000-5100batch: loss=24.51, loss_att=20.63, loss_ctc=33.54
5epoch:train:5100-5200batch: loss=24.99, loss_att=20.99, loss_ctc=34.32
5epoch:train:5200-5300batch: loss=23.94, loss_att=20.24, loss_ctc=32.57
5epoch:train:5300-5400batch: loss=25.29, loss_att=21.25, loss_ctc=34.71
5epoch:train:5400-5500batch: loss=26.33, loss_att=22.17, loss_ctc=36.03
5epoch:train:5500-5600batch: loss=25.39, loss_att=21.43, loss_ctc=34.63
5epoch:train:5600-5700batch: loss=24.81, loss_att=20.89, loss_ctc=33.94
5epoch:train:5700-5800batch: loss=24.14, loss_att=20.4, loss_ctc=32.89
5epoch:train:5800-5900batch: loss=24.44, loss_att=20.63, loss_ctc=33.33
5epoch:train:5900-6000batch: loss=24.52, loss_att=20.66, loss_ctc=33.51
Validation average objf: -8.925979 over 5505.0 utts
5epoch:train:6000-6100batch: loss=25.2, loss_att=21.21, loss_ctc=34.51
5epoch:train:6100-6200batch: loss=25.18, loss_att=21.22, loss_ctc=34.42
5epoch:train:6200-6300batch: loss=24.84, loss_att=20.89, loss_ctc=34.05
5epoch:train:6300-6400batch: loss=24.75, loss_att=21.0, loss_ctc=33.5
5epoch:train:6400-6500batch: loss=25.75, loss_att=21.73, loss_ctc=35.13
5epoch:train:6500-6600batch: loss=25.25, loss_att=21.23, loss_ctc=34.65
5epoch:train:6600-6700batch: loss=25.07, loss_att=21.12, loss_ctc=34.29
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-5.pt: epoch=5, learning_rate=0.0014956413276441083, objf=-1.5802427903581522, valid_objf=-8.925979223433243
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-5-info
epoch 6, learning rate 0.0018695240381964952
6epoch:train:0-0batch: loss=21.89, loss_att=19.12, loss_ctc=28.36
6epoch:train:0-100batch: loss=20.62, loss_att=17.4, loss_ctc=28.15
6epoch:train:100-200batch: loss=21.1, loss_att=17.76, loss_ctc=28.89
6epoch:train:200-300batch: loss=20.3, loss_att=17.06, loss_ctc=27.86
6epoch:train:300-400batch: loss=20.41, loss_att=17.11, loss_ctc=28.11
6epoch:train:400-500batch: loss=21.44, loss_att=18.1, loss_ctc=29.24
6epoch:train:500-600batch: loss=20.12, loss_att=16.95, loss_ctc=27.54
6epoch:train:600-700batch: loss=18.88, loss_att=15.91, loss_ctc=25.81
6epoch:train:700-800batch: loss=19.43, loss_att=16.3, loss_ctc=26.76
6epoch:train:800-900batch: loss=20.44, loss_att=17.19, loss_ctc=28.01
6epoch:train:900-1000batch: loss=21.41, loss_att=17.93, loss_ctc=29.52
Validation average objf: -8.534703 over 5505.0 utts
6epoch:train:1000-1100batch: loss=19.76, loss_att=16.56, loss_ctc=27.25
6epoch:train:1100-1200batch: loss=19.59, loss_att=16.48, loss_ctc=26.84
6epoch:train:1200-1300batch: loss=20.07, loss_att=16.83, loss_ctc=27.64
6epoch:train:1300-1400batch: loss=21.62, loss_att=18.14, loss_ctc=29.76
6epoch:train:1400-1500batch: loss=21.05, loss_att=17.76, loss_ctc=28.71
6epoch:train:1500-1600batch: loss=20.93, loss_att=17.57, loss_ctc=28.78
6epoch:train:1600-1700batch: loss=22.19, loss_att=18.59, loss_ctc=30.6
6epoch:train:1700-1800batch: loss=22.51, loss_att=18.92, loss_ctc=30.9
6epoch:train:1800-1900batch: loss=20.5, loss_att=17.25, loss_ctc=28.07
6epoch:train:1900-2000batch: loss=20.68, loss_att=17.34, loss_ctc=28.46
Validation average objf: -8.828461 over 5505.0 utts
6epoch:train:2000-2100batch: loss=20.13, loss_att=16.92, loss_ctc=27.62
6epoch:train:2100-2200batch: loss=22.14, loss_att=18.58, loss_ctc=30.44
6epoch:train:2200-2300batch: loss=22.96, loss_att=19.19, loss_ctc=31.77
6epoch:train:2300-2400batch: loss=21.89, loss_att=18.28, loss_ctc=30.32
6epoch:train:2400-2500batch: loss=22.52, loss_att=18.83, loss_ctc=31.12
6epoch:train:2500-2600batch: loss=21.6, loss_att=18.19, loss_ctc=29.54
6epoch:train:2600-2700batch: loss=21.98, loss_att=18.41, loss_ctc=30.32
6epoch:train:2700-2800batch: loss=22.76, loss_att=18.95, loss_ctc=31.63
6epoch:train:2800-2900batch: loss=23.32, loss_att=19.48, loss_ctc=32.29
6epoch:train:2900-3000batch: loss=23.32, loss_att=19.43, loss_ctc=32.4
Validation average objf: -8.967942 over 5505.0 utts
6epoch:train:3000-3100batch: loss=23.42, loss_att=19.69, loss_ctc=32.13
6epoch:train:3100-3200batch: loss=23.76, loss_att=19.88, loss_ctc=32.82
6epoch:train:3200-3300batch: loss=23.44, loss_att=19.66, loss_ctc=32.27
6epoch:train:3300-3400batch: loss=24.1, loss_att=20.19, loss_ctc=33.23
6epoch:train:3400-3500batch: loss=22.84, loss_att=19.15, loss_ctc=31.46
6epoch:train:3500-3600batch: loss=22.92, loss_att=19.23, loss_ctc=31.53
6epoch:train:3600-3700batch: loss=24.39, loss_att=20.44, loss_ctc=33.6
6epoch:train:3700-3800batch: loss=23.59, loss_att=19.8, loss_ctc=32.44
6epoch:train:3800-3900batch: loss=24.06, loss_att=20.1, loss_ctc=33.32
6epoch:train:3900-4000batch: loss=22.77, loss_att=19.06, loss_ctc=31.43
Validation average objf: -8.895424 over 5505.0 utts
6epoch:train:4000-4100batch: loss=23.3, loss_att=19.47, loss_ctc=32.24
6epoch:train:4100-4200batch: loss=23.73, loss_att=19.94, loss_ctc=32.55
6epoch:train:4200-4300batch: loss=23.61, loss_att=19.69, loss_ctc=32.76
6epoch:train:4300-4400batch: loss=23.71, loss_att=19.79, loss_ctc=32.86
6epoch:train:4400-4500batch: loss=23.97, loss_att=20.12, loss_ctc=32.96
6epoch:train:4500-4600batch: loss=23.05, loss_att=19.31, loss_ctc=31.76
6epoch:train:4600-4700batch: loss=23.24, loss_att=19.48, loss_ctc=32.02
6epoch:train:4700-4800batch: loss=23.35, loss_att=19.57, loss_ctc=32.18
6epoch:train:4800-4900batch: loss=23.82, loss_att=19.92, loss_ctc=32.93
6epoch:train:4900-5000batch: loss=24.07, loss_att=20.11, loss_ctc=33.3
Validation average objf: -8.820909 over 5505.0 utts
6epoch:train:5000-5100batch: loss=24.32, loss_att=20.32, loss_ctc=33.65
6epoch:train:5100-5200batch: loss=24.09, loss_att=20.22, loss_ctc=33.11
6epoch:train:5200-5300batch: loss=23.82, loss_att=19.9, loss_ctc=32.99
6epoch:train:5300-5400batch: loss=23.75, loss_att=19.9, loss_ctc=32.72
6epoch:train:5400-5500batch: loss=23.8, loss_att=19.95, loss_ctc=32.78
6epoch:train:5500-5600batch: loss=24.51, loss_att=20.56, loss_ctc=33.73
6epoch:train:5600-5700batch: loss=24.33, loss_att=20.34, loss_ctc=33.62
6epoch:train:5700-5800batch: loss=24.59, loss_att=20.64, loss_ctc=33.79
6epoch:train:5800-5900batch: loss=24.55, loss_att=20.57, loss_ctc=33.82
6epoch:train:5900-6000batch: loss=24.45, loss_att=20.5, loss_ctc=33.65
Validation average objf: -9.032330 over 5505.0 utts
6epoch:train:6000-6100batch: loss=22.41, loss_att=18.58, loss_ctc=31.33
6epoch:train:6100-6200batch: loss=23.7, loss_att=19.84, loss_ctc=32.71
6epoch:train:6200-6300batch: loss=24.21, loss_att=20.22, loss_ctc=33.54
6epoch:train:6300-6400batch: loss=23.88, loss_att=20.01, loss_ctc=32.94
6epoch:train:6400-6500batch: loss=25.21, loss_att=21.03, loss_ctc=34.97
6epoch:train:6500-6600batch: loss=24.0, loss_att=20.08, loss_ctc=33.16
6epoch:train:6600-6700batch: loss=25.09, loss_att=20.89, loss_ctc=34.88
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-6.pt: epoch=6, learning_rate=0.0018695240381964952, objf=-1.5072341597414876, valid_objf=-9.032329984105358
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-6-info
epoch 7, learning rate 0.0021930229555531936
7epoch:train:0-0batch: loss=34.11, loss_att=28.94, loss_ctc=46.16
7epoch:train:0-100batch: loss=20.62, loss_att=17.31, loss_ctc=28.34
7epoch:train:100-200batch: loss=20.0, loss_att=16.69, loss_ctc=27.72
7epoch:train:200-300batch: loss=20.71, loss_att=17.31, loss_ctc=28.63
7epoch:train:300-400batch: loss=19.89, loss_att=16.61, loss_ctc=27.55
7epoch:train:400-500batch: loss=19.01, loss_att=15.88, loss_ctc=26.3
7epoch:train:500-600batch: loss=19.88, loss_att=16.56, loss_ctc=27.61
7epoch:train:600-700batch: loss=18.13, loss_att=15.12, loss_ctc=25.16
7epoch:train:700-800batch: loss=19.76, loss_att=16.52, loss_ctc=27.31
7epoch:train:800-900batch: loss=18.55, loss_att=15.43, loss_ctc=25.82
7epoch:train:900-1000batch: loss=18.84, loss_att=15.66, loss_ctc=26.26
Validation average objf: -8.132480 over 5505.0 utts
7epoch:train:1000-1100batch: loss=18.67, loss_att=15.48, loss_ctc=26.12
7epoch:train:1100-1200batch: loss=20.2, loss_att=16.79, loss_ctc=28.16
7epoch:train:1200-1300batch: loss=18.71, loss_att=15.69, loss_ctc=25.75
7epoch:train:1300-1400batch: loss=19.83, loss_att=16.55, loss_ctc=27.46
7epoch:train:1400-1500batch: loss=20.31, loss_att=16.9, loss_ctc=28.28
7epoch:train:1500-1600batch: loss=20.45, loss_att=17.05, loss_ctc=28.39
7epoch:train:1600-1700batch: loss=20.65, loss_att=17.24, loss_ctc=28.6
7epoch:train:1700-1800batch: loss=19.28, loss_att=16.04, loss_ctc=26.82
7epoch:train:1800-1900batch: loss=20.27, loss_att=16.88, loss_ctc=28.19
7epoch:train:1900-2000batch: loss=20.98, loss_att=17.53, loss_ctc=29.05
Validation average objf: -8.241313 over 5505.0 utts
7epoch:train:2000-2100batch: loss=20.99, loss_att=17.53, loss_ctc=29.06
7epoch:train:2100-2200batch: loss=21.07, loss_att=17.52, loss_ctc=29.34
7epoch:train:2200-2300batch: loss=20.33, loss_att=16.96, loss_ctc=28.19
7epoch:train:2300-2400batch: loss=20.72, loss_att=17.25, loss_ctc=28.81
7epoch:train:2400-2500batch: loss=20.62, loss_att=17.21, loss_ctc=28.58
7epoch:train:2500-2600batch: loss=19.73, loss_att=16.35, loss_ctc=27.59
7epoch:train:2600-2700batch: loss=19.69, loss_att=16.3, loss_ctc=27.61
7epoch:train:2700-2800batch: loss=20.74, loss_att=17.23, loss_ctc=28.92
7epoch:train:2800-2900batch: loss=19.87, loss_att=16.66, loss_ctc=27.38
7epoch:train:2900-3000batch: loss=18.85, loss_att=15.69, loss_ctc=26.21
Validation average objf: -7.865908 over 5505.0 utts
7epoch:train:3000-3100batch: loss=20.52, loss_att=17.09, loss_ctc=28.54
7epoch:train:3100-3200batch: loss=20.63, loss_att=17.26, loss_ctc=28.5
7epoch:train:3200-3300batch: loss=20.14, loss_att=16.76, loss_ctc=28.04
7epoch:train:3300-3400batch: loss=20.19, loss_att=16.75, loss_ctc=28.22
7epoch:train:3400-3500batch: loss=21.79, loss_att=18.08, loss_ctc=30.46
7epoch:train:3500-3600batch: loss=21.16, loss_att=17.75, loss_ctc=29.13
7epoch:train:3600-3700batch: loss=21.33, loss_att=17.72, loss_ctc=29.76
7epoch:train:3700-3800batch: loss=21.8, loss_att=18.2, loss_ctc=30.19
7epoch:train:3800-3900batch: loss=21.99, loss_att=18.29, loss_ctc=30.62
7epoch:train:3900-4000batch: loss=22.14, loss_att=18.47, loss_ctc=30.7
Validation average objf: -7.870344 over 5505.0 utts
7epoch:train:4000-4100batch: loss=21.55, loss_att=17.96, loss_ctc=29.91
7epoch:train:4100-4200batch: loss=20.18, loss_att=16.85, loss_ctc=27.95
7epoch:train:4200-4300batch: loss=20.96, loss_att=17.5, loss_ctc=29.03
7epoch:train:4300-4400batch: loss=20.12, loss_att=16.79, loss_ctc=27.89
7epoch:train:4400-4500batch: loss=21.58, loss_att=17.96, loss_ctc=30.02
7epoch:train:4500-4600batch: loss=20.72, loss_att=17.21, loss_ctc=28.9
7epoch:train:4600-4700batch: loss=21.11, loss_att=17.68, loss_ctc=29.12
7epoch:train:4700-4800batch: loss=20.87, loss_att=17.35, loss_ctc=29.1
7epoch:train:4800-4900batch: loss=21.55, loss_att=18.0, loss_ctc=29.84
7epoch:train:4900-5000batch: loss=20.9, loss_att=17.4, loss_ctc=29.05
Validation average objf: -7.883959 over 5505.0 utts
7epoch:train:5000-5100batch: loss=21.52, loss_att=17.86, loss_ctc=30.08
7epoch:train:5100-5200batch: loss=22.03, loss_att=18.2, loss_ctc=30.96
7epoch:train:5200-5300batch: loss=20.99, loss_att=17.47, loss_ctc=29.2
7epoch:train:5300-5400batch: loss=22.4, loss_att=18.57, loss_ctc=31.35
7epoch:train:5400-5500batch: loss=21.7, loss_att=18.11, loss_ctc=30.09
7epoch:train:5500-5600batch: loss=20.62, loss_att=17.19, loss_ctc=28.62
7epoch:train:5600-5700batch: loss=22.37, loss_att=18.65, loss_ctc=31.03
7epoch:train:5700-5800batch: loss=21.65, loss_att=18.12, loss_ctc=29.9
7epoch:train:5800-5900batch: loss=20.96, loss_att=17.37, loss_ctc=29.33
7epoch:train:5900-6000batch: loss=21.31, loss_att=17.73, loss_ctc=29.67
Validation average objf: -7.768178 over 5505.0 utts
7epoch:train:6000-6100batch: loss=21.9, loss_att=18.28, loss_ctc=30.35
7epoch:train:6100-6200batch: loss=21.25, loss_att=17.67, loss_ctc=29.61
7epoch:train:6200-6300batch: loss=21.55, loss_att=18.02, loss_ctc=29.79
7epoch:train:6300-6400batch: loss=21.65, loss_att=17.95, loss_ctc=30.26
7epoch:train:6400-6500batch: loss=20.85, loss_att=17.4, loss_ctc=28.93
7epoch:train:6500-6600batch: loss=20.56, loss_att=17.12, loss_ctc=28.57
7epoch:train:6600-6700batch: loss=20.47, loss_att=17.07, loss_ctc=28.4
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-7.pt: epoch=7, learning_rate=0.0021930229555531936, objf=-1.3742758044379382, valid_objf=-7.76817807674841
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-7-info
epoch 8, learning rate 0.0020303554448011536
8epoch:train:0-0batch: loss=16.09, loss_att=12.67, loss_ctc=24.08
8epoch:train:0-100batch: loss=17.92, loss_att=14.96, loss_ctc=24.84
8epoch:train:100-200batch: loss=17.74, loss_att=14.78, loss_ctc=24.65
8epoch:train:200-300batch: loss=17.74, loss_att=14.76, loss_ctc=24.69
8epoch:train:300-400batch: loss=15.79, loss_att=13.19, loss_ctc=21.88
8epoch:train:400-500batch: loss=16.96, loss_att=14.09, loss_ctc=23.65
8epoch:train:500-600batch: loss=17.37, loss_att=14.41, loss_ctc=24.25
8epoch:train:600-700batch: loss=17.4, loss_att=14.4, loss_ctc=24.39
8epoch:train:700-800batch: loss=16.87, loss_att=14.0, loss_ctc=23.59
8epoch:train:800-900batch: loss=17.4, loss_att=14.44, loss_ctc=24.31
8epoch:train:900-1000batch: loss=17.3, loss_att=14.32, loss_ctc=24.25
Validation average objf: -7.452264 over 5505.0 utts
8epoch:train:1000-1100batch: loss=17.57, loss_att=14.6, loss_ctc=24.49
8epoch:train:1100-1200batch: loss=17.48, loss_att=14.5, loss_ctc=24.46
8epoch:train:1200-1300batch: loss=17.95, loss_att=14.87, loss_ctc=25.14
8epoch:train:1300-1400batch: loss=17.6, loss_att=14.57, loss_ctc=24.65
8epoch:train:1400-1500batch: loss=17.82, loss_att=14.82, loss_ctc=24.81
8epoch:train:1500-1600batch: loss=17.18, loss_att=14.33, loss_ctc=23.83
8epoch:train:1600-1700batch: loss=18.17, loss_att=15.05, loss_ctc=25.44
8epoch:train:1700-1800batch: loss=17.06, loss_att=14.17, loss_ctc=23.79
8epoch:train:1800-1900batch: loss=17.82, loss_att=14.7, loss_ctc=25.12
8epoch:train:1900-2000batch: loss=18.79, loss_att=15.57, loss_ctc=26.29
Validation average objf: -7.527173 over 5505.0 utts
8epoch:train:2000-2100batch: loss=19.32, loss_att=16.07, loss_ctc=26.92
8epoch:train:2100-2200batch: loss=18.54, loss_att=15.36, loss_ctc=25.96
8epoch:train:2200-2300batch: loss=18.15, loss_att=15.09, loss_ctc=25.3
8epoch:train:2300-2400batch: loss=18.59, loss_att=15.46, loss_ctc=25.91
8epoch:train:2400-2500batch: loss=18.44, loss_att=15.32, loss_ctc=25.73
8epoch:train:2500-2600batch: loss=19.02, loss_att=15.87, loss_ctc=26.37
8epoch:train:2600-2700batch: loss=17.76, loss_att=14.78, loss_ctc=24.7
8epoch:train:2700-2800batch: loss=18.57, loss_att=15.42, loss_ctc=25.92
8epoch:train:2800-2900batch: loss=18.64, loss_att=15.48, loss_ctc=26.02
8epoch:train:2900-3000batch: loss=19.08, loss_att=15.8, loss_ctc=26.73
Validation average objf: -7.413002 over 5505.0 utts
8epoch:train:3000-3100batch: loss=18.71, loss_att=15.56, loss_ctc=26.07
8epoch:train:3100-3200batch: loss=19.6, loss_att=16.28, loss_ctc=27.35
8epoch:train:3200-3300batch: loss=19.3, loss_att=15.97, loss_ctc=27.05
8epoch:train:3300-3400batch: loss=19.34, loss_att=16.01, loss_ctc=27.12
8epoch:train:3400-3500batch: loss=19.1, loss_att=15.96, loss_ctc=26.42
8epoch:train:3500-3600batch: loss=19.35, loss_att=16.03, loss_ctc=27.09
8epoch:train:3600-3700batch: loss=18.87, loss_att=15.65, loss_ctc=26.36
8epoch:train:3700-3800batch: loss=19.6, loss_att=16.33, loss_ctc=27.23
8epoch:train:3800-3900batch: loss=18.73, loss_att=15.57, loss_ctc=26.11
8epoch:train:3900-4000batch: loss=19.1, loss_att=15.86, loss_ctc=26.67
Validation average objf: -7.425176 over 5505.0 utts
8epoch:train:4000-4100batch: loss=19.48, loss_att=16.07, loss_ctc=27.43
8epoch:train:4100-4200batch: loss=19.48, loss_att=16.17, loss_ctc=27.22
8epoch:train:4200-4300batch: loss=18.26, loss_att=15.12, loss_ctc=25.56
8epoch:train:4300-4400batch: loss=18.82, loss_att=15.62, loss_ctc=26.29
8epoch:train:4400-4500batch: loss=18.86, loss_att=15.69, loss_ctc=26.28
8epoch:train:4500-4600batch: loss=18.17, loss_att=15.18, loss_ctc=25.14
8epoch:train:4600-4700batch: loss=19.4, loss_att=16.16, loss_ctc=26.96
8epoch:train:4700-4800batch: loss=19.3, loss_att=16.0, loss_ctc=26.98
8epoch:train:4800-4900batch: loss=19.95, loss_att=16.61, loss_ctc=27.74
8epoch:train:4900-5000batch: loss=19.33, loss_att=16.07, loss_ctc=26.93
Validation average objf: -7.377741 over 5505.0 utts
8epoch:train:5000-5100batch: loss=20.08, loss_att=16.65, loss_ctc=28.09
8epoch:train:5100-5200batch: loss=19.48, loss_att=16.15, loss_ctc=27.25
8epoch:train:5200-5300batch: loss=19.32, loss_att=16.01, loss_ctc=27.06
8epoch:train:5300-5400batch: loss=19.2, loss_att=15.98, loss_ctc=26.7
8epoch:train:5400-5500batch: loss=19.37, loss_att=16.05, loss_ctc=27.11
8epoch:train:5500-5600batch: loss=19.04, loss_att=15.79, loss_ctc=26.65
8epoch:train:5600-5700batch: loss=18.86, loss_att=15.67, loss_ctc=26.3
8epoch:train:5700-5800batch: loss=19.62, loss_att=16.15, loss_ctc=27.72
8epoch:train:5800-5900batch: loss=19.75, loss_att=16.37, loss_ctc=27.63
8epoch:train:5900-6000batch: loss=19.31, loss_att=16.12, loss_ctc=26.73
Validation average objf: -7.171008 over 5505.0 utts
8epoch:train:6000-6100batch: loss=19.48, loss_att=16.2, loss_ctc=27.14
8epoch:train:6100-6200batch: loss=19.68, loss_att=16.36, loss_ctc=27.43
8epoch:train:6200-6300batch: loss=19.12, loss_att=15.97, loss_ctc=26.46
8epoch:train:6300-6400batch: loss=19.71, loss_att=16.31, loss_ctc=27.65
8epoch:train:6400-6500batch: loss=19.39, loss_att=16.09, loss_ctc=27.09
8epoch:train:6500-6600batch: loss=19.0, loss_att=15.76, loss_ctc=26.57
8epoch:train:6600-6700batch: loss=19.08, loss_att=15.8, loss_ctc=26.73
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-8.pt: epoch=8, learning_rate=0.0020303554448011536, objf=-1.236555628180787, valid_objf=-7.1710078905540415
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-8-info
epoch 9, learning rate 0.0018992311280803882
9epoch:train:0-0batch: loss=15.91, loss_att=12.48, loss_ctc=23.93
9epoch:train:0-100batch: loss=14.83, loss_att=12.33, loss_ctc=20.64
9epoch:train:100-200batch: loss=17.33, loss_att=14.37, loss_ctc=24.25
9epoch:train:200-300batch: loss=17.03, loss_att=14.1, loss_ctc=23.88
9epoch:train:300-400batch: loss=16.54, loss_att=13.66, loss_ctc=23.26
9epoch:train:400-500batch: loss=15.51, loss_att=12.92, loss_ctc=21.56
9epoch:train:500-600batch: loss=16.48, loss_att=13.7, loss_ctc=22.96
9epoch:train:600-700batch: loss=16.22, loss_att=13.41, loss_ctc=22.77
9epoch:train:700-800batch: loss=16.08, loss_att=13.4, loss_ctc=22.33
9epoch:train:800-900batch: loss=15.96, loss_att=13.27, loss_ctc=22.24
9epoch:train:900-1000batch: loss=17.3, loss_att=14.28, loss_ctc=24.32
Validation average objf: -7.055775 over 5505.0 utts
9epoch:train:1000-1100batch: loss=15.32, loss_att=12.67, loss_ctc=21.53
9epoch:train:1100-1200batch: loss=16.89, loss_att=14.03, loss_ctc=23.54
9epoch:train:1200-1300batch: loss=16.32, loss_att=13.48, loss_ctc=22.97
9epoch:train:1300-1400batch: loss=16.8, loss_att=13.85, loss_ctc=23.67
9epoch:train:1400-1500batch: loss=16.15, loss_att=13.35, loss_ctc=22.68
9epoch:train:1500-1600batch: loss=15.51, loss_att=12.79, loss_ctc=21.86
9epoch:train:1600-1700batch: loss=15.92, loss_att=13.2, loss_ctc=22.26
9epoch:train:1700-1800batch: loss=17.43, loss_att=14.46, loss_ctc=24.38
9epoch:train:1800-1900batch: loss=17.07, loss_att=14.21, loss_ctc=23.74
9epoch:train:1900-2000batch: loss=16.95, loss_att=14.01, loss_ctc=23.81
Validation average objf: -6.968806 over 5505.0 utts
9epoch:train:2000-2100batch: loss=16.68, loss_att=13.82, loss_ctc=23.35
9epoch:train:2100-2200batch: loss=16.7, loss_att=13.88, loss_ctc=23.26
9epoch:train:2200-2300batch: loss=17.38, loss_att=14.44, loss_ctc=24.23
9epoch:train:2300-2400batch: loss=16.69, loss_att=13.86, loss_ctc=23.3
9epoch:train:2400-2500batch: loss=16.39, loss_att=13.54, loss_ctc=23.05
9epoch:train:2500-2600batch: loss=16.12, loss_att=13.31, loss_ctc=22.69
9epoch:train:2600-2700batch: loss=16.95, loss_att=14.09, loss_ctc=23.63
9epoch:train:2700-2800batch: loss=17.12, loss_att=14.21, loss_ctc=23.91
9epoch:train:2800-2900batch: loss=17.54, loss_att=14.48, loss_ctc=24.7
9epoch:train:2900-3000batch: loss=17.12, loss_att=14.2, loss_ctc=23.92
Validation average objf: -6.971937 over 5505.0 utts
9epoch:train:3000-3100batch: loss=17.26, loss_att=14.24, loss_ctc=24.32
9epoch:train:3100-3200batch: loss=17.5, loss_att=14.55, loss_ctc=24.38
9epoch:train:3200-3300batch: loss=17.13, loss_att=14.25, loss_ctc=23.85
9epoch:train:3300-3400batch: loss=16.84, loss_att=13.8, loss_ctc=23.94
9epoch:train:3400-3500batch: loss=17.61, loss_att=14.51, loss_ctc=24.84
9epoch:train:3500-3600batch: loss=17.03, loss_att=14.16, loss_ctc=23.71
9epoch:train:3600-3700batch: loss=18.7, loss_att=15.51, loss_ctc=26.13
9epoch:train:3700-3800batch: loss=17.93, loss_att=14.8, loss_ctc=25.22
9epoch:train:3800-3900batch: loss=16.99, loss_att=14.09, loss_ctc=23.77
9epoch:train:3900-4000batch: loss=18.07, loss_att=14.94, loss_ctc=25.36
Validation average objf: -7.199908 over 5505.0 utts
9epoch:train:4000-4100batch: loss=18.02, loss_att=14.91, loss_ctc=25.28
9epoch:train:4100-4200batch: loss=18.06, loss_att=14.92, loss_ctc=25.4
9epoch:train:4200-4300batch: loss=18.43, loss_att=15.3, loss_ctc=25.74
9epoch:train:4300-4400batch: loss=17.95, loss_att=14.96, loss_ctc=24.92
9epoch:train:4400-4500batch: loss=18.68, loss_att=15.55, loss_ctc=25.99
9epoch:train:4500-4600batch: loss=17.87, loss_att=14.84, loss_ctc=24.94
9epoch:train:4600-4700batch: loss=17.97, loss_att=14.86, loss_ctc=25.21
9epoch:train:4700-4800batch: loss=17.64, loss_att=14.56, loss_ctc=24.83
9epoch:train:4800-4900batch: loss=18.18, loss_att=15.06, loss_ctc=25.46
9epoch:train:4900-5000batch: loss=17.56, loss_att=14.63, loss_ctc=24.4
Validation average objf: -7.001182 over 5505.0 utts
9epoch:train:5000-5100batch: loss=18.57, loss_att=15.42, loss_ctc=25.93
9epoch:train:5100-5200batch: loss=18.16, loss_att=15.01, loss_ctc=25.5
9epoch:train:5200-5300batch: loss=17.69, loss_att=14.69, loss_ctc=24.69
9epoch:train:5300-5400batch: loss=17.5, loss_att=14.49, loss_ctc=24.52
9epoch:train:5400-5500batch: loss=18.0, loss_att=14.88, loss_ctc=25.3
9epoch:train:5500-5600batch: loss=17.46, loss_att=14.46, loss_ctc=24.45
9epoch:train:5600-5700batch: loss=18.13, loss_att=15.01, loss_ctc=25.41
9epoch:train:5700-5800batch: loss=18.15, loss_att=15.07, loss_ctc=25.34
9epoch:train:5800-5900batch: loss=17.46, loss_att=14.57, loss_ctc=24.21
9epoch:train:5900-6000batch: loss=18.16, loss_att=15.12, loss_ctc=25.24
Validation average objf: -6.941102 over 5505.0 utts
9epoch:train:6000-6100batch: loss=17.7, loss_att=14.73, loss_ctc=24.61
9epoch:train:6100-6200batch: loss=17.88, loss_att=14.72, loss_ctc=25.24
9epoch:train:6200-6300batch: loss=18.23, loss_att=14.96, loss_ctc=25.84
9epoch:train:6300-6400batch: loss=18.62, loss_att=15.48, loss_ctc=25.97
9epoch:train:6400-6500batch: loss=17.7, loss_att=14.67, loss_ctc=24.75
9epoch:train:6500-6600batch: loss=17.08, loss_att=14.07, loss_ctc=24.11
9epoch:train:6600-6700batch: loss=18.2, loss_att=15.13, loss_ctc=25.35
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-9.pt: epoch=9, learning_rate=0.0018992311280803882, objf=-1.1426288913699878, valid_objf=-6.941101839237057
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-9-info
epoch 10, learning rate 0.0017906177912111107
10epoch:train:0-0batch: loss=29.13, loss_att=23.83, loss_ctc=41.5
10epoch:train:0-100batch: loss=16.27, loss_att=13.47, loss_ctc=22.81
10epoch:train:100-200batch: loss=16.32, loss_att=13.48, loss_ctc=22.96
10epoch:train:200-300batch: loss=15.26, loss_att=12.59, loss_ctc=21.49
10epoch:train:300-400batch: loss=15.44, loss_att=12.88, loss_ctc=21.43
10epoch:train:400-500batch: loss=14.64, loss_att=12.14, loss_ctc=20.47
10epoch:train:500-600batch: loss=15.59, loss_att=12.83, loss_ctc=22.02
10epoch:train:600-700batch: loss=15.62, loss_att=13.02, loss_ctc=21.71
10epoch:train:700-800batch: loss=15.21, loss_att=12.55, loss_ctc=21.4
10epoch:train:800-900batch: loss=15.2, loss_att=12.55, loss_ctc=21.39
10epoch:train:900-1000batch: loss=15.02, loss_att=12.4, loss_ctc=21.12
Validation average objf: -6.739225 over 5505.0 utts
10epoch:train:1000-1100batch: loss=15.79, loss_att=13.06, loss_ctc=22.16
10epoch:train:1100-1200batch: loss=14.17, loss_att=11.79, loss_ctc=19.71
10epoch:train:1200-1300batch: loss=14.26, loss_att=11.82, loss_ctc=19.96
10epoch:train:1300-1400batch: loss=15.69, loss_att=12.94, loss_ctc=22.11
10epoch:train:1400-1500batch: loss=15.13, loss_att=12.44, loss_ctc=21.41
10epoch:train:1500-1600batch: loss=16.09, loss_att=13.25, loss_ctc=22.72
10epoch:train:1600-1700batch: loss=15.35, loss_att=12.62, loss_ctc=21.73
10epoch:train:1700-1800batch: loss=15.22, loss_att=12.62, loss_ctc=21.29
10epoch:train:1800-1900batch: loss=15.56, loss_att=12.8, loss_ctc=22.0
10epoch:train:1900-2000batch: loss=15.53, loss_att=12.91, loss_ctc=21.64
Validation average objf: -6.833452 over 5505.0 utts
10epoch:train:2000-2100batch: loss=15.48, loss_att=12.74, loss_ctc=21.86
10epoch:train:2100-2200batch: loss=15.45, loss_att=12.7, loss_ctc=21.84
10epoch:train:2200-2300batch: loss=16.28, loss_att=13.4, loss_ctc=23.01
10epoch:train:2300-2400batch: loss=14.98, loss_att=12.3, loss_ctc=21.23
10epoch:train:2400-2500batch: loss=15.82, loss_att=13.1, loss_ctc=22.17
10epoch:train:2500-2600batch: loss=16.6, loss_att=13.64, loss_ctc=23.5
10epoch:train:2600-2700batch: loss=15.55, loss_att=12.87, loss_ctc=21.78
10epoch:train:2700-2800batch: loss=16.59, loss_att=13.72, loss_ctc=23.3
10epoch:train:2800-2900batch: loss=16.95, loss_att=14.0, loss_ctc=23.84
10epoch:train:2900-3000batch: loss=16.54, loss_att=13.66, loss_ctc=23.24
Validation average objf: -6.662311 over 5505.0 utts
10epoch:train:3000-3100batch: loss=15.71, loss_att=12.92, loss_ctc=22.23
10epoch:train:3100-3200batch: loss=16.78, loss_att=13.84, loss_ctc=23.65
10epoch:train:3200-3300batch: loss=16.57, loss_att=13.7, loss_ctc=23.27
10epoch:train:3300-3400batch: loss=16.31, loss_att=13.48, loss_ctc=22.92
10epoch:train:3400-3500batch: loss=16.22, loss_att=13.48, loss_ctc=22.62
10epoch:train:3500-3600batch: loss=16.9, loss_att=13.95, loss_ctc=23.8
10epoch:train:3600-3700batch: loss=16.53, loss_att=13.69, loss_ctc=23.15
10epoch:train:3700-3800batch: loss=16.68, loss_att=13.84, loss_ctc=23.3
10epoch:train:3800-3900batch: loss=16.65, loss_att=13.85, loss_ctc=23.19
10epoch:train:3900-4000batch: loss=16.77, loss_att=13.88, loss_ctc=23.5
Validation average objf: -6.708422 over 5505.0 utts
10epoch:train:4000-4100batch: loss=17.36, loss_att=14.3, loss_ctc=24.49
10epoch:train:4100-4200batch: loss=17.15, loss_att=14.19, loss_ctc=24.06
10epoch:train:4200-4300batch: loss=17.17, loss_att=14.19, loss_ctc=24.13
10epoch:train:4300-4400batch: loss=17.61, loss_att=14.66, loss_ctc=24.51
10epoch:train:4400-4500batch: loss=17.66, loss_att=14.63, loss_ctc=24.73
10epoch:train:4500-4600batch: loss=16.93, loss_att=14.01, loss_ctc=23.75
10epoch:train:4600-4700batch: loss=16.13, loss_att=13.33, loss_ctc=22.68
10epoch:train:4700-4800batch: loss=17.22, loss_att=14.16, loss_ctc=24.37
10epoch:train:4800-4900batch: loss=16.85, loss_att=13.99, loss_ctc=23.52
10epoch:train:4900-5000batch: loss=16.85, loss_att=13.92, loss_ctc=23.69
Validation average objf: -6.789656 over 5505.0 utts
10epoch:train:5000-5100batch: loss=17.23, loss_att=14.25, loss_ctc=24.18
10epoch:train:5100-5200batch: loss=16.46, loss_att=13.62, loss_ctc=23.08
10epoch:train:5200-5300batch: loss=17.1, loss_att=14.1, loss_ctc=24.11
10epoch:train:5300-5400batch: loss=16.64, loss_att=13.71, loss_ctc=23.49
10epoch:train:5400-5500batch: loss=17.15, loss_att=14.08, loss_ctc=24.33
10epoch:train:5500-5600batch: loss=16.6, loss_att=13.72, loss_ctc=23.31
10epoch:train:5600-5700batch: loss=16.54, loss_att=13.74, loss_ctc=23.07
10epoch:train:5700-5800batch: loss=17.32, loss_att=14.38, loss_ctc=24.18
10epoch:train:5800-5900batch: loss=17.22, loss_att=14.2, loss_ctc=24.26
10epoch:train:5900-6000batch: loss=16.8, loss_att=13.96, loss_ctc=23.42
Validation average objf: -6.564147 over 5505.0 utts
10epoch:train:6000-6100batch: loss=17.09, loss_att=14.2, loss_ctc=23.84
10epoch:train:6100-6200batch: loss=17.2, loss_att=14.21, loss_ctc=24.2
10epoch:train:6200-6300batch: loss=16.56, loss_att=13.73, loss_ctc=23.17
10epoch:train:6300-6400batch: loss=16.44, loss_att=13.6, loss_ctc=23.08
10epoch:train:6400-6500batch: loss=16.53, loss_att=13.69, loss_ctc=23.16
10epoch:train:6500-6600batch: loss=16.51, loss_att=13.69, loss_ctc=23.1
10epoch:train:6600-6700batch: loss=17.41, loss_att=14.45, loss_ctc=24.33
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-10.pt: epoch=10, learning_rate=0.0017906177912111107, objf=-1.0741295121612289, valid_objf=-6.564146940281562
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-10-info
epoch 11, learning rate 0.0016987333747612787
11epoch:train:0-0batch: loss=13.53, loss_att=11.16, loss_ctc=19.06
11epoch:train:0-100batch: loss=14.7, loss_att=12.16, loss_ctc=20.65
11epoch:train:100-200batch: loss=15.31, loss_att=12.67, loss_ctc=21.45
11epoch:train:200-300batch: loss=14.44, loss_att=11.96, loss_ctc=20.2
11epoch:train:300-400batch: loss=14.22, loss_att=11.79, loss_ctc=19.89
11epoch:train:400-500batch: loss=13.65, loss_att=11.34, loss_ctc=19.05
11epoch:train:500-600batch: loss=13.8, loss_att=11.36, loss_ctc=19.48
11epoch:train:600-700batch: loss=13.4, loss_att=11.01, loss_ctc=18.97
11epoch:train:700-800batch: loss=14.48, loss_att=11.91, loss_ctc=20.47
11epoch:train:800-900batch: loss=14.28, loss_att=11.78, loss_ctc=20.11
11epoch:train:900-1000batch: loss=14.01, loss_att=11.58, loss_ctc=19.68
Validation average objf: -6.601133 over 5505.0 utts
11epoch:train:1000-1100batch: loss=14.82, loss_att=12.22, loss_ctc=20.86
11epoch:train:1100-1200batch: loss=15.17, loss_att=12.58, loss_ctc=21.22
11epoch:train:1200-1300batch: loss=14.64, loss_att=12.04, loss_ctc=20.69
11epoch:train:1300-1400batch: loss=14.25, loss_att=11.75, loss_ctc=20.1
11epoch:train:1400-1500batch: loss=14.14, loss_att=11.7, loss_ctc=19.82
11epoch:train:1500-1600batch: loss=15.19, loss_att=12.49, loss_ctc=21.5
11epoch:train:1600-1700batch: loss=15.27, loss_att=12.6, loss_ctc=21.51
11epoch:train:1700-1800batch: loss=13.92, loss_att=11.42, loss_ctc=19.77
11epoch:train:1800-1900batch: loss=14.5, loss_att=11.96, loss_ctc=20.44
11epoch:train:1900-2000batch: loss=14.46, loss_att=11.92, loss_ctc=20.39
Validation average objf: -6.568173 over 5505.0 utts
11epoch:train:2000-2100batch: loss=15.42, loss_att=12.71, loss_ctc=21.73
11epoch:train:2100-2200batch: loss=15.18, loss_att=12.53, loss_ctc=21.36
11epoch:train:2200-2300batch: loss=16.07, loss_att=13.26, loss_ctc=22.63
11epoch:train:2300-2400batch: loss=16.12, loss_att=13.25, loss_ctc=22.83
11epoch:train:2400-2500batch: loss=15.59, loss_att=12.9, loss_ctc=21.85
11epoch:train:2500-2600batch: loss=15.39, loss_att=12.65, loss_ctc=21.76
11epoch:train:2600-2700batch: loss=14.79, loss_att=12.28, loss_ctc=20.62
11epoch:train:2700-2800batch: loss=15.43, loss_att=12.74, loss_ctc=21.69
11epoch:train:2800-2900batch: loss=15.47, loss_att=12.8, loss_ctc=21.68
11epoch:train:2900-3000batch: loss=16.09, loss_att=13.26, loss_ctc=22.7
Validation average objf: -6.622035 over 5505.0 utts
11epoch:train:3000-3100batch: loss=15.42, loss_att=12.77, loss_ctc=21.59
11epoch:train:3100-3200batch: loss=16.22, loss_att=13.43, loss_ctc=22.72
11epoch:train:3200-3300batch: loss=15.5, loss_att=12.8, loss_ctc=21.8
11epoch:train:3300-3400batch: loss=16.01, loss_att=13.22, loss_ctc=22.5
11epoch:train:3400-3500batch: loss=15.11, loss_att=12.53, loss_ctc=21.14
11epoch:train:3500-3600batch: loss=16.14, loss_att=13.24, loss_ctc=22.9
11epoch:train:3600-3700batch: loss=15.54, loss_att=12.83, loss_ctc=21.86
11epoch:train:3700-3800batch: loss=16.33, loss_att=13.5, loss_ctc=22.94
11epoch:train:3800-3900batch: loss=15.68, loss_att=13.0, loss_ctc=21.93
11epoch:train:3900-4000batch: loss=16.74, loss_att=13.94, loss_ctc=23.29
Validation average objf: -6.683002 over 5505.0 utts
11epoch:train:4000-4100batch: loss=17.01, loss_att=13.94, loss_ctc=24.17
11epoch:train:4100-4200batch: loss=15.69, loss_att=12.97, loss_ctc=22.04
11epoch:train:4200-4300batch: loss=15.48, loss_att=12.78, loss_ctc=21.76
11epoch:train:4300-4400batch: loss=16.66, loss_att=13.8, loss_ctc=23.33
11epoch:train:4400-4500batch: loss=15.42, loss_att=12.86, loss_ctc=21.4
11epoch:train:4500-4600batch: loss=16.42, loss_att=13.54, loss_ctc=23.14
11epoch:train:4600-4700batch: loss=16.0, loss_att=13.2, loss_ctc=22.51
11epoch:train:4700-4800batch: loss=16.53, loss_att=13.65, loss_ctc=23.22
11epoch:train:4800-4900batch: loss=16.18, loss_att=13.23, loss_ctc=23.05
11epoch:train:4900-5000batch: loss=15.88, loss_att=13.14, loss_ctc=22.26
Validation average objf: -6.530506 over 5505.0 utts
11epoch:train:5000-5100batch: loss=15.81, loss_att=13.06, loss_ctc=22.23
11epoch:train:5100-5200batch: loss=16.42, loss_att=13.53, loss_ctc=23.17
11epoch:train:5200-5300batch: loss=16.26, loss_att=13.44, loss_ctc=22.85
11epoch:train:5300-5400batch: loss=16.83, loss_att=13.85, loss_ctc=23.77
11epoch:train:5400-5500batch: loss=16.69, loss_att=13.71, loss_ctc=23.65
11epoch:train:5500-5600batch: loss=16.42, loss_att=13.51, loss_ctc=23.2
11epoch:train:5600-5700batch: loss=16.65, loss_att=13.82, loss_ctc=23.26
11epoch:train:5700-5800batch: loss=16.24, loss_att=13.34, loss_ctc=23.0
11epoch:train:5800-5900batch: loss=16.13, loss_att=13.34, loss_ctc=22.64
11epoch:train:5900-6000batch: loss=16.52, loss_att=13.78, loss_ctc=22.91
Validation average objf: -6.547285 over 5505.0 utts
11epoch:train:6000-6100batch: loss=16.74, loss_att=13.8, loss_ctc=23.61
11epoch:train:6100-6200batch: loss=15.58, loss_att=12.83, loss_ctc=21.98
11epoch:train:6200-6300batch: loss=15.73, loss_att=12.97, loss_ctc=22.18
11epoch:train:6300-6400batch: loss=16.33, loss_att=13.48, loss_ctc=22.97
11epoch:train:6400-6500batch: loss=15.45, loss_att=12.83, loss_ctc=21.57
11epoch:train:6500-6600batch: loss=16.08, loss_att=13.27, loss_ctc=22.65
11epoch:train:6600-6700batch: loss=15.28, loss_att=12.65, loss_ctc=21.41
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-11.pt: epoch=11, learning_rate=0.0016987333747612787, objf=-1.021285924856963, valid_objf=-6.547285138510445
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-11-info
epoch 12, learning rate 0.0016196711077852487
12epoch:train:0-0batch: loss=22.55, loss_att=18.54, loss_ctc=31.9
12epoch:train:0-100batch: loss=14.52, loss_att=11.96, loss_ctc=20.49
12epoch:train:100-200batch: loss=14.23, loss_att=11.72, loss_ctc=20.1
12epoch:train:200-300batch: loss=14.09, loss_att=11.64, loss_ctc=19.79
12epoch:train:300-400batch: loss=14.63, loss_att=12.1, loss_ctc=20.53
12epoch:train:400-500batch: loss=13.66, loss_att=11.31, loss_ctc=19.15
12epoch:train:500-600batch: loss=12.63, loss_att=10.4, loss_ctc=17.82
12epoch:train:600-700batch: loss=14.7, loss_att=12.09, loss_ctc=20.81
12epoch:train:700-800batch: loss=14.62, loss_att=11.99, loss_ctc=20.76
12epoch:train:800-900batch: loss=13.18, loss_att=10.87, loss_ctc=18.58
12epoch:train:900-1000batch: loss=13.66, loss_att=11.28, loss_ctc=19.23
Validation average objf: -6.526819 over 5505.0 utts
12epoch:train:1000-1100batch: loss=13.26, loss_att=10.92, loss_ctc=18.75
12epoch:train:1100-1200batch: loss=13.3, loss_att=10.97, loss_ctc=18.74
12epoch:train:1200-1300batch: loss=13.82, loss_att=11.38, loss_ctc=19.51
12epoch:train:1300-1400batch: loss=15.56, loss_att=12.82, loss_ctc=21.95
12epoch:train:1400-1500batch: loss=14.19, loss_att=11.69, loss_ctc=20.03
12epoch:train:1500-1600batch: loss=14.43, loss_att=11.88, loss_ctc=20.37
12epoch:train:1600-1700batch: loss=14.76, loss_att=12.2, loss_ctc=20.73
12epoch:train:1700-1800batch: loss=13.76, loss_att=11.41, loss_ctc=19.24
12epoch:train:1800-1900batch: loss=13.71, loss_att=11.25, loss_ctc=19.44
12epoch:train:1900-2000batch: loss=13.73, loss_att=11.31, loss_ctc=19.39
Validation average objf: -6.433008 over 5505.0 utts
12epoch:train:2000-2100batch: loss=15.25, loss_att=12.59, loss_ctc=21.45
12epoch:train:2100-2200batch: loss=14.82, loss_att=12.16, loss_ctc=21.03
12epoch:train:2200-2300batch: loss=15.45, loss_att=12.79, loss_ctc=21.66
12epoch:train:2300-2400batch: loss=14.83, loss_att=12.23, loss_ctc=20.88
12epoch:train:2400-2500batch: loss=15.02, loss_att=12.35, loss_ctc=21.23
12epoch:train:2500-2600batch: loss=14.86, loss_att=12.18, loss_ctc=21.11
12epoch:train:2600-2700batch: loss=15.66, loss_att=12.88, loss_ctc=22.13
12epoch:train:2700-2800batch: loss=15.16, loss_att=12.53, loss_ctc=21.29
12epoch:train:2800-2900batch: loss=14.43, loss_att=11.89, loss_ctc=20.37
12epoch:train:2900-3000batch: loss=14.37, loss_att=11.8, loss_ctc=20.36
Validation average objf: -6.454897 over 5505.0 utts
12epoch:train:3000-3100batch: loss=15.14, loss_att=12.5, loss_ctc=21.29
12epoch:train:3100-3200batch: loss=15.57, loss_att=12.78, loss_ctc=22.06
12epoch:train:3200-3300batch: loss=15.05, loss_att=12.52, loss_ctc=20.98
12epoch:train:3300-3400batch: loss=14.71, loss_att=12.15, loss_ctc=20.67
12epoch:train:3400-3500batch: loss=14.54, loss_att=12.0, loss_ctc=20.46
12epoch:train:3500-3600batch: loss=15.83, loss_att=13.09, loss_ctc=22.23
12epoch:train:3600-3700batch: loss=15.37, loss_att=12.72, loss_ctc=21.55
12epoch:train:3700-3800batch: loss=14.83, loss_att=12.18, loss_ctc=21.01
12epoch:train:3800-3900batch: loss=14.7, loss_att=12.19, loss_ctc=20.56
12epoch:train:3900-4000batch: loss=15.31, loss_att=12.67, loss_ctc=21.47
Validation average objf: -6.549896 over 5505.0 utts
12epoch:train:4000-4100batch: loss=15.02, loss_att=12.42, loss_ctc=21.1
12epoch:train:4100-4200batch: loss=14.86, loss_att=12.32, loss_ctc=20.77
12epoch:train:4200-4300batch: loss=14.85, loss_att=12.31, loss_ctc=20.76
12epoch:train:4300-4400batch: loss=15.79, loss_att=13.08, loss_ctc=22.13
12epoch:train:4400-4500batch: loss=15.63, loss_att=12.79, loss_ctc=22.25
12epoch:train:4500-4600batch: loss=15.33, loss_att=12.71, loss_ctc=21.45
12epoch:train:4600-4700batch: loss=15.65, loss_att=12.99, loss_ctc=21.87
12epoch:train:4700-4800batch: loss=15.32, loss_att=12.65, loss_ctc=21.57
12epoch:train:4800-4900batch: loss=14.84, loss_att=12.25, loss_ctc=20.89
12epoch:train:4900-5000batch: loss=15.82, loss_att=12.98, loss_ctc=22.46
Validation average objf: -6.623385 over 5505.0 utts
12epoch:train:5000-5100batch: loss=15.57, loss_att=12.86, loss_ctc=21.88
12epoch:train:5100-5200batch: loss=15.1, loss_att=12.54, loss_ctc=21.08
12epoch:train:5200-5300batch: loss=15.83, loss_att=13.15, loss_ctc=22.08
12epoch:train:5300-5400batch: loss=15.33, loss_att=12.73, loss_ctc=21.38
12epoch:train:5400-5500batch: loss=15.63, loss_att=12.91, loss_ctc=21.96
12epoch:train:5500-5600batch: loss=14.67, loss_att=12.27, loss_ctc=20.26
12epoch:train:5600-5700batch: loss=15.17, loss_att=12.58, loss_ctc=21.2
12epoch:train:5700-5800batch: loss=16.07, loss_att=13.3, loss_ctc=22.54
12epoch:train:5800-5900batch: loss=15.37, loss_att=12.72, loss_ctc=21.55
12epoch:train:5900-6000batch: loss=15.56, loss_att=12.88, loss_ctc=21.82
Validation average objf: -6.354122 over 5505.0 utts
12epoch:train:6000-6100batch: loss=15.74, loss_att=12.97, loss_ctc=22.2
12epoch:train:6100-6200batch: loss=14.86, loss_att=12.3, loss_ctc=20.84
12epoch:train:6200-6300batch: loss=14.86, loss_att=12.15, loss_ctc=21.17
12epoch:train:6300-6400batch: loss=14.66, loss_att=12.06, loss_ctc=20.73
12epoch:train:6400-6500batch: loss=15.01, loss_att=12.4, loss_ctc=21.1
12epoch:train:6500-6600batch: loss=15.68, loss_att=12.9, loss_ctc=22.16
12epoch:train:6600-6700batch: loss=15.36, loss_att=12.67, loss_ctc=21.63
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-12.pt: epoch=12, learning_rate=0.0016196711077852487, objf=-0.9772352678839662, valid_objf=-6.354121962988192
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-12-info
epoch 13, learning rate 0.0015507204956572017
13epoch:train:0-0batch: loss=18.85, loss_att=15.08, loss_ctc=27.66
13epoch:train:0-100batch: loss=13.24, loss_att=10.92, loss_ctc=18.65
13epoch:train:100-200batch: loss=14.39, loss_att=11.79, loss_ctc=20.46
13epoch:train:200-300batch: loss=14.04, loss_att=11.56, loss_ctc=19.84
13epoch:train:300-400batch: loss=12.94, loss_att=10.64, loss_ctc=18.3
13epoch:train:400-500batch: loss=13.98, loss_att=11.52, loss_ctc=19.73
13epoch:train:500-600batch: loss=12.96, loss_att=10.63, loss_ctc=18.41
13epoch:train:600-700batch: loss=13.8, loss_att=11.34, loss_ctc=19.55
13epoch:train:700-800batch: loss=13.54, loss_att=11.1, loss_ctc=19.22
13epoch:train:800-900batch: loss=14.18, loss_att=11.59, loss_ctc=20.23
13epoch:train:900-1000batch: loss=13.56, loss_att=11.13, loss_ctc=19.24
Validation average objf: -6.348548 over 5505.0 utts
13epoch:train:1000-1100batch: loss=12.8, loss_att=10.51, loss_ctc=18.15
13epoch:train:1100-1200batch: loss=13.82, loss_att=11.35, loss_ctc=19.6
13epoch:train:1200-1300batch: loss=13.76, loss_att=11.34, loss_ctc=19.41
13epoch:train:1300-1400batch: loss=12.84, loss_att=10.56, loss_ctc=18.16
13epoch:train:1400-1500batch: loss=12.67, loss_att=10.38, loss_ctc=17.99
13epoch:train:1500-1600batch: loss=13.73, loss_att=11.27, loss_ctc=19.45
13epoch:train:1600-1700batch: loss=14.66, loss_att=12.09, loss_ctc=20.65
13epoch:train:1700-1800batch: loss=14.5, loss_att=11.91, loss_ctc=20.53
13epoch:train:1800-1900batch: loss=13.82, loss_att=11.4, loss_ctc=19.46
13epoch:train:1900-2000batch: loss=14.48, loss_att=11.95, loss_ctc=20.38
Validation average objf: -6.440266 over 5505.0 utts
13epoch:train:2000-2100batch: loss=14.99, loss_att=12.28, loss_ctc=21.31
13epoch:train:2100-2200batch: loss=14.26, loss_att=11.81, loss_ctc=19.97
13epoch:train:2200-2300batch: loss=13.61, loss_att=11.26, loss_ctc=19.08
13epoch:train:2300-2400batch: loss=15.12, loss_att=12.48, loss_ctc=21.28
13epoch:train:2400-2500batch: loss=14.67, loss_att=12.04, loss_ctc=20.82
13epoch:train:2500-2600batch: loss=14.74, loss_att=12.14, loss_ctc=20.8
13epoch:train:2600-2700batch: loss=14.28, loss_att=11.75, loss_ctc=20.17
13epoch:train:2700-2800batch: loss=14.11, loss_att=11.64, loss_ctc=19.88
13epoch:train:2800-2900batch: loss=14.54, loss_att=11.98, loss_ctc=20.51
13epoch:train:2900-3000batch: loss=14.37, loss_att=11.83, loss_ctc=20.31
Validation average objf: -6.333218 over 5505.0 utts
13epoch:train:3000-3100batch: loss=14.65, loss_att=12.09, loss_ctc=20.64
13epoch:train:3100-3200batch: loss=14.57, loss_att=12.04, loss_ctc=20.49
13epoch:train:3200-3300batch: loss=14.27, loss_att=11.67, loss_ctc=20.33
13epoch:train:3300-3400batch: loss=15.32, loss_att=12.6, loss_ctc=21.68
13epoch:train:3400-3500batch: loss=14.04, loss_att=11.58, loss_ctc=19.8
13epoch:train:3500-3600batch: loss=14.96, loss_att=12.27, loss_ctc=21.24
13epoch:train:3600-3700batch: loss=14.91, loss_att=12.34, loss_ctc=20.93
13epoch:train:3700-3800batch: loss=14.71, loss_att=12.15, loss_ctc=20.67
13epoch:train:3800-3900batch: loss=15.01, loss_att=12.3, loss_ctc=21.33
13epoch:train:3900-4000batch: loss=14.3, loss_att=11.8, loss_ctc=20.15
Validation average objf: -6.268123 over 5505.0 utts
13epoch:train:4000-4100batch: loss=14.88, loss_att=12.29, loss_ctc=20.91
13epoch:train:4100-4200batch: loss=14.84, loss_att=12.24, loss_ctc=20.92
13epoch:train:4200-4300batch: loss=14.81, loss_att=12.33, loss_ctc=20.62
13epoch:train:4300-4400batch: loss=15.4, loss_att=12.7, loss_ctc=21.71
13epoch:train:4400-4500batch: loss=15.24, loss_att=12.53, loss_ctc=21.55
13epoch:train:4500-4600batch: loss=15.26, loss_att=12.53, loss_ctc=21.65
13epoch:train:4600-4700batch: loss=15.03, loss_att=12.37, loss_ctc=21.26
13epoch:train:4700-4800batch: loss=14.62, loss_att=12.06, loss_ctc=20.59
13epoch:train:4800-4900batch: loss=15.16, loss_att=12.47, loss_ctc=21.44
13epoch:train:4900-5000batch: loss=14.55, loss_att=12.12, loss_ctc=20.23
Validation average objf: -6.396657 over 5505.0 utts
13epoch:train:5000-5100batch: loss=14.66, loss_att=12.15, loss_ctc=20.51
13epoch:train:5100-5200batch: loss=15.45, loss_att=12.78, loss_ctc=21.67
13epoch:train:5200-5300batch: loss=14.7, loss_att=12.12, loss_ctc=20.74
13epoch:train:5300-5400batch: loss=14.96, loss_att=12.36, loss_ctc=21.02
13epoch:train:5400-5500batch: loss=14.61, loss_att=12.11, loss_ctc=20.47
13epoch:train:5500-5600batch: loss=15.54, loss_att=12.79, loss_ctc=21.94
13epoch:train:5600-5700batch: loss=14.65, loss_att=12.08, loss_ctc=20.64
13epoch:train:5700-5800batch: loss=14.82, loss_att=12.27, loss_ctc=20.76
13epoch:train:5800-5900batch: loss=15.14, loss_att=12.51, loss_ctc=21.26
13epoch:train:5900-6000batch: loss=15.67, loss_att=12.95, loss_ctc=22.01
Validation average objf: -6.348167 over 5505.0 utts
13epoch:train:6000-6100batch: loss=15.15, loss_att=12.5, loss_ctc=21.33
13epoch:train:6100-6200batch: loss=14.24, loss_att=11.78, loss_ctc=19.97
13epoch:train:6200-6300batch: loss=15.71, loss_att=13.01, loss_ctc=22.0
13epoch:train:6300-6400batch: loss=14.68, loss_att=12.13, loss_ctc=20.64
13epoch:train:6400-6500batch: loss=14.93, loss_att=12.29, loss_ctc=21.1
13epoch:train:6500-6600batch: loss=15.03, loss_att=12.38, loss_ctc=21.24
13epoch:train:6600-6700batch: loss=14.63, loss_att=12.13, loss_ctc=20.48
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-13.pt: epoch=13, learning_rate=0.0015507204956572017, objf=-0.9516774780765902, valid_objf=-6.3481671491825615
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-13-info
epoch 14, learning rate 0.001489886882356062
14epoch:train:0-0batch: loss=11.76, loss_att=9.528, loss_ctc=16.97
14epoch:train:0-100batch: loss=13.52, loss_att=11.15, loss_ctc=19.04
14epoch:train:100-200batch: loss=14.01, loss_att=11.54, loss_ctc=19.78
14epoch:train:200-300batch: loss=12.42, loss_att=10.25, loss_ctc=17.47
14epoch:train:300-400batch: loss=13.27, loss_att=10.89, loss_ctc=18.82
14epoch:train:400-500batch: loss=13.24, loss_att=10.91, loss_ctc=18.7
14epoch:train:500-600batch: loss=12.84, loss_att=10.51, loss_ctc=18.27
14epoch:train:600-700batch: loss=12.94, loss_att=10.66, loss_ctc=18.27
14epoch:train:700-800batch: loss=13.16, loss_att=10.8, loss_ctc=18.67
14epoch:train:800-900batch: loss=12.56, loss_att=10.27, loss_ctc=17.91
14epoch:train:900-1000batch: loss=12.81, loss_att=10.52, loss_ctc=18.16
Validation average objf: -6.123462 over 5505.0 utts
14epoch:train:1000-1100batch: loss=12.66, loss_att=10.43, loss_ctc=17.88
14epoch:train:1100-1200batch: loss=12.5, loss_att=10.25, loss_ctc=17.72
14epoch:train:1200-1300batch: loss=13.71, loss_att=11.25, loss_ctc=19.44
14epoch:train:1300-1400batch: loss=13.83, loss_att=11.35, loss_ctc=19.61
14epoch:train:1400-1500batch: loss=14.04, loss_att=11.6, loss_ctc=19.75
14epoch:train:1500-1600batch: loss=13.07, loss_att=10.71, loss_ctc=18.58
14epoch:train:1600-1700batch: loss=13.22, loss_att=10.91, loss_ctc=18.6
14epoch:train:1700-1800batch: loss=13.48, loss_att=11.13, loss_ctc=18.96
14epoch:train:1800-1900batch: loss=12.92, loss_att=10.55, loss_ctc=18.48
14epoch:train:1900-2000batch: loss=13.71, loss_att=11.24, loss_ctc=19.46
Validation average objf: -6.119665 over 5505.0 utts
14epoch:train:2000-2100batch: loss=12.74, loss_att=10.45, loss_ctc=18.06
14epoch:train:2100-2200batch: loss=13.57, loss_att=11.24, loss_ctc=19.01
14epoch:train:2200-2300batch: loss=13.35, loss_att=11.03, loss_ctc=18.76
14epoch:train:2300-2400batch: loss=14.13, loss_att=11.56, loss_ctc=20.14
14epoch:train:2400-2500batch: loss=13.83, loss_att=11.45, loss_ctc=19.37
14epoch:train:2500-2600batch: loss=13.46, loss_att=11.08, loss_ctc=18.99
14epoch:train:2600-2700batch: loss=13.93, loss_att=11.5, loss_ctc=19.6
14epoch:train:2700-2800batch: loss=14.51, loss_att=11.98, loss_ctc=20.42
14epoch:train:2800-2900batch: loss=14.0, loss_att=11.63, loss_ctc=19.53
14epoch:train:2900-3000batch: loss=14.02, loss_att=11.6, loss_ctc=19.69
Validation average objf: -6.305307 over 5505.0 utts
14epoch:train:3000-3100batch: loss=13.64, loss_att=11.25, loss_ctc=19.23
14epoch:train:3100-3200batch: loss=13.89, loss_att=11.48, loss_ctc=19.51
14epoch:train:3200-3300batch: loss=14.71, loss_att=12.18, loss_ctc=20.62
14epoch:train:3300-3400batch: loss=13.79, loss_att=11.38, loss_ctc=19.41
14epoch:train:3400-3500batch: loss=13.8, loss_att=11.43, loss_ctc=19.32
14epoch:train:3500-3600batch: loss=15.15, loss_att=12.46, loss_ctc=21.45
14epoch:train:3600-3700batch: loss=14.13, loss_att=11.72, loss_ctc=19.77
14epoch:train:3700-3800batch: loss=14.97, loss_att=12.31, loss_ctc=21.17
14epoch:train:3800-3900batch: loss=14.96, loss_att=12.42, loss_ctc=20.88
14epoch:train:3900-4000batch: loss=14.39, loss_att=11.91, loss_ctc=20.17
Validation average objf: -6.281879 over 5505.0 utts
14epoch:train:4000-4100batch: loss=14.76, loss_att=12.17, loss_ctc=20.8
14epoch:train:4100-4200batch: loss=14.81, loss_att=12.15, loss_ctc=21.02
14epoch:train:4200-4300batch: loss=14.75, loss_att=12.21, loss_ctc=20.69
14epoch:train:4300-4400batch: loss=14.6, loss_att=11.99, loss_ctc=20.71
14epoch:train:4400-4500batch: loss=14.3, loss_att=11.78, loss_ctc=20.17
14epoch:train:4500-4600batch: loss=14.23, loss_att=11.72, loss_ctc=20.09
14epoch:train:4600-4700batch: loss=14.53, loss_att=11.98, loss_ctc=20.46
14epoch:train:4700-4800batch: loss=14.88, loss_att=12.29, loss_ctc=20.92
14epoch:train:4800-4900batch: loss=13.9, loss_att=11.53, loss_ctc=19.41
14epoch:train:4900-5000batch: loss=14.3, loss_att=11.85, loss_ctc=20.02
Validation average objf: -6.282878 over 5505.0 utts
14epoch:train:5000-5100batch: loss=14.71, loss_att=12.08, loss_ctc=20.83
14epoch:train:5100-5200batch: loss=14.85, loss_att=12.3, loss_ctc=20.79
14epoch:train:5200-5300batch: loss=14.69, loss_att=12.11, loss_ctc=20.7
14epoch:train:5300-5400batch: loss=14.5, loss_att=12.05, loss_ctc=20.22
14epoch:train:5400-5500batch: loss=14.61, loss_att=12.04, loss_ctc=20.62
14epoch:train:5500-5600batch: loss=14.87, loss_att=12.31, loss_ctc=20.84
14epoch:train:5600-5700batch: loss=14.65, loss_att=12.03, loss_ctc=20.77
14epoch:train:5700-5800batch: loss=14.19, loss_att=11.68, loss_ctc=20.06
14epoch:train:5800-5900batch: loss=15.35, loss_att=12.66, loss_ctc=21.62
14epoch:train:5900-6000batch: loss=14.78, loss_att=12.2, loss_ctc=20.8
Validation average objf: -6.178938 over 5505.0 utts
14epoch:train:6000-6100batch: loss=14.49, loss_att=12.04, loss_ctc=20.19
14epoch:train:6100-6200batch: loss=15.01, loss_att=12.27, loss_ctc=21.41
14epoch:train:6200-6300batch: loss=14.62, loss_att=12.12, loss_ctc=20.46
14epoch:train:6300-6400batch: loss=14.7, loss_att=12.12, loss_ctc=20.71
14epoch:train:6400-6500batch: loss=14.28, loss_att=11.81, loss_ctc=20.05
14epoch:train:6500-6600batch: loss=14.27, loss_att=11.83, loss_ctc=19.97
14epoch:train:6600-6700batch: loss=14.95, loss_att=12.35, loss_ctc=21.0
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-14.pt: epoch=14, learning_rate=0.001489886882356062, objf=-0.918991898302522, valid_objf=-6.17893818119891
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-14-info
epoch 15, learning rate 0.0014356856787960126
15epoch:train:0-0batch: loss=15.09, loss_att=12.42, loss_ctc=21.33
15epoch:train:0-100batch: loss=12.47, loss_att=10.29, loss_ctc=17.55
15epoch:train:100-200batch: loss=12.68, loss_att=10.41, loss_ctc=17.97
15epoch:train:200-300batch: loss=13.3, loss_att=10.86, loss_ctc=19.0
15epoch:train:300-400batch: loss=12.65, loss_att=10.36, loss_ctc=17.99
15epoch:train:400-500batch: loss=13.17, loss_att=10.83, loss_ctc=18.62
15epoch:train:500-600batch: loss=12.61, loss_att=10.4, loss_ctc=17.79
15epoch:train:600-700batch: loss=12.46, loss_att=10.21, loss_ctc=17.7
15epoch:train:700-800batch: loss=12.77, loss_att=10.45, loss_ctc=18.17
15epoch:train:800-900batch: loss=12.76, loss_att=10.47, loss_ctc=18.1
15epoch:train:900-1000batch: loss=12.2, loss_att=9.973, loss_ctc=17.39
Validation average objf: -6.056180 over 5505.0 utts
15epoch:train:1000-1100batch: loss=12.65, loss_att=10.4, loss_ctc=17.91
15epoch:train:1100-1200batch: loss=13.29, loss_att=10.89, loss_ctc=18.9
15epoch:train:1200-1300batch: loss=13.13, loss_att=10.84, loss_ctc=18.49
15epoch:train:1300-1400batch: loss=13.25, loss_att=10.89, loss_ctc=18.75
15epoch:train:1400-1500batch: loss=13.44, loss_att=11.02, loss_ctc=19.06
15epoch:train:1500-1600batch: loss=13.57, loss_att=11.2, loss_ctc=19.1
15epoch:train:1600-1700batch: loss=13.79, loss_att=11.38, loss_ctc=19.4
15epoch:train:1700-1800batch: loss=12.61, loss_att=10.39, loss_ctc=17.79
15epoch:train:1800-1900batch: loss=13.99, loss_att=11.55, loss_ctc=19.7
15epoch:train:1900-2000batch: loss=13.69, loss_att=11.31, loss_ctc=19.23
Validation average objf: -6.159930 over 5505.0 utts
15epoch:train:2000-2100batch: loss=13.85, loss_att=11.35, loss_ctc=19.7
15epoch:train:2100-2200batch: loss=13.0, loss_att=10.68, loss_ctc=18.42
15epoch:train:2200-2300batch: loss=13.44, loss_att=11.14, loss_ctc=18.8
15epoch:train:2300-2400batch: loss=13.29, loss_att=10.83, loss_ctc=19.03
15epoch:train:2400-2500batch: loss=13.15, loss_att=10.76, loss_ctc=18.72
15epoch:train:2500-2600batch: loss=13.68, loss_att=11.31, loss_ctc=19.22
15epoch:train:2600-2700batch: loss=13.86, loss_att=11.37, loss_ctc=19.66
15epoch:train:2700-2800batch: loss=13.36, loss_att=10.97, loss_ctc=18.95
15epoch:train:2800-2900batch: loss=13.69, loss_att=11.31, loss_ctc=19.24
15epoch:train:2900-3000batch: loss=13.34, loss_att=11.0, loss_ctc=18.82
Validation average objf: -6.156031 over 5505.0 utts
15epoch:train:3000-3100batch: loss=13.81, loss_att=11.33, loss_ctc=19.57
15epoch:train:3100-3200batch: loss=14.24, loss_att=11.67, loss_ctc=20.23
15epoch:train:3200-3300batch: loss=13.32, loss_att=11.04, loss_ctc=18.63
15epoch:train:3300-3400batch: loss=13.66, loss_att=11.34, loss_ctc=19.08
15epoch:train:3400-3500batch: loss=13.92, loss_att=11.48, loss_ctc=19.61
15epoch:train:3500-3600batch: loss=13.71, loss_att=11.33, loss_ctc=19.25
15epoch:train:3600-3700batch: loss=13.48, loss_att=11.14, loss_ctc=18.93
15epoch:train:3700-3800batch: loss=13.99, loss_att=11.52, loss_ctc=19.77
15epoch:train:3800-3900batch: loss=14.35, loss_att=11.86, loss_ctc=20.17
15epoch:train:3900-4000batch: loss=13.9, loss_att=11.4, loss_ctc=19.72
Validation average objf: -6.263027 over 5505.0 utts
15epoch:train:4000-4100batch: loss=14.25, loss_att=11.76, loss_ctc=20.06
15epoch:train:4100-4200batch: loss=13.67, loss_att=11.31, loss_ctc=19.19
15epoch:train:4200-4300batch: loss=14.27, loss_att=11.81, loss_ctc=20.01
15epoch:train:4300-4400batch: loss=14.57, loss_att=12.03, loss_ctc=20.48
15epoch:train:4400-4500batch: loss=14.29, loss_att=11.81, loss_ctc=20.08
15epoch:train:4500-4600batch: loss=14.05, loss_att=11.65, loss_ctc=19.65
15epoch:train:4600-4700batch: loss=14.75, loss_att=12.05, loss_ctc=21.08
15epoch:train:4700-4800batch: loss=14.3, loss_att=11.72, loss_ctc=20.33
15epoch:train:4800-4900batch: loss=14.24, loss_att=11.72, loss_ctc=20.13
15epoch:train:4900-5000batch: loss=14.52, loss_att=11.91, loss_ctc=20.6
Validation average objf: -6.307824 over 5505.0 utts
15epoch:train:5000-5100batch: loss=13.68, loss_att=11.44, loss_ctc=18.89
15epoch:train:5100-5200batch: loss=13.5, loss_att=11.19, loss_ctc=18.9
15epoch:train:5200-5300batch: loss=14.44, loss_att=11.89, loss_ctc=20.39
15epoch:train:5300-5400batch: loss=13.91, loss_att=11.43, loss_ctc=19.71
15epoch:train:5400-5500batch: loss=13.68, loss_att=11.34, loss_ctc=19.14
15epoch:train:5500-5600batch: loss=14.28, loss_att=11.8, loss_ctc=20.06
15epoch:train:5600-5700batch: loss=14.82, loss_att=12.21, loss_ctc=20.9
15epoch:train:5700-5800batch: loss=13.98, loss_att=11.58, loss_ctc=19.57
15epoch:train:5800-5900batch: loss=14.78, loss_att=12.17, loss_ctc=20.87
15epoch:train:5900-6000batch: loss=13.72, loss_att=11.31, loss_ctc=19.32
Validation average objf: -6.217869 over 5505.0 utts
15epoch:train:6000-6100batch: loss=14.3, loss_att=11.74, loss_ctc=20.29
15epoch:train:6100-6200batch: loss=14.27, loss_att=11.73, loss_ctc=20.21
15epoch:train:6200-6300batch: loss=14.12, loss_att=11.7, loss_ctc=19.79
15epoch:train:6300-6400batch: loss=14.39, loss_att=11.88, loss_ctc=20.25
15epoch:train:6400-6500batch: loss=15.2, loss_att=12.5, loss_ctc=21.53
15epoch:train:6500-6600batch: loss=13.36, loss_att=11.1, loss_ctc=18.62
15epoch:train:6600-6700batch: loss=13.38, loss_att=11.01, loss_ctc=18.92
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-15.pt: epoch=15, learning_rate=0.0014356856787960126, objf=-0.8964837411248817, valid_objf=-6.217869408492279
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-15-info
epoch 16, learning rate 0.0013870065769566683
16epoch:train:0-0batch: loss=13.98, loss_att=11.78, loss_ctc=19.14
16epoch:train:0-100batch: loss=12.65, loss_att=10.45, loss_ctc=17.78
16epoch:train:100-200batch: loss=12.41, loss_att=10.26, loss_ctc=17.41
16epoch:train:200-300batch: loss=13.23, loss_att=10.88, loss_ctc=18.73
16epoch:train:300-400batch: loss=12.31, loss_att=10.15, loss_ctc=17.35
16epoch:train:400-500batch: loss=12.47, loss_att=10.22, loss_ctc=17.7
16epoch:train:500-600batch: loss=12.72, loss_att=10.49, loss_ctc=17.93
16epoch:train:600-700batch: loss=12.83, loss_att=10.54, loss_ctc=18.16
16epoch:train:700-800batch: loss=12.27, loss_att=10.12, loss_ctc=17.31
16epoch:train:800-900batch: loss=12.69, loss_att=10.39, loss_ctc=18.05
16epoch:train:900-1000batch: loss=12.56, loss_att=10.39, loss_ctc=17.61
Validation average objf: -6.023819 over 5505.0 utts
16epoch:train:1000-1100batch: loss=12.12, loss_att=9.928, loss_ctc=17.23
16epoch:train:1100-1200batch: loss=12.99, loss_att=10.68, loss_ctc=18.38
16epoch:train:1200-1300batch: loss=11.81, loss_att=9.637, loss_ctc=16.87
16epoch:train:1300-1400batch: loss=11.86, loss_att=9.741, loss_ctc=16.8
16epoch:train:1400-1500batch: loss=12.43, loss_att=10.14, loss_ctc=17.76
16epoch:train:1500-1600batch: loss=12.92, loss_att=10.58, loss_ctc=18.38
16epoch:train:1600-1700batch: loss=13.06, loss_att=10.72, loss_ctc=18.53
16epoch:train:1700-1800batch: loss=13.37, loss_att=10.97, loss_ctc=18.96
16epoch:train:1800-1900batch: loss=12.58, loss_att=10.38, loss_ctc=17.72
16epoch:train:1900-2000batch: loss=13.08, loss_att=10.77, loss_ctc=18.47
Validation average objf: -6.140603 over 5505.0 utts
16epoch:train:2000-2100batch: loss=13.33, loss_att=10.96, loss_ctc=18.85
16epoch:train:2100-2200batch: loss=13.56, loss_att=11.2, loss_ctc=19.05
16epoch:train:2200-2300batch: loss=13.16, loss_att=10.89, loss_ctc=18.45
16epoch:train:2300-2400batch: loss=13.26, loss_att=10.9, loss_ctc=18.76
16epoch:train:2400-2500batch: loss=12.89, loss_att=10.57, loss_ctc=18.32
16epoch:train:2500-2600batch: loss=13.28, loss_att=10.83, loss_ctc=18.99
16epoch:train:2600-2700batch: loss=12.55, loss_att=10.23, loss_ctc=17.97
16epoch:train:2700-2800batch: loss=12.47, loss_att=10.28, loss_ctc=17.58
16epoch:train:2800-2900batch: loss=13.39, loss_att=10.97, loss_ctc=19.03
16epoch:train:2900-3000batch: loss=12.9, loss_att=10.54, loss_ctc=18.4
Validation average objf: -6.052200 over 5505.0 utts
16epoch:train:3000-3100batch: loss=13.62, loss_att=11.14, loss_ctc=19.42
16epoch:train:3100-3200batch: loss=13.1, loss_att=10.75, loss_ctc=18.58
16epoch:train:3200-3300batch: loss=13.58, loss_att=11.17, loss_ctc=19.21
16epoch:train:3300-3400batch: loss=13.61, loss_att=11.17, loss_ctc=19.3
16epoch:train:3400-3500batch: loss=13.86, loss_att=11.44, loss_ctc=19.5
16epoch:train:3500-3600batch: loss=14.6, loss_att=12.04, loss_ctc=20.59
16epoch:train:3600-3700batch: loss=13.69, loss_att=11.23, loss_ctc=19.44
16epoch:train:3700-3800batch: loss=13.75, loss_att=11.21, loss_ctc=19.68
16epoch:train:3800-3900batch: loss=13.54, loss_att=11.17, loss_ctc=19.07
16epoch:train:3900-4000batch: loss=13.67, loss_att=11.34, loss_ctc=19.12
Validation average objf: -6.202943 over 5505.0 utts
16epoch:train:4000-4100batch: loss=12.74, loss_att=10.57, loss_ctc=17.83
16epoch:train:4100-4200batch: loss=12.74, loss_att=10.45, loss_ctc=18.08
16epoch:train:4200-4300batch: loss=14.26, loss_att=11.71, loss_ctc=20.21
16epoch:train:4300-4400batch: loss=13.3, loss_att=10.87, loss_ctc=18.97
16epoch:train:4400-4500batch: loss=13.32, loss_att=11.02, loss_ctc=18.7
16epoch:train:4500-4600batch: loss=13.05, loss_att=10.78, loss_ctc=18.34
16epoch:train:4600-4700batch: loss=13.79, loss_att=11.35, loss_ctc=19.48
16epoch:train:4700-4800batch: loss=12.97, loss_att=10.73, loss_ctc=18.21
16epoch:train:4800-4900batch: loss=13.75, loss_att=11.35, loss_ctc=19.36
16epoch:train:4900-5000batch: loss=13.75, loss_att=11.36, loss_ctc=19.32
Validation average objf: -6.283651 over 5505.0 utts
16epoch:train:5000-5100batch: loss=13.42, loss_att=10.98, loss_ctc=19.13
16epoch:train:5100-5200batch: loss=14.09, loss_att=11.6, loss_ctc=19.9
16epoch:train:5200-5300batch: loss=13.83, loss_att=11.49, loss_ctc=19.3
16epoch:train:5300-5400batch: loss=14.46, loss_att=11.82, loss_ctc=20.62
16epoch:train:5400-5500batch: loss=13.17, loss_att=10.9, loss_ctc=18.48
16epoch:train:5500-5600batch: loss=13.91, loss_att=11.57, loss_ctc=19.35
16epoch:train:5600-5700batch: loss=13.84, loss_att=11.39, loss_ctc=19.58
16epoch:train:5700-5800batch: loss=14.21, loss_att=11.79, loss_ctc=19.87
16epoch:train:5800-5900batch: loss=13.56, loss_att=11.2, loss_ctc=19.08
16epoch:train:5900-6000batch: loss=13.97, loss_att=11.54, loss_ctc=19.62
Validation average objf: -6.188454 over 5505.0 utts
16epoch:train:6000-6100batch: loss=14.17, loss_att=11.68, loss_ctc=19.96
16epoch:train:6100-6200batch: loss=14.44, loss_att=11.84, loss_ctc=20.49
16epoch:train:6200-6300batch: loss=13.46, loss_att=11.13, loss_ctc=18.89
16epoch:train:6300-6400batch: loss=13.38, loss_att=11.14, loss_ctc=18.59
16epoch:train:6400-6500batch: loss=14.62, loss_att=12.06, loss_ctc=20.59
16epoch:train:6500-6600batch: loss=12.97, loss_att=10.75, loss_ctc=18.15
16epoch:train:6600-6700batch: loss=12.99, loss_att=10.75, loss_ctc=18.22
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-16.pt: epoch=16, learning_rate=0.0013870065769566683, objf=-0.8675791280958807, valid_objf=-6.188453678474114
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-16-info
epoch 17, learning rate 0.0013429654102737043
17epoch:train:0-0batch: loss=13.26, loss_att=10.93, loss_ctc=18.69
17epoch:train:0-100batch: loss=12.37, loss_att=10.2, loss_ctc=17.43
17epoch:train:100-200batch: loss=12.29, loss_att=10.09, loss_ctc=17.42
17epoch:train:200-300batch: loss=12.54, loss_att=10.26, loss_ctc=17.85
17epoch:train:300-400batch: loss=11.69, loss_att=9.636, loss_ctc=16.47
17epoch:train:400-500batch: loss=11.89, loss_att=9.745, loss_ctc=16.91
17epoch:train:500-600batch: loss=12.6, loss_att=10.38, loss_ctc=17.78
17epoch:train:600-700batch: loss=12.17, loss_att=10.0, loss_ctc=17.24
17epoch:train:700-800batch: loss=12.33, loss_att=10.1, loss_ctc=17.53
17epoch:train:800-900batch: loss=12.07, loss_att=9.994, loss_ctc=16.91
17epoch:train:900-1000batch: loss=12.3, loss_att=10.1, loss_ctc=17.42
Validation average objf: -6.190349 over 5505.0 utts
17epoch:train:1000-1100batch: loss=11.97, loss_att=9.813, loss_ctc=17.02
17epoch:train:1100-1200batch: loss=11.91, loss_att=9.818, loss_ctc=16.78
17epoch:train:1200-1300batch: loss=11.45, loss_att=9.405, loss_ctc=16.23
17epoch:train:1300-1400batch: loss=12.33, loss_att=10.13, loss_ctc=17.45
17epoch:train:1400-1500batch: loss=12.85, loss_att=10.58, loss_ctc=18.16
17epoch:train:1500-1600batch: loss=13.02, loss_att=10.69, loss_ctc=18.45
17epoch:train:1600-1700batch: loss=12.65, loss_att=10.42, loss_ctc=17.85
17epoch:train:1700-1800batch: loss=12.1, loss_att=9.894, loss_ctc=17.24
17epoch:train:1800-1900batch: loss=12.75, loss_att=10.55, loss_ctc=17.9
17epoch:train:1900-2000batch: loss=12.4, loss_att=10.23, loss_ctc=17.45
Validation average objf: -6.066220 over 5505.0 utts
17epoch:train:2000-2100batch: loss=11.83, loss_att=9.681, loss_ctc=16.85
17epoch:train:2100-2200batch: loss=11.98, loss_att=9.829, loss_ctc=17.0
17epoch:train:2200-2300batch: loss=13.25, loss_att=10.89, loss_ctc=18.77
17epoch:train:2300-2400batch: loss=12.31, loss_att=10.14, loss_ctc=17.37
17epoch:train:2400-2500batch: loss=13.29, loss_att=10.91, loss_ctc=18.83
17epoch:train:2500-2600batch: loss=12.42, loss_att=10.18, loss_ctc=17.65
17epoch:train:2600-2700batch: loss=12.62, loss_att=10.43, loss_ctc=17.71
17epoch:train:2700-2800batch: loss=13.28, loss_att=10.84, loss_ctc=18.96
17epoch:train:2800-2900batch: loss=12.83, loss_att=10.58, loss_ctc=18.07
17epoch:train:2900-3000batch: loss=12.69, loss_att=10.48, loss_ctc=17.84
Validation average objf: -6.141296 over 5505.0 utts
17epoch:train:3000-3100batch: loss=13.01, loss_att=10.75, loss_ctc=18.29
17epoch:train:3100-3200batch: loss=13.16, loss_att=10.84, loss_ctc=18.58
17epoch:train:3200-3300batch: loss=13.15, loss_att=10.9, loss_ctc=18.39
17epoch:train:3300-3400batch: loss=13.2, loss_att=10.95, loss_ctc=18.45
17epoch:train:3400-3500batch: loss=13.48, loss_att=11.16, loss_ctc=18.88
17epoch:train:3500-3600batch: loss=13.09, loss_att=10.83, loss_ctc=18.34
17epoch:train:3600-3700batch: loss=13.35, loss_att=11.03, loss_ctc=18.75
17epoch:train:3700-3800batch: loss=14.2, loss_att=11.65, loss_ctc=20.15
17epoch:train:3800-3900batch: loss=12.88, loss_att=10.67, loss_ctc=18.02
17epoch:train:3900-4000batch: loss=12.98, loss_att=10.75, loss_ctc=18.18
Validation average objf: -6.158696 over 5505.0 utts
17epoch:train:4000-4100batch: loss=13.59, loss_att=11.16, loss_ctc=19.27
17epoch:train:4100-4200batch: loss=13.36, loss_att=11.11, loss_ctc=18.6
17epoch:train:4200-4300batch: loss=13.38, loss_att=11.04, loss_ctc=18.86
17epoch:train:4300-4400batch: loss=13.31, loss_att=10.96, loss_ctc=18.78
17epoch:train:4400-4500batch: loss=12.96, loss_att=10.76, loss_ctc=18.09
17epoch:train:4500-4600batch: loss=13.61, loss_att=11.24, loss_ctc=19.12
17epoch:train:4600-4700batch: loss=13.4, loss_att=11.09, loss_ctc=18.8
17epoch:train:4700-4800batch: loss=13.12, loss_att=10.85, loss_ctc=18.42
17epoch:train:4800-4900batch: loss=13.54, loss_att=11.1, loss_ctc=19.23
17epoch:train:4900-5000batch: loss=13.81, loss_att=11.44, loss_ctc=19.36
Validation average objf: -6.282359 over 5505.0 utts
17epoch:train:5000-5100batch: loss=13.1, loss_att=10.86, loss_ctc=18.34
17epoch:train:5100-5200batch: loss=13.56, loss_att=11.19, loss_ctc=19.1
17epoch:train:5200-5300batch: loss=13.43, loss_att=11.15, loss_ctc=18.74
17epoch:train:5300-5400batch: loss=13.27, loss_att=11.09, loss_ctc=18.36
17epoch:train:5400-5500batch: loss=14.83, loss_att=12.18, loss_ctc=21.02
17epoch:train:5500-5600batch: loss=13.97, loss_att=11.5, loss_ctc=19.74
17epoch:train:5600-5700batch: loss=13.29, loss_att=11.02, loss_ctc=18.57
17epoch:train:5700-5800batch: loss=12.68, loss_att=10.5, loss_ctc=17.76
17epoch:train:5800-5900batch: loss=13.39, loss_att=11.05, loss_ctc=18.87
17epoch:train:5900-6000batch: loss=13.6, loss_att=11.22, loss_ctc=19.15
Validation average objf: -6.062116 over 5505.0 utts
17epoch:train:6000-6100batch: loss=13.07, loss_att=10.86, loss_ctc=18.22
17epoch:train:6100-6200batch: loss=13.23, loss_att=10.97, loss_ctc=18.51
17epoch:train:6200-6300batch: loss=13.25, loss_att=11.0, loss_ctc=18.51
17epoch:train:6300-6400batch: loss=13.18, loss_att=10.84, loss_ctc=18.65
17epoch:train:6400-6500batch: loss=13.16, loss_att=10.74, loss_ctc=18.78
17epoch:train:6500-6600batch: loss=12.26, loss_att=10.2, loss_ctc=17.06
17epoch:train:6600-6700batch: loss=12.82, loss_att=10.62, loss_ctc=17.96
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-17.pt: epoch=17, learning_rate=0.0013429654102737043, objf=-0.8452785201374964, valid_objf=-6.062116116030881
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-17-info
epoch 18, learning rate 0.0013028695899927967
18epoch:train:0-0batch: loss=11.88, loss_att=9.766, loss_ctc=16.82
18epoch:train:0-100batch: loss=12.7, loss_att=10.41, loss_ctc=18.05
18epoch:train:100-200batch: loss=11.98, loss_att=9.812, loss_ctc=17.03
18epoch:train:200-300batch: loss=11.74, loss_att=9.658, loss_ctc=16.6
18epoch:train:300-400batch: loss=12.16, loss_att=9.931, loss_ctc=17.35
18epoch:train:400-500batch: loss=11.56, loss_att=9.501, loss_ctc=16.35
18epoch:train:500-600batch: loss=12.27, loss_att=9.98, loss_ctc=17.62
18epoch:train:600-700batch: loss=11.64, loss_att=9.526, loss_ctc=16.58
18epoch:train:700-800batch: loss=11.45, loss_att=9.379, loss_ctc=16.27
18epoch:train:800-900batch: loss=12.08, loss_att=10.01, loss_ctc=16.92
18epoch:train:900-1000batch: loss=11.69, loss_att=9.62, loss_ctc=16.51
Validation average objf: -6.051392 over 5505.0 utts
18epoch:train:1000-1100batch: loss=11.43, loss_att=9.39, loss_ctc=16.19
18epoch:train:1100-1200batch: loss=12.01, loss_att=9.859, loss_ctc=17.03
18epoch:train:1200-1300batch: loss=11.45, loss_att=9.403, loss_ctc=16.22
18epoch:train:1300-1400batch: loss=12.11, loss_att=9.942, loss_ctc=17.18
18epoch:train:1400-1500batch: loss=12.36, loss_att=10.13, loss_ctc=17.58
18epoch:train:1500-1600batch: loss=12.2, loss_att=10.07, loss_ctc=17.18
18epoch:train:1600-1700batch: loss=12.6, loss_att=10.3, loss_ctc=17.96
18epoch:train:1700-1800batch: loss=12.75, loss_att=10.45, loss_ctc=18.11
18epoch:train:1800-1900batch: loss=12.13, loss_att=9.976, loss_ctc=17.15
18epoch:train:1900-2000batch: loss=12.26, loss_att=10.01, loss_ctc=17.53
Validation average objf: -6.023738 over 5505.0 utts
18epoch:train:2000-2100batch: loss=13.0, loss_att=10.73, loss_ctc=18.29
18epoch:train:2100-2200batch: loss=12.18, loss_att=10.03, loss_ctc=17.21
18epoch:train:2200-2300batch: loss=12.55, loss_att=10.41, loss_ctc=17.54
18epoch:train:2300-2400batch: loss=12.73, loss_att=10.46, loss_ctc=18.01
18epoch:train:2400-2500batch: loss=11.96, loss_att=9.877, loss_ctc=16.82
18epoch:train:2500-2600batch: loss=12.65, loss_att=10.48, loss_ctc=17.72
18epoch:train:2600-2700batch: loss=12.53, loss_att=10.25, loss_ctc=17.84
18epoch:train:2700-2800batch: loss=12.62, loss_att=10.4, loss_ctc=17.81
18epoch:train:2800-2900batch: loss=12.94, loss_att=10.73, loss_ctc=18.09
18epoch:train:2900-3000batch: loss=12.36, loss_att=10.23, loss_ctc=17.32
Validation average objf: -6.049583 over 5505.0 utts
18epoch:train:3000-3100batch: loss=13.19, loss_att=10.84, loss_ctc=18.68
18epoch:train:3100-3200batch: loss=12.94, loss_att=10.67, loss_ctc=18.23
18epoch:train:3200-3300batch: loss=12.75, loss_att=10.55, loss_ctc=17.89
18epoch:train:3300-3400batch: loss=12.88, loss_att=10.6, loss_ctc=18.19
18epoch:train:3400-3500batch: loss=13.26, loss_att=10.94, loss_ctc=18.67
18epoch:train:3500-3600batch: loss=12.74, loss_att=10.58, loss_ctc=17.79
18epoch:train:3600-3700batch: loss=12.96, loss_att=10.67, loss_ctc=18.3
18epoch:train:3700-3800batch: loss=13.19, loss_att=10.84, loss_ctc=18.66
18epoch:train:3800-3900batch: loss=13.08, loss_att=10.76, loss_ctc=18.48
18epoch:train:3900-4000batch: loss=13.42, loss_att=11.0, loss_ctc=19.05
Validation average objf: -6.081904 over 5505.0 utts
18epoch:train:4000-4100batch: loss=12.88, loss_att=10.69, loss_ctc=17.97
18epoch:train:4100-4200batch: loss=13.01, loss_att=10.74, loss_ctc=18.32
18epoch:train:4200-4300batch: loss=13.52, loss_att=11.11, loss_ctc=19.13
18epoch:train:4300-4400batch: loss=12.68, loss_att=10.42, loss_ctc=17.94
18epoch:train:4400-4500batch: loss=13.72, loss_att=11.27, loss_ctc=19.43
18epoch:train:4500-4600batch: loss=12.99, loss_att=10.74, loss_ctc=18.25
18epoch:train:4600-4700batch: loss=13.31, loss_att=11.0, loss_ctc=18.69
18epoch:train:4700-4800batch: loss=14.04, loss_att=11.63, loss_ctc=19.65
18epoch:train:4800-4900batch: loss=13.45, loss_att=11.06, loss_ctc=19.03
18epoch:train:4900-5000batch: loss=13.01, loss_att=10.73, loss_ctc=18.34
Validation average objf: -6.203473 over 5505.0 utts
18epoch:train:5000-5100batch: loss=13.62, loss_att=11.22, loss_ctc=19.22
18epoch:train:5100-5200batch: loss=12.89, loss_att=10.69, loss_ctc=18.03
18epoch:train:5200-5300batch: loss=13.3, loss_att=11.07, loss_ctc=18.52
18epoch:train:5300-5400batch: loss=13.75, loss_att=11.33, loss_ctc=19.39
18epoch:train:5400-5500batch: loss=14.19, loss_att=11.68, loss_ctc=20.04
18epoch:train:5500-5600batch: loss=12.84, loss_att=10.61, loss_ctc=18.04
18epoch:train:5600-5700batch: loss=13.21, loss_att=10.86, loss_ctc=18.71
18epoch:train:5700-5800batch: loss=13.77, loss_att=11.34, loss_ctc=19.43
18epoch:train:5800-5900batch: loss=13.31, loss_att=10.99, loss_ctc=18.74
18epoch:train:5900-6000batch: loss=12.99, loss_att=10.81, loss_ctc=18.07
Validation average objf: -6.082127 over 5505.0 utts
18epoch:train:6000-6100batch: loss=13.16, loss_att=10.88, loss_ctc=18.46
18epoch:train:6100-6200batch: loss=12.95, loss_att=10.77, loss_ctc=18.02
18epoch:train:6200-6300batch: loss=13.22, loss_att=10.85, loss_ctc=18.74
18epoch:train:6300-6400batch: loss=13.31, loss_att=10.92, loss_ctc=18.88
18epoch:train:6400-6500batch: loss=12.15, loss_att=10.17, loss_ctc=16.75
18epoch:train:6500-6600batch: loss=13.08, loss_att=10.83, loss_ctc=18.34
18epoch:train:6600-6700batch: loss=12.65, loss_att=10.5, loss_ctc=17.65
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-18.pt: epoch=18, learning_rate=0.0013028695899927967, objf=-0.8329030527434472, valid_objf=-6.0821270435967305
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-18-info
epoch 19, learning rate 0.0012661631791236073
19epoch:train:0-0batch: loss=16.08, loss_att=13.47, loss_ctc=22.16
19epoch:train:0-100batch: loss=11.59, loss_att=9.548, loss_ctc=16.34
19epoch:train:100-200batch: loss=12.08, loss_att=9.925, loss_ctc=17.11
19epoch:train:200-300batch: loss=11.73, loss_att=9.648, loss_ctc=16.58
19epoch:train:300-400batch: loss=12.26, loss_att=10.07, loss_ctc=17.37
19epoch:train:400-500batch: loss=11.94, loss_att=9.861, loss_ctc=16.78
19epoch:train:500-600batch: loss=11.42, loss_att=9.418, loss_ctc=16.08
19epoch:train:600-700batch: loss=11.73, loss_att=9.65, loss_ctc=16.57
19epoch:train:700-800batch: loss=11.29, loss_att=9.276, loss_ctc=15.98
19epoch:train:800-900batch: loss=11.48, loss_att=9.457, loss_ctc=16.21
19epoch:train:900-1000batch: loss=11.79, loss_att=9.722, loss_ctc=16.61
Validation average objf: -5.986947 over 5505.0 utts
19epoch:train:1000-1100batch: loss=11.23, loss_att=9.186, loss_ctc=15.99
19epoch:train:1100-1200batch: loss=11.66, loss_att=9.569, loss_ctc=16.55
19epoch:train:1200-1300batch: loss=11.32, loss_att=9.314, loss_ctc=16.0
19epoch:train:1300-1400batch: loss=11.7, loss_att=9.639, loss_ctc=16.5
19epoch:train:1400-1500batch: loss=11.83, loss_att=9.789, loss_ctc=16.6
19epoch:train:1500-1600batch: loss=11.64, loss_att=9.564, loss_ctc=16.49
19epoch:train:1600-1700batch: loss=12.01, loss_att=9.903, loss_ctc=16.92
19epoch:train:1700-1800batch: loss=12.06, loss_att=9.909, loss_ctc=17.08
19epoch:train:1800-1900batch: loss=12.35, loss_att=10.15, loss_ctc=17.5
19epoch:train:1900-2000batch: loss=12.55, loss_att=10.29, loss_ctc=17.82
Validation average objf: -6.052214 over 5505.0 utts
19epoch:train:2000-2100batch: loss=11.3, loss_att=9.305, loss_ctc=15.95
19epoch:train:2100-2200batch: loss=12.08, loss_att=9.915, loss_ctc=17.12
19epoch:train:2200-2300batch: loss=12.5, loss_att=10.32, loss_ctc=17.58
19epoch:train:2300-2400batch: loss=11.66, loss_att=9.64, loss_ctc=16.38
19epoch:train:2400-2500batch: loss=12.09, loss_att=10.02, loss_ctc=16.93
19epoch:train:2500-2600batch: loss=11.68, loss_att=9.65, loss_ctc=16.41
19epoch:train:2600-2700batch: loss=11.67, loss_att=9.555, loss_ctc=16.61
19epoch:train:2700-2800batch: loss=11.87, loss_att=9.778, loss_ctc=16.75
19epoch:train:2800-2900batch: loss=12.33, loss_att=10.13, loss_ctc=17.47
19epoch:train:2900-3000batch: loss=12.0, loss_att=9.869, loss_ctc=16.98
Validation average objf: -6.016425 over 5505.0 utts
19epoch:train:3000-3100batch: loss=11.99, loss_att=9.912, loss_ctc=16.85
19epoch:train:3100-3200batch: loss=12.31, loss_att=10.08, loss_ctc=17.51
19epoch:train:3200-3300batch: loss=12.64, loss_att=10.46, loss_ctc=17.73
19epoch:train:3300-3400batch: loss=12.62, loss_att=10.48, loss_ctc=17.63
19epoch:train:3400-3500batch: loss=12.09, loss_att=9.977, loss_ctc=17.02
19epoch:train:3500-3600batch: loss=12.75, loss_att=10.54, loss_ctc=17.92
19epoch:train:3600-3700batch: loss=12.7, loss_att=10.47, loss_ctc=17.93
19epoch:train:3700-3800batch: loss=12.3, loss_att=10.13, loss_ctc=17.36
19epoch:train:3800-3900batch: loss=12.59, loss_att=10.4, loss_ctc=17.71
19epoch:train:3900-4000batch: loss=12.28, loss_att=10.09, loss_ctc=17.38
Validation average objf: -6.043434 over 5505.0 utts
19epoch:train:4000-4100batch: loss=12.77, loss_att=10.46, loss_ctc=18.15
19epoch:train:4100-4200batch: loss=12.41, loss_att=10.16, loss_ctc=17.65
19epoch:train:4200-4300batch: loss=13.54, loss_att=11.09, loss_ctc=19.26
19epoch:train:4300-4400batch: loss=12.59, loss_att=10.47, loss_ctc=17.56
19epoch:train:4400-4500batch: loss=12.04, loss_att=9.984, loss_ctc=16.85
19epoch:train:4500-4600batch: loss=12.65, loss_att=10.44, loss_ctc=17.79
19epoch:train:4600-4700batch: loss=12.64, loss_att=10.41, loss_ctc=17.84
19epoch:train:4700-4800batch: loss=13.0, loss_att=10.83, loss_ctc=18.09
19epoch:train:4800-4900batch: loss=12.44, loss_att=10.23, loss_ctc=17.6
19epoch:train:4900-5000batch: loss=12.59, loss_att=10.43, loss_ctc=17.64
Validation average objf: -6.004106 over 5505.0 utts
19epoch:train:5000-5100batch: loss=12.74, loss_att=10.5, loss_ctc=17.96
19epoch:train:5100-5200batch: loss=12.71, loss_att=10.46, loss_ctc=17.97
19epoch:train:5200-5300batch: loss=12.74, loss_att=10.57, loss_ctc=17.81
19epoch:train:5300-5400batch: loss=12.47, loss_att=10.33, loss_ctc=17.46
19epoch:train:5400-5500batch: loss=12.6, loss_att=10.38, loss_ctc=17.8
19epoch:train:5500-5600batch: loss=13.27, loss_att=10.92, loss_ctc=18.74
19epoch:train:5600-5700batch: loss=12.93, loss_att=10.75, loss_ctc=18.01
19epoch:train:5700-5800batch: loss=12.37, loss_att=10.24, loss_ctc=17.32
19epoch:train:5800-5900batch: loss=13.23, loss_att=10.99, loss_ctc=18.46
19epoch:train:5900-6000batch: loss=12.98, loss_att=10.71, loss_ctc=18.28
Validation average objf: -6.056260 over 5505.0 utts
19epoch:train:6000-6100batch: loss=13.11, loss_att=10.78, loss_ctc=18.56
19epoch:train:6100-6200batch: loss=12.96, loss_att=10.7, loss_ctc=18.24
19epoch:train:6200-6300batch: loss=12.84, loss_att=10.67, loss_ctc=17.91
19epoch:train:6300-6400batch: loss=12.66, loss_att=10.52, loss_ctc=17.67
19epoch:train:6400-6500batch: loss=12.11, loss_att=10.05, loss_ctc=16.9
19epoch:train:6500-6600batch: loss=12.43, loss_att=10.32, loss_ctc=17.34
19epoch:train:6600-6700batch: loss=12.15, loss_att=10.06, loss_ctc=17.04
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-19.pt: epoch=19, learning_rate=0.0012661631791236073, objf=-0.8017295935275487, valid_objf=-6.056259934150772
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-19-info
epoch 20, learning rate 0.001232394072828266
20epoch:train:0-0batch: loss=9.794, loss_att=8.203, loss_ctc=13.51
20epoch:train:0-100batch: loss=11.31, loss_att=9.271, loss_ctc=16.05
20epoch:train:100-200batch: loss=11.27, loss_att=9.303, loss_ctc=15.85
20epoch:train:200-300batch: loss=11.55, loss_att=9.549, loss_ctc=16.22
20epoch:train:300-400batch: loss=11.51, loss_att=9.486, loss_ctc=16.23
20epoch:train:400-500batch: loss=11.16, loss_att=9.157, loss_ctc=15.83
20epoch:train:500-600batch: loss=11.4, loss_att=9.353, loss_ctc=16.18
20epoch:train:600-700batch: loss=11.24, loss_att=9.213, loss_ctc=15.98
20epoch:train:700-800batch: loss=10.98, loss_att=9.001, loss_ctc=15.6
20epoch:train:800-900batch: loss=10.37, loss_att=8.495, loss_ctc=14.74
20epoch:train:900-1000batch: loss=11.28, loss_att=9.294, loss_ctc=15.91
Validation average objf: -5.948862 over 5505.0 utts
20epoch:train:1000-1100batch: loss=11.2, loss_att=9.243, loss_ctc=15.77
20epoch:train:1100-1200batch: loss=11.54, loss_att=9.562, loss_ctc=16.15
20epoch:train:1200-1300batch: loss=11.23, loss_att=9.178, loss_ctc=16.0
20epoch:train:1300-1400batch: loss=11.51, loss_att=9.413, loss_ctc=16.41
20epoch:train:1400-1500batch: loss=12.27, loss_att=10.02, loss_ctc=17.52
20epoch:train:1500-1600batch: loss=12.08, loss_att=9.939, loss_ctc=17.09
20epoch:train:1600-1700batch: loss=11.92, loss_att=9.805, loss_ctc=16.85
20epoch:train:1700-1800batch: loss=11.18, loss_att=9.198, loss_ctc=15.82
20epoch:train:1800-1900batch: loss=11.93, loss_att=9.854, loss_ctc=16.79
20epoch:train:1900-2000batch: loss=11.34, loss_att=9.335, loss_ctc=16.01
Validation average objf: -6.013461 over 5505.0 utts
20epoch:train:2000-2100batch: loss=12.07, loss_att=9.988, loss_ctc=16.93
20epoch:train:2100-2200batch: loss=11.76, loss_att=9.703, loss_ctc=16.56
20epoch:train:2200-2300batch: loss=11.95, loss_att=9.831, loss_ctc=16.88
20epoch:train:2300-2400batch: loss=11.97, loss_att=9.859, loss_ctc=16.9
20epoch:train:2400-2500batch: loss=11.79, loss_att=9.715, loss_ctc=16.62
20epoch:train:2500-2600batch: loss=11.76, loss_att=9.685, loss_ctc=16.59
20epoch:train:2600-2700batch: loss=12.15, loss_att=10.02, loss_ctc=17.1
20epoch:train:2700-2800batch: loss=11.99, loss_att=9.878, loss_ctc=16.91
20epoch:train:2800-2900batch: loss=12.53, loss_att=10.33, loss_ctc=17.65
20epoch:train:2900-3000batch: loss=12.27, loss_att=10.03, loss_ctc=17.5
Validation average objf: -6.014409 over 5505.0 utts
20epoch:train:3000-3100batch: loss=12.46, loss_att=10.21, loss_ctc=17.72
20epoch:train:3100-3200batch: loss=12.83, loss_att=10.6, loss_ctc=18.05
20epoch:train:3200-3300batch: loss=12.24, loss_att=10.09, loss_ctc=17.26
20epoch:train:3300-3400batch: loss=11.96, loss_att=9.89, loss_ctc=16.79
20epoch:train:3400-3500batch: loss=11.69, loss_att=9.67, loss_ctc=16.41
20epoch:train:3500-3600batch: loss=12.44, loss_att=10.31, loss_ctc=17.41
20epoch:train:3600-3700batch: loss=13.07, loss_att=10.75, loss_ctc=18.48
20epoch:train:3700-3800batch: loss=12.6, loss_att=10.4, loss_ctc=17.73
20epoch:train:3800-3900batch: loss=12.28, loss_att=10.04, loss_ctc=17.53
20epoch:train:3900-4000batch: loss=12.11, loss_att=10.02, loss_ctc=16.97
Validation average objf: -6.125882 over 5505.0 utts
20epoch:train:4000-4100batch: loss=12.27, loss_att=10.18, loss_ctc=17.16
20epoch:train:4100-4200batch: loss=12.02, loss_att=9.917, loss_ctc=16.94
20epoch:train:4200-4300batch: loss=12.29, loss_att=10.18, loss_ctc=17.22
20epoch:train:4300-4400batch: loss=12.77, loss_att=10.55, loss_ctc=17.96
20epoch:train:4400-4500batch: loss=12.66, loss_att=10.46, loss_ctc=17.81
20epoch:train:4500-4600batch: loss=12.32, loss_att=10.15, loss_ctc=17.37
20epoch:train:4600-4700batch: loss=12.45, loss_att=10.37, loss_ctc=17.32
20epoch:train:4700-4800batch: loss=12.72, loss_att=10.59, loss_ctc=17.7
20epoch:train:4800-4900batch: loss=12.07, loss_att=10.01, loss_ctc=16.88
20epoch:train:4900-5000batch: loss=12.19, loss_att=10.08, loss_ctc=17.12
Validation average objf: -6.056095 over 5505.0 utts
20epoch:train:5000-5100batch: loss=12.31, loss_att=10.21, loss_ctc=17.23
20epoch:train:5100-5200batch: loss=12.49, loss_att=10.25, loss_ctc=17.73
20epoch:train:5200-5300batch: loss=11.85, loss_att=9.801, loss_ctc=16.64
20epoch:train:5300-5400batch: loss=12.5, loss_att=10.36, loss_ctc=17.5
20epoch:train:5400-5500batch: loss=13.22, loss_att=10.99, loss_ctc=18.4
20epoch:train:5500-5600batch: loss=12.36, loss_att=10.27, loss_ctc=17.24
20epoch:train:5600-5700batch: loss=11.98, loss_att=9.913, loss_ctc=16.81
20epoch:train:5700-5800batch: loss=12.66, loss_att=10.5, loss_ctc=17.68
20epoch:train:5800-5900batch: loss=13.03, loss_att=10.83, loss_ctc=18.16
20epoch:train:5900-6000batch: loss=13.04, loss_att=10.77, loss_ctc=18.35
Validation average objf: -6.228459 over 5505.0 utts
20epoch:train:6000-6100batch: loss=11.83, loss_att=9.794, loss_ctc=16.56
20epoch:train:6100-6200batch: loss=12.26, loss_att=10.13, loss_ctc=17.22
20epoch:train:6200-6300batch: loss=12.7, loss_att=10.54, loss_ctc=17.75
20epoch:train:6300-6400batch: loss=12.25, loss_att=10.09, loss_ctc=17.29
20epoch:train:6400-6500batch: loss=12.46, loss_att=10.32, loss_ctc=17.47
20epoch:train:6500-6600batch: loss=12.21, loss_att=10.16, loss_ctc=16.99
20epoch:train:6600-6700batch: loss=12.54, loss_att=10.45, loss_ctc=17.42
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-20.pt: epoch=20, learning_rate=0.001232394072828266, objf=-0.784728489054949, valid_objf=-6.228459213215259
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-20-info
epoch 21, learning rate 0.001201190325554137
21epoch:train:0-0batch: loss=18.14, loss_att=15.36, loss_ctc=24.62
21epoch:train:0-100batch: loss=11.62, loss_att=9.484, loss_ctc=16.61
21epoch:train:100-200batch: loss=11.29, loss_att=9.296, loss_ctc=15.95
21epoch:train:200-300batch: loss=11.47, loss_att=9.42, loss_ctc=16.26
21epoch:train:300-400batch: loss=11.65, loss_att=9.635, loss_ctc=16.36
21epoch:train:400-500batch: loss=11.15, loss_att=9.189, loss_ctc=15.73
21epoch:train:500-600batch: loss=11.12, loss_att=9.123, loss_ctc=15.8
21epoch:train:600-700batch: loss=11.49, loss_att=9.429, loss_ctc=16.28
21epoch:train:700-800batch: loss=11.15, loss_att=9.089, loss_ctc=15.96
21epoch:train:800-900batch: loss=11.71, loss_att=9.666, loss_ctc=16.49
21epoch:train:900-1000batch: loss=11.96, loss_att=9.821, loss_ctc=16.95
Validation average objf: -6.071333 over 5505.0 utts
21epoch:train:1000-1100batch: loss=10.51, loss_att=8.604, loss_ctc=14.96
21epoch:train:1100-1200batch: loss=11.1, loss_att=9.078, loss_ctc=15.81
21epoch:train:1200-1300batch: loss=10.76, loss_att=8.797, loss_ctc=15.36
21epoch:train:1300-1400batch: loss=11.02, loss_att=9.031, loss_ctc=15.66
21epoch:train:1400-1500batch: loss=12.35, loss_att=10.19, loss_ctc=17.41
21epoch:train:1500-1600batch: loss=11.04, loss_att=9.097, loss_ctc=15.58
21epoch:train:1600-1700batch: loss=11.45, loss_att=9.366, loss_ctc=16.31
21epoch:train:1700-1800batch: loss=11.56, loss_att=9.451, loss_ctc=16.47
21epoch:train:1800-1900batch: loss=11.45, loss_att=9.368, loss_ctc=16.3
21epoch:train:1900-2000batch: loss=11.34, loss_att=9.315, loss_ctc=16.05
Validation average objf: -5.974537 over 5505.0 utts
21epoch:train:2000-2100batch: loss=11.34, loss_att=9.346, loss_ctc=15.99
21epoch:train:2100-2200batch: loss=10.83, loss_att=8.909, loss_ctc=15.31
21epoch:train:2200-2300batch: loss=11.48, loss_att=9.442, loss_ctc=16.22
21epoch:train:2300-2400batch: loss=11.46, loss_att=9.471, loss_ctc=16.09
21epoch:train:2400-2500batch: loss=11.61, loss_att=9.598, loss_ctc=16.3
21epoch:train:2500-2600batch: loss=11.86, loss_att=9.795, loss_ctc=16.68
21epoch:train:2600-2700batch: loss=11.48, loss_att=9.465, loss_ctc=16.18
21epoch:train:2700-2800batch: loss=11.81, loss_att=9.752, loss_ctc=16.59
21epoch:train:2800-2900batch: loss=11.59, loss_att=9.565, loss_ctc=16.31
21epoch:train:2900-3000batch: loss=11.44, loss_att=9.44, loss_ctc=16.11
Validation average objf: -6.086864 over 5505.0 utts
21epoch:train:3000-3100batch: loss=11.5, loss_att=9.486, loss_ctc=16.21
21epoch:train:3100-3200batch: loss=11.86, loss_att=9.766, loss_ctc=16.76
21epoch:train:3200-3300batch: loss=11.8, loss_att=9.78, loss_ctc=16.51
21epoch:train:3300-3400batch: loss=12.43, loss_att=10.19, loss_ctc=17.65
21epoch:train:3400-3500batch: loss=11.9, loss_att=9.851, loss_ctc=16.68
21epoch:train:3500-3600batch: loss=11.99, loss_att=9.841, loss_ctc=17.0
21epoch:train:3600-3700batch: loss=12.53, loss_att=10.31, loss_ctc=17.7
21epoch:train:3700-3800batch: loss=12.68, loss_att=10.46, loss_ctc=17.85
21epoch:train:3800-3900batch: loss=11.76, loss_att=9.605, loss_ctc=16.8
21epoch:train:3900-4000batch: loss=12.03, loss_att=9.905, loss_ctc=17.0
Validation average objf: -6.037635 over 5505.0 utts
21epoch:train:4000-4100batch: loss=12.46, loss_att=10.27, loss_ctc=17.6
21epoch:train:4100-4200batch: loss=12.78, loss_att=10.48, loss_ctc=18.12
21epoch:train:4200-4300batch: loss=12.3, loss_att=10.21, loss_ctc=17.19
21epoch:train:4300-4400batch: loss=12.07, loss_att=9.959, loss_ctc=17.0
21epoch:train:4400-4500batch: loss=12.52, loss_att=10.46, loss_ctc=17.35
21epoch:train:4500-4600batch: loss=11.96, loss_att=9.918, loss_ctc=16.71
21epoch:train:4600-4700batch: loss=12.87, loss_att=10.62, loss_ctc=18.13
21epoch:train:4700-4800batch: loss=13.07, loss_att=10.76, loss_ctc=18.45
21epoch:train:4800-4900batch: loss=12.57, loss_att=10.29, loss_ctc=17.9
21epoch:train:4900-5000batch: loss=13.43, loss_att=11.06, loss_ctc=18.95
Validation average objf: -6.121290 over 5505.0 utts
21epoch:train:5000-5100batch: loss=13.22, loss_att=10.92, loss_ctc=18.58
21epoch:train:5100-5200batch: loss=12.55, loss_att=10.26, loss_ctc=17.88
21epoch:train:5200-5300batch: loss=12.46, loss_att=10.26, loss_ctc=17.6
21epoch:train:5300-5400batch: loss=11.92, loss_att=9.759, loss_ctc=16.97
21epoch:train:5400-5500batch: loss=12.31, loss_att=10.14, loss_ctc=17.36
21epoch:train:5500-5600batch: loss=13.08, loss_att=10.76, loss_ctc=18.49
21epoch:train:5600-5700batch: loss=12.53, loss_att=10.29, loss_ctc=17.78
21epoch:train:5700-5800batch: loss=12.44, loss_att=10.27, loss_ctc=17.52
21epoch:train:5800-5900batch: loss=13.13, loss_att=10.83, loss_ctc=18.51
21epoch:train:5900-6000batch: loss=12.86, loss_att=10.66, loss_ctc=17.99
Validation average objf: -6.104367 over 5505.0 utts
21epoch:train:6000-6100batch: loss=12.16, loss_att=10.08, loss_ctc=17.0
21epoch:train:6100-6200batch: loss=11.67, loss_att=9.663, loss_ctc=16.37
21epoch:train:6200-6300batch: loss=12.46, loss_att=10.28, loss_ctc=17.56
21epoch:train:6300-6400batch: loss=12.66, loss_att=10.4, loss_ctc=17.91
21epoch:train:6400-6500batch: loss=11.54, loss_att=9.531, loss_ctc=16.21
21epoch:train:6500-6600batch: loss=12.46, loss_att=10.17, loss_ctc=17.8
21epoch:train:6600-6700batch: loss=11.54, loss_att=9.589, loss_ctc=16.09
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-21.pt: epoch=21, learning_rate=0.001201190325554137, objf=-0.7788912894389899, valid_objf=-6.1043667688465035
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-21-info
epoch 22, learning rate 0.0011722386335390389
22epoch:train:0-0batch: loss=10.34, loss_att=8.82, loss_ctc=13.88
22epoch:train:0-100batch: loss=11.0, loss_att=9.072, loss_ctc=15.51
22epoch:train:100-200batch: loss=11.4, loss_att=9.35, loss_ctc=16.17
22epoch:train:200-300batch: loss=11.02, loss_att=8.985, loss_ctc=15.76
22epoch:train:300-400batch: loss=11.21, loss_att=9.249, loss_ctc=15.8
22epoch:train:400-500batch: loss=10.91, loss_att=8.955, loss_ctc=15.48
22epoch:train:500-600batch: loss=10.34, loss_att=8.497, loss_ctc=14.63
22epoch:train:600-700batch: loss=11.18, loss_att=9.151, loss_ctc=15.92
22epoch:train:700-800batch: loss=10.62, loss_att=8.722, loss_ctc=15.06
22epoch:train:800-900batch: loss=10.84, loss_att=8.837, loss_ctc=15.51
22epoch:train:900-1000batch: loss=11.78, loss_att=9.645, loss_ctc=16.77
Validation average objf: -6.062948 over 5505.0 utts
22epoch:train:1000-1100batch: loss=11.6, loss_att=9.5, loss_ctc=16.49
22epoch:train:1100-1200batch: loss=11.23, loss_att=9.187, loss_ctc=16.01
22epoch:train:1200-1300batch: loss=11.17, loss_att=9.119, loss_ctc=15.94
22epoch:train:1300-1400batch: loss=10.69, loss_att=8.807, loss_ctc=15.07
22epoch:train:1400-1500batch: loss=11.61, loss_att=9.523, loss_ctc=16.47
22epoch:train:1500-1600batch: loss=10.98, loss_att=8.966, loss_ctc=15.7
22epoch:train:1600-1700batch: loss=11.66, loss_att=9.517, loss_ctc=16.67
22epoch:train:1700-1800batch: loss=11.54, loss_att=9.474, loss_ctc=16.35
22epoch:train:1800-1900batch: loss=11.21, loss_att=9.139, loss_ctc=16.06
22epoch:train:1900-2000batch: loss=11.82, loss_att=9.759, loss_ctc=16.63
Validation average objf: -6.038437 over 5505.0 utts
22epoch:train:2000-2100batch: loss=11.32, loss_att=9.312, loss_ctc=16.01
22epoch:train:2100-2200batch: loss=11.19, loss_att=9.158, loss_ctc=15.95
22epoch:train:2200-2300batch: loss=11.27, loss_att=9.315, loss_ctc=15.82
22epoch:train:2300-2400batch: loss=12.29, loss_att=10.09, loss_ctc=17.44
22epoch:train:2400-2500batch: loss=12.17, loss_att=9.959, loss_ctc=17.35
22epoch:train:2500-2600batch: loss=11.17, loss_att=9.22, loss_ctc=15.72
22epoch:train:2600-2700batch: loss=12.04, loss_att=9.866, loss_ctc=17.13
22epoch:train:2700-2800batch: loss=11.76, loss_att=9.68, loss_ctc=16.62
22epoch:train:2800-2900batch: loss=11.01, loss_att=9.059, loss_ctc=15.55
22epoch:train:2900-3000batch: loss=11.64, loss_att=9.637, loss_ctc=16.3
Validation average objf: -6.057568 over 5505.0 utts
22epoch:train:3000-3100batch: loss=10.68, loss_att=8.809, loss_ctc=15.05
22epoch:train:3100-3200batch: loss=11.65, loss_att=9.626, loss_ctc=16.36
22epoch:train:3200-3300batch: loss=12.23, loss_att=10.03, loss_ctc=17.35
22epoch:train:3300-3400batch: loss=11.24, loss_att=9.313, loss_ctc=15.74
22epoch:train:3400-3500batch: loss=12.04, loss_att=9.979, loss_ctc=16.86
22epoch:train:3500-3600batch: loss=12.22, loss_att=10.03, loss_ctc=17.32
22epoch:train:3600-3700batch: loss=12.32, loss_att=10.12, loss_ctc=17.47
22epoch:train:3700-3800batch: loss=11.47, loss_att=9.48, loss_ctc=16.12
22epoch:train:3800-3900batch: loss=12.63, loss_att=10.4, loss_ctc=17.83
22epoch:train:3900-4000batch: loss=13.01, loss_att=10.74, loss_ctc=18.31
Validation average objf: -6.042601 over 5505.0 utts
22epoch:train:4000-4100batch: loss=11.73, loss_att=9.723, loss_ctc=16.4
22epoch:train:4100-4200batch: loss=11.85, loss_att=9.734, loss_ctc=16.78
22epoch:train:4200-4300batch: loss=11.86, loss_att=9.829, loss_ctc=16.61
22epoch:train:4300-4400batch: loss=11.38, loss_att=9.385, loss_ctc=16.05
22epoch:train:4400-4500batch: loss=12.09, loss_att=9.956, loss_ctc=17.07
22epoch:train:4500-4600batch: loss=11.61, loss_att=9.591, loss_ctc=16.34
22epoch:train:4600-4700batch: loss=12.44, loss_att=10.26, loss_ctc=17.51
22epoch:train:4700-4800batch: loss=11.22, loss_att=9.323, loss_ctc=15.64
22epoch:train:4800-4900batch: loss=11.98, loss_att=9.84, loss_ctc=16.97
22epoch:train:4900-5000batch: loss=12.1, loss_att=9.916, loss_ctc=17.21
Validation average objf: -6.203272 over 5505.0 utts
22epoch:train:5000-5100batch: loss=12.76, loss_att=10.52, loss_ctc=17.98
22epoch:train:5100-5200batch: loss=11.8, loss_att=9.712, loss_ctc=16.68
22epoch:train:5200-5300batch: loss=11.89, loss_att=9.871, loss_ctc=16.61
22epoch:train:5300-5400batch: loss=12.31, loss_att=10.19, loss_ctc=17.26
22epoch:train:5400-5500batch: loss=12.41, loss_att=10.29, loss_ctc=17.36
22epoch:train:5500-5600batch: loss=12.09, loss_att=10.0, loss_ctc=16.96
22epoch:train:5600-5700batch: loss=11.94, loss_att=9.808, loss_ctc=16.92
22epoch:train:5700-5800batch: loss=12.77, loss_att=10.54, loss_ctc=17.96
22epoch:train:5800-5900batch: loss=12.27, loss_att=10.08, loss_ctc=17.37
22epoch:train:5900-6000batch: loss=12.03, loss_att=9.894, loss_ctc=17.01
Validation average objf: -6.178344 over 5505.0 utts
22epoch:train:6000-6100batch: loss=12.29, loss_att=10.19, loss_ctc=17.19
22epoch:train:6100-6200batch: loss=12.45, loss_att=10.28, loss_ctc=17.5
22epoch:train:6200-6300batch: loss=12.09, loss_att=9.981, loss_ctc=17.0
22epoch:train:6300-6400batch: loss=12.11, loss_att=9.996, loss_ctc=17.03
22epoch:train:6400-6500batch: loss=12.0, loss_att=9.889, loss_ctc=16.94
22epoch:train:6500-6600batch: loss=11.46, loss_att=9.538, loss_ctc=15.93
22epoch:train:6600-6700batch: loss=11.6, loss_att=9.559, loss_ctc=16.37
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-22.pt: epoch=22, learning_rate=0.0011722386335390389, objf=-0.7621512933903094, valid_objf=-6.178343551316985
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-22-info
epoch 23, learning rate 0.0011452881150856877
23epoch:train:0-0batch: loss=7.045, loss_att=5.694, loss_ctc=10.2
23epoch:train:0-100batch: loss=11.39, loss_att=9.339, loss_ctc=16.18
23epoch:train:100-200batch: loss=11.54, loss_att=9.404, loss_ctc=16.52
23epoch:train:200-300batch: loss=11.5, loss_att=9.346, loss_ctc=16.54
23epoch:train:300-400batch: loss=11.51, loss_att=9.441, loss_ctc=16.33
23epoch:train:400-500batch: loss=10.94, loss_att=9.074, loss_ctc=15.29
23epoch:train:500-600batch: loss=10.82, loss_att=8.916, loss_ctc=15.25
23epoch:train:600-700batch: loss=11.15, loss_att=9.097, loss_ctc=15.93
23epoch:train:700-800batch: loss=11.16, loss_att=9.148, loss_ctc=15.85
23epoch:train:800-900batch: loss=10.84, loss_att=8.912, loss_ctc=15.34
23epoch:train:900-1000batch: loss=11.16, loss_att=9.046, loss_ctc=16.08
Validation average objf: -5.952628 over 5505.0 utts
23epoch:train:1000-1100batch: loss=10.81, loss_att=8.829, loss_ctc=15.44
23epoch:train:1100-1200batch: loss=11.02, loss_att=8.998, loss_ctc=15.73
23epoch:train:1200-1300batch: loss=10.41, loss_att=8.529, loss_ctc=14.79
23epoch:train:1300-1400batch: loss=11.35, loss_att=9.321, loss_ctc=16.07
23epoch:train:1400-1500batch: loss=11.25, loss_att=9.194, loss_ctc=16.04
23epoch:train:1500-1600batch: loss=11.24, loss_att=9.252, loss_ctc=15.87
23epoch:train:1600-1700batch: loss=11.22, loss_att=9.184, loss_ctc=15.97
23epoch:train:1700-1800batch: loss=11.13, loss_att=9.051, loss_ctc=15.99
23epoch:train:1800-1900batch: loss=11.04, loss_att=9.029, loss_ctc=15.72
23epoch:train:1900-2000batch: loss=11.56, loss_att=9.612, loss_ctc=16.09
Validation average objf: -5.990239 over 5505.0 utts
23epoch:train:2000-2100batch: loss=11.34, loss_att=9.365, loss_ctc=15.96
23epoch:train:2100-2200batch: loss=11.33, loss_att=9.24, loss_ctc=16.19
23epoch:train:2200-2300batch: loss=11.31, loss_att=9.378, loss_ctc=15.83
23epoch:train:2300-2400batch: loss=11.62, loss_att=9.6, loss_ctc=16.32
23epoch:train:2400-2500batch: loss=10.9, loss_att=8.953, loss_ctc=15.46
23epoch:train:2500-2600batch: loss=12.18, loss_att=10.08, loss_ctc=17.1
23epoch:train:2600-2700batch: loss=11.81, loss_att=9.739, loss_ctc=16.65
23epoch:train:2700-2800batch: loss=11.2, loss_att=9.303, loss_ctc=15.63
23epoch:train:2800-2900batch: loss=11.84, loss_att=9.722, loss_ctc=16.77
23epoch:train:2900-3000batch: loss=11.11, loss_att=9.083, loss_ctc=15.83
Validation average objf: -5.938525 over 5505.0 utts
23epoch:train:3000-3100batch: loss=11.14, loss_att=9.169, loss_ctc=15.72
23epoch:train:3100-3200batch: loss=11.27, loss_att=9.333, loss_ctc=15.79
23epoch:train:3200-3300batch: loss=11.89, loss_att=9.78, loss_ctc=16.82
23epoch:train:3300-3400batch: loss=11.49, loss_att=9.475, loss_ctc=16.18
23epoch:train:3400-3500batch: loss=11.85, loss_att=9.667, loss_ctc=16.95
23epoch:train:3500-3600batch: loss=11.76, loss_att=9.713, loss_ctc=16.55
23epoch:train:3600-3700batch: loss=12.33, loss_att=10.17, loss_ctc=17.36
23epoch:train:3700-3800batch: loss=11.9, loss_att=9.821, loss_ctc=16.76
23epoch:train:3800-3900batch: loss=11.9, loss_att=9.823, loss_ctc=16.74
23epoch:train:3900-4000batch: loss=12.04, loss_att=9.956, loss_ctc=16.92
Validation average objf: -6.052387 over 5505.0 utts
23epoch:train:4000-4100batch: loss=12.23, loss_att=10.02, loss_ctc=17.37
23epoch:train:4100-4200batch: loss=11.57, loss_att=9.534, loss_ctc=16.33
23epoch:train:4200-4300batch: loss=12.12, loss_att=10.02, loss_ctc=17.03
23epoch:train:4300-4400batch: loss=12.4, loss_att=10.18, loss_ctc=17.6
23epoch:train:4400-4500batch: loss=11.9, loss_att=9.857, loss_ctc=16.67
23epoch:train:4500-4600batch: loss=11.85, loss_att=9.754, loss_ctc=16.75
23epoch:train:4600-4700batch: loss=11.84, loss_att=9.748, loss_ctc=16.73
23epoch:train:4700-4800batch: loss=12.66, loss_att=10.42, loss_ctc=17.89
23epoch:train:4800-4900batch: loss=12.68, loss_att=10.45, loss_ctc=17.9
23epoch:train:4900-5000batch: loss=12.06, loss_att=9.94, loss_ctc=16.99
Validation average objf: -6.043765 over 5505.0 utts
23epoch:train:5000-5100batch: loss=11.84, loss_att=9.727, loss_ctc=16.79
23epoch:train:5100-5200batch: loss=11.92, loss_att=9.851, loss_ctc=16.76
23epoch:train:5200-5300batch: loss=11.9, loss_att=9.792, loss_ctc=16.8
23epoch:train:5300-5400batch: loss=11.86, loss_att=9.81, loss_ctc=16.64
23epoch:train:5400-5500batch: loss=11.23, loss_att=9.248, loss_ctc=15.86
23epoch:train:5500-5600batch: loss=11.82, loss_att=9.733, loss_ctc=16.7
23epoch:train:5600-5700batch: loss=12.63, loss_att=10.46, loss_ctc=17.7
23epoch:train:5700-5800batch: loss=12.15, loss_att=10.06, loss_ctc=17.04
23epoch:train:5800-5900batch: loss=11.8, loss_att=9.782, loss_ctc=16.5
23epoch:train:5900-6000batch: loss=12.13, loss_att=10.03, loss_ctc=17.05
Validation average objf: -6.040820 over 5505.0 utts
23epoch:train:6000-6100batch: loss=12.4, loss_att=10.2, loss_ctc=17.54
23epoch:train:6100-6200batch: loss=11.97, loss_att=9.809, loss_ctc=17.0
23epoch:train:6200-6300batch: loss=11.55, loss_att=9.546, loss_ctc=16.24
23epoch:train:6300-6400batch: loss=11.39, loss_att=9.396, loss_ctc=16.05
23epoch:train:6400-6500batch: loss=11.62, loss_att=9.556, loss_ctc=16.43
23epoch:train:6500-6600batch: loss=11.36, loss_att=9.343, loss_ctc=16.07
23epoch:train:6600-6700batch: loss=11.77, loss_att=9.747, loss_ctc=16.48
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-23.pt: epoch=23, learning_rate=0.0011452881150856877, objf=-0.7566034535319406, valid_objf=-6.04082013510445
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-23-info
epoch 24, learning rate 0.0011201112570543802
24epoch:train:0-0batch: loss=14.05, loss_att=10.97, loss_ctc=21.25
24epoch:train:0-100batch: loss=11.66, loss_att=9.542, loss_ctc=16.6
24epoch:train:100-200batch: loss=10.78, loss_att=8.875, loss_ctc=15.22
24epoch:train:200-300batch: loss=10.8, loss_att=8.881, loss_ctc=15.28
24epoch:train:300-400batch: loss=10.29, loss_att=8.43, loss_ctc=14.62
24epoch:train:400-500batch: loss=10.78, loss_att=8.838, loss_ctc=15.32
24epoch:train:500-600batch: loss=11.07, loss_att=9.059, loss_ctc=15.77
24epoch:train:600-700batch: loss=10.66, loss_att=8.713, loss_ctc=15.2
24epoch:train:700-800batch: loss=10.17, loss_att=8.363, loss_ctc=14.37
24epoch:train:800-900batch: loss=11.29, loss_att=9.228, loss_ctc=16.1
24epoch:train:900-1000batch: loss=11.07, loss_att=9.078, loss_ctc=15.72
Validation average objf: -5.936055 over 5505.0 utts
24epoch:train:1000-1100batch: loss=11.06, loss_att=8.986, loss_ctc=15.89
24epoch:train:1100-1200batch: loss=10.27, loss_att=8.408, loss_ctc=14.6
24epoch:train:1200-1300batch: loss=10.52, loss_att=8.634, loss_ctc=14.9
24epoch:train:1300-1400batch: loss=10.57, loss_att=8.686, loss_ctc=14.98
24epoch:train:1400-1500batch: loss=11.25, loss_att=9.199, loss_ctc=16.03
24epoch:train:1500-1600batch: loss=11.52, loss_att=9.47, loss_ctc=16.31
24epoch:train:1600-1700batch: loss=11.38, loss_att=9.32, loss_ctc=16.19
24epoch:train:1700-1800batch: loss=11.23, loss_att=9.189, loss_ctc=15.98
24epoch:train:1800-1900batch: loss=11.23, loss_att=9.155, loss_ctc=16.09
24epoch:train:1900-2000batch: loss=11.04, loss_att=9.048, loss_ctc=15.68
Validation average objf: -5.997793 over 5505.0 utts
24epoch:train:2000-2100batch: loss=10.46, loss_att=8.633, loss_ctc=14.71
24epoch:train:2100-2200batch: loss=11.4, loss_att=9.35, loss_ctc=16.17
24epoch:train:2200-2300batch: loss=10.83, loss_att=8.845, loss_ctc=15.47
24epoch:train:2300-2400batch: loss=11.14, loss_att=9.258, loss_ctc=15.52
24epoch:train:2400-2500batch: loss=11.6, loss_att=9.495, loss_ctc=16.51
24epoch:train:2500-2600batch: loss=11.23, loss_att=9.19, loss_ctc=15.99
24epoch:train:2600-2700batch: loss=11.3, loss_att=9.344, loss_ctc=15.88
24epoch:train:2700-2800batch: loss=11.24, loss_att=9.29, loss_ctc=15.78
24epoch:train:2800-2900batch: loss=11.48, loss_att=9.438, loss_ctc=16.24
24epoch:train:2900-3000batch: loss=10.98, loss_att=9.021, loss_ctc=15.55
Validation average objf: -5.999305 over 5505.0 utts
24epoch:train:3000-3100batch: loss=12.26, loss_att=10.11, loss_ctc=17.27
24epoch:train:3100-3200batch: loss=10.93, loss_att=9.004, loss_ctc=15.43
24epoch:train:3200-3300batch: loss=11.46, loss_att=9.516, loss_ctc=15.98
24epoch:train:3300-3400batch: loss=10.98, loss_att=9.073, loss_ctc=15.44
24epoch:train:3400-3500batch: loss=11.28, loss_att=9.256, loss_ctc=16.01
24epoch:train:3500-3600batch: loss=11.51, loss_att=9.387, loss_ctc=16.45
24epoch:train:3600-3700batch: loss=11.24, loss_att=9.215, loss_ctc=15.98
24epoch:train:3700-3800batch: loss=11.69, loss_att=9.627, loss_ctc=16.52
24epoch:train:3800-3900batch: loss=11.95, loss_att=9.869, loss_ctc=16.82
24epoch:train:3900-4000batch: loss=11.82, loss_att=9.832, loss_ctc=16.45
Validation average objf: -5.930595 over 5505.0 utts
24epoch:train:4000-4100batch: loss=11.78, loss_att=9.68, loss_ctc=16.66
24epoch:train:4100-4200batch: loss=11.72, loss_att=9.667, loss_ctc=16.51
24epoch:train:4200-4300batch: loss=12.2, loss_att=10.08, loss_ctc=17.14
24epoch:train:4300-4400batch: loss=12.02, loss_att=9.983, loss_ctc=16.76
24epoch:train:4400-4500batch: loss=10.84, loss_att=9.041, loss_ctc=15.03
24epoch:train:4500-4600batch: loss=12.26, loss_att=10.16, loss_ctc=17.16
24epoch:train:4600-4700batch: loss=11.44, loss_att=9.493, loss_ctc=15.98
24epoch:train:4700-4800batch: loss=11.68, loss_att=9.582, loss_ctc=16.57
24epoch:train:4800-4900batch: loss=11.44, loss_att=9.421, loss_ctc=16.15
24epoch:train:4900-5000batch: loss=11.91, loss_att=9.733, loss_ctc=16.98
Validation average objf: -6.059424 over 5505.0 utts
24epoch:train:5000-5100batch: loss=12.44, loss_att=10.17, loss_ctc=17.75
24epoch:train:5100-5200batch: loss=12.04, loss_att=9.903, loss_ctc=17.04
24epoch:train:5200-5300batch: loss=11.39, loss_att=9.383, loss_ctc=16.07
24epoch:train:5300-5400batch: loss=11.64, loss_att=9.583, loss_ctc=16.43
24epoch:train:5400-5500batch: loss=11.71, loss_att=9.646, loss_ctc=16.52
24epoch:train:5500-5600batch: loss=11.4, loss_att=9.472, loss_ctc=15.9
24epoch:train:5600-5700batch: loss=11.46, loss_att=9.534, loss_ctc=15.95
24epoch:train:5700-5800batch: loss=11.42, loss_att=9.478, loss_ctc=15.95
24epoch:train:5800-5900batch: loss=11.66, loss_att=9.653, loss_ctc=16.35
24epoch:train:5900-6000batch: loss=11.54, loss_att=9.455, loss_ctc=16.4
Validation average objf: -6.076128 over 5505.0 utts
24epoch:train:6000-6100batch: loss=11.05, loss_att=9.191, loss_ctc=15.37
24epoch:train:6100-6200batch: loss=11.13, loss_att=9.21, loss_ctc=15.63
24epoch:train:6200-6300batch: loss=11.32, loss_att=9.368, loss_ctc=15.88
24epoch:train:6300-6400batch: loss=11.49, loss_att=9.486, loss_ctc=16.17
24epoch:train:6400-6500batch: loss=11.25, loss_att=9.311, loss_ctc=15.78
24epoch:train:6500-6600batch: loss=11.39, loss_att=9.425, loss_ctc=15.96
24epoch:train:6600-6700batch: loss=11.88, loss_att=9.774, loss_ctc=16.8
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-24.pt: epoch=24, learning_rate=0.0011201112570543802, objf=-0.7360365373245977, valid_objf=-6.0761282356948225
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-24-info
epoch 25, learning rate 0.0010965283533889813
25epoch:train:0-0batch: loss=8.631, loss_att=7.159, loss_ctc=12.07
25epoch:train:0-100batch: loss=10.78, loss_att=8.891, loss_ctc=15.17
25epoch:train:100-200batch: loss=11.37, loss_att=9.385, loss_ctc=16.02
25epoch:train:200-300batch: loss=11.0, loss_att=9.002, loss_ctc=15.66
25epoch:train:300-400batch: loss=10.53, loss_att=8.575, loss_ctc=15.11
25epoch:train:400-500batch: loss=10.34, loss_att=8.509, loss_ctc=14.6
25epoch:train:500-600batch: loss=10.29, loss_att=8.486, loss_ctc=14.5
25epoch:train:600-700batch: loss=10.61, loss_att=8.656, loss_ctc=15.16
25epoch:train:700-800batch: loss=10.79, loss_att=8.88, loss_ctc=15.24
25epoch:train:800-900batch: loss=10.5, loss_att=8.587, loss_ctc=14.95
25epoch:train:900-1000batch: loss=10.5, loss_att=8.63, loss_ctc=14.86
Validation average objf: -5.926379 over 5505.0 utts
25epoch:train:1000-1100batch: loss=10.84, loss_att=8.905, loss_ctc=15.37
25epoch:train:1100-1200batch: loss=10.53, loss_att=8.582, loss_ctc=15.08
25epoch:train:1200-1300batch: loss=10.66, loss_att=8.759, loss_ctc=15.1
25epoch:train:1300-1400batch: loss=11.03, loss_att=9.018, loss_ctc=15.73
25epoch:train:1400-1500batch: loss=10.58, loss_att=8.651, loss_ctc=15.08
25epoch:train:1500-1600batch: loss=10.41, loss_att=8.525, loss_ctc=14.79
25epoch:train:1600-1700batch: loss=11.03, loss_att=9.011, loss_ctc=15.76
25epoch:train:1700-1800batch: loss=10.69, loss_att=8.766, loss_ctc=15.17
25epoch:train:1800-1900batch: loss=11.21, loss_att=9.219, loss_ctc=15.85
25epoch:train:1900-2000batch: loss=10.77, loss_att=8.883, loss_ctc=15.16
Validation average objf: -6.139214 over 5505.0 utts
25epoch:train:2000-2100batch: loss=11.08, loss_att=9.094, loss_ctc=15.71
25epoch:train:2100-2200batch: loss=11.75, loss_att=9.644, loss_ctc=16.67
25epoch:train:2200-2300batch: loss=11.66, loss_att=9.569, loss_ctc=16.54
25epoch:train:2300-2400batch: loss=11.16, loss_att=9.194, loss_ctc=15.74
25epoch:train:2400-2500batch: loss=11.13, loss_att=9.116, loss_ctc=15.84
25epoch:train:2500-2600batch: loss=11.62, loss_att=9.485, loss_ctc=16.61
25epoch:train:2600-2700batch: loss=11.54, loss_att=9.435, loss_ctc=16.45
25epoch:train:2700-2800batch: loss=11.23, loss_att=9.198, loss_ctc=15.99
25epoch:train:2800-2900batch: loss=11.35, loss_att=9.403, loss_ctc=15.89
25epoch:train:2900-3000batch: loss=10.77, loss_att=8.909, loss_ctc=15.12
Validation average objf: -5.948650 over 5505.0 utts
25epoch:train:3000-3100batch: loss=11.26, loss_att=9.279, loss_ctc=15.87
25epoch:train:3100-3200batch: loss=11.27, loss_att=9.23, loss_ctc=16.01
25epoch:train:3200-3300batch: loss=11.57, loss_att=9.572, loss_ctc=16.25
25epoch:train:3300-3400batch: loss=11.52, loss_att=9.455, loss_ctc=16.35
25epoch:train:3400-3500batch: loss=11.93, loss_att=9.796, loss_ctc=16.92
25epoch:train:3500-3600batch: loss=11.85, loss_att=9.715, loss_ctc=16.84
25epoch:train:3600-3700batch: loss=11.83, loss_att=9.73, loss_ctc=16.73
25epoch:train:3700-3800batch: loss=11.56, loss_att=9.496, loss_ctc=16.39
25epoch:train:3800-3900batch: loss=11.97, loss_att=9.896, loss_ctc=16.82
25epoch:train:3900-4000batch: loss=11.59, loss_att=9.488, loss_ctc=16.5
Validation average objf: -6.026106 over 5505.0 utts
25epoch:train:4000-4100batch: loss=11.58, loss_att=9.523, loss_ctc=16.39
25epoch:train:4100-4200batch: loss=11.4, loss_att=9.431, loss_ctc=16.01
25epoch:train:4200-4300batch: loss=11.16, loss_att=9.257, loss_ctc=15.61
25epoch:train:4300-4400batch: loss=11.44, loss_att=9.483, loss_ctc=15.99
25epoch:train:4400-4500batch: loss=11.24, loss_att=9.259, loss_ctc=15.88
25epoch:train:4500-4600batch: loss=11.44, loss_att=9.472, loss_ctc=16.02
25epoch:train:4600-4700batch: loss=11.28, loss_att=9.338, loss_ctc=15.81
25epoch:train:4700-4800batch: loss=11.2, loss_att=9.203, loss_ctc=15.86
25epoch:train:4800-4900batch: loss=11.37, loss_att=9.443, loss_ctc=15.86
25epoch:train:4900-5000batch: loss=11.2, loss_att=9.255, loss_ctc=15.76
Validation average objf: -5.949554 over 5505.0 utts
25epoch:train:5000-5100batch: loss=11.51, loss_att=9.467, loss_ctc=16.29
25epoch:train:5100-5200batch: loss=11.49, loss_att=9.511, loss_ctc=16.12
25epoch:train:5200-5300batch: loss=11.77, loss_att=9.769, loss_ctc=16.43
25epoch:train:5300-5400batch: loss=11.38, loss_att=9.335, loss_ctc=16.15
25epoch:train:5400-5500batch: loss=11.28, loss_att=9.315, loss_ctc=15.86
25epoch:train:5500-5600batch: loss=11.29, loss_att=9.352, loss_ctc=15.82
25epoch:train:5600-5700batch: loss=12.02, loss_att=9.896, loss_ctc=16.98
25epoch:train:5700-5800batch: loss=11.95, loss_att=9.88, loss_ctc=16.78
25epoch:train:5800-5900batch: loss=11.81, loss_att=9.659, loss_ctc=16.83
25epoch:train:5900-6000batch: loss=12.06, loss_att=10.03, loss_ctc=16.79
Validation average objf: -6.120363 over 5505.0 utts
25epoch:train:6000-6100batch: loss=11.23, loss_att=9.292, loss_ctc=15.76
25epoch:train:6100-6200batch: loss=11.45, loss_att=9.456, loss_ctc=16.1
25epoch:train:6200-6300batch: loss=11.69, loss_att=9.634, loss_ctc=16.47
25epoch:train:6300-6400batch: loss=11.14, loss_att=9.249, loss_ctc=15.56
25epoch:train:6400-6500batch: loss=11.18, loss_att=9.195, loss_ctc=15.82
25epoch:train:6500-6600batch: loss=11.21, loss_att=9.3, loss_ctc=15.66
25epoch:train:6600-6700batch: loss=10.82, loss_att=8.96, loss_ctc=15.16
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-25.pt: epoch=25, learning_rate=0.0010965283533889813, objf=-0.7306276435727213, valid_objf=-6.120362880336058
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-25-info
epoch 26, learning rate 0.0010743749076821915
26epoch:train:0-0batch: loss=13.91, loss_att=11.08, loss_ctc=20.53
26epoch:train:0-100batch: loss=10.49, loss_att=8.663, loss_ctc=14.76
26epoch:train:100-200batch: loss=10.53, loss_att=8.613, loss_ctc=15.0
26epoch:train:200-300batch: loss=10.82, loss_att=8.867, loss_ctc=15.38
26epoch:train:300-400batch: loss=10.87, loss_att=8.957, loss_ctc=15.32
26epoch:train:400-500batch: loss=11.28, loss_att=9.237, loss_ctc=16.03
26epoch:train:500-600batch: loss=10.1, loss_att=8.283, loss_ctc=14.33
26epoch:train:600-700batch: loss=10.2, loss_att=8.421, loss_ctc=14.36
26epoch:train:700-800batch: loss=9.946, loss_att=8.156, loss_ctc=14.12
26epoch:train:800-900batch: loss=10.48, loss_att=8.573, loss_ctc=14.92
26epoch:train:900-1000batch: loss=10.49, loss_att=8.651, loss_ctc=14.77
Validation average objf: -6.015011 over 5505.0 utts
26epoch:train:1000-1100batch: loss=10.2, loss_att=8.382, loss_ctc=14.43
26epoch:train:1100-1200batch: loss=10.56, loss_att=8.678, loss_ctc=14.95
26epoch:train:1200-1300batch: loss=10.21, loss_att=8.358, loss_ctc=14.53
26epoch:train:1300-1400batch: loss=10.05, loss_att=8.18, loss_ctc=14.41
26epoch:train:1400-1500batch: loss=11.08, loss_att=9.039, loss_ctc=15.84
26epoch:train:1500-1600batch: loss=10.76, loss_att=8.832, loss_ctc=15.27
26epoch:train:1600-1700batch: loss=10.9, loss_att=8.923, loss_ctc=15.51
26epoch:train:1700-1800batch: loss=10.76, loss_att=8.85, loss_ctc=15.23
26epoch:train:1800-1900batch: loss=10.67, loss_att=8.781, loss_ctc=15.07
26epoch:train:1900-2000batch: loss=10.93, loss_att=9.06, loss_ctc=15.3
Validation average objf: -6.014726 over 5505.0 utts
26epoch:train:2000-2100batch: loss=10.36, loss_att=8.489, loss_ctc=14.72
26epoch:train:2100-2200batch: loss=10.65, loss_att=8.771, loss_ctc=15.03
26epoch:train:2200-2300batch: loss=10.64, loss_att=8.815, loss_ctc=14.89
26epoch:train:2300-2400batch: loss=10.97, loss_att=9.043, loss_ctc=15.46
26epoch:train:2400-2500batch: loss=10.54, loss_att=8.637, loss_ctc=14.97
26epoch:train:2500-2600batch: loss=10.78, loss_att=8.82, loss_ctc=15.34
26epoch:train:2600-2700batch: loss=10.92, loss_att=8.981, loss_ctc=15.45
26epoch:train:2700-2800batch: loss=10.77, loss_att=8.889, loss_ctc=15.15
26epoch:train:2800-2900batch: loss=11.09, loss_att=9.11, loss_ctc=15.7
26epoch:train:2900-3000batch: loss=10.92, loss_att=8.902, loss_ctc=15.64
Validation average objf: -5.945670 over 5505.0 utts
26epoch:train:3000-3100batch: loss=11.09, loss_att=9.095, loss_ctc=15.76
26epoch:train:3100-3200batch: loss=10.98, loss_att=9.026, loss_ctc=15.55
26epoch:train:3200-3300batch: loss=11.7, loss_att=9.586, loss_ctc=16.65
26epoch:train:3300-3400batch: loss=11.78, loss_att=9.743, loss_ctc=16.54
26epoch:train:3400-3500batch: loss=11.28, loss_att=9.347, loss_ctc=15.79
26epoch:train:3500-3600batch: loss=10.92, loss_att=9.027, loss_ctc=15.33
26epoch:train:3600-3700batch: loss=10.51, loss_att=8.687, loss_ctc=14.77
26epoch:train:3700-3800batch: loss=11.5, loss_att=9.499, loss_ctc=16.18
26epoch:train:3800-3900batch: loss=11.12, loss_att=9.208, loss_ctc=15.58
26epoch:train:3900-4000batch: loss=11.54, loss_att=9.506, loss_ctc=16.27
Validation average objf: -6.060371 over 5505.0 utts
26epoch:train:4000-4100batch: loss=11.14, loss_att=9.262, loss_ctc=15.51
26epoch:train:4100-4200batch: loss=11.69, loss_att=9.654, loss_ctc=16.45
26epoch:train:4200-4300batch: loss=11.98, loss_att=9.891, loss_ctc=16.84
26epoch:train:4300-4400batch: loss=11.52, loss_att=9.549, loss_ctc=16.13
26epoch:train:4400-4500batch: loss=11.19, loss_att=9.267, loss_ctc=15.69
26epoch:train:4500-4600batch: loss=11.78, loss_att=9.692, loss_ctc=16.65
26epoch:train:4600-4700batch: loss=11.37, loss_att=9.416, loss_ctc=15.92
26epoch:train:4700-4800batch: loss=11.74, loss_att=9.69, loss_ctc=16.51
26epoch:train:4800-4900batch: loss=11.31, loss_att=9.316, loss_ctc=15.95
26epoch:train:4900-5000batch: loss=11.39, loss_att=9.395, loss_ctc=16.06
Validation average objf: -6.125085 over 5505.0 utts
26epoch:train:5000-5100batch: loss=11.33, loss_att=9.382, loss_ctc=15.89
26epoch:train:5100-5200batch: loss=11.69, loss_att=9.597, loss_ctc=16.57
26epoch:train:5200-5300batch: loss=10.95, loss_att=9.081, loss_ctc=15.32
26epoch:train:5300-5400batch: loss=11.25, loss_att=9.294, loss_ctc=15.83
26epoch:train:5400-5500batch: loss=11.21, loss_att=9.325, loss_ctc=15.61
26epoch:train:5500-5600batch: loss=11.79, loss_att=9.817, loss_ctc=16.39
26epoch:train:5600-5700batch: loss=11.32, loss_att=9.222, loss_ctc=16.23
26epoch:train:5700-5800batch: loss=11.16, loss_att=9.331, loss_ctc=15.44
26epoch:train:5800-5900batch: loss=11.43, loss_att=9.448, loss_ctc=16.05
26epoch:train:5900-6000batch: loss=11.26, loss_att=9.333, loss_ctc=15.76
Validation average objf: -6.141276 over 5505.0 utts
26epoch:train:6000-6100batch: loss=11.31, loss_att=9.377, loss_ctc=15.83
26epoch:train:6100-6200batch: loss=11.22, loss_att=9.306, loss_ctc=15.68
26epoch:train:6200-6300batch: loss=10.61, loss_att=8.861, loss_ctc=14.71
26epoch:train:6300-6400batch: loss=11.13, loss_att=9.189, loss_ctc=15.66
26epoch:train:6400-6500batch: loss=11.25, loss_att=9.311, loss_ctc=15.79
26epoch:train:6500-6600batch: loss=10.55, loss_att=8.684, loss_ctc=14.9
26epoch:train:6600-6700batch: loss=10.73, loss_att=8.894, loss_ctc=15.01
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-26.pt: epoch=26, learning_rate=0.0010743749076821915, objf=-0.7155236906128505, valid_objf=-6.141275686875567
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-26-info
epoch 27, learning rate 0.0010535091177148554
27epoch:train:0-0batch: loss=9.342, loss_att=7.452, loss_ctc=13.75
27epoch:train:0-100batch: loss=10.41, loss_att=8.583, loss_ctc=14.66
27epoch:train:100-200batch: loss=10.42, loss_att=8.556, loss_ctc=14.75
27epoch:train:200-300batch: loss=10.84, loss_att=8.926, loss_ctc=15.3
27epoch:train:300-400batch: loss=10.41, loss_att=8.546, loss_ctc=14.76
27epoch:train:400-500batch: loss=10.73, loss_att=8.817, loss_ctc=15.19
27epoch:train:500-600batch: loss=10.55, loss_att=8.585, loss_ctc=15.14
27epoch:train:600-700batch: loss=10.2, loss_att=8.361, loss_ctc=14.5
27epoch:train:700-800batch: loss=10.38, loss_att=8.483, loss_ctc=14.81
27epoch:train:800-900batch: loss=10.63, loss_att=8.755, loss_ctc=15.0
27epoch:train:900-1000batch: loss=10.16, loss_att=8.333, loss_ctc=14.42
Validation average objf: -6.117830 over 5505.0 utts
27epoch:train:1000-1100batch: loss=10.03, loss_att=8.233, loss_ctc=14.23
27epoch:train:1100-1200batch: loss=10.49, loss_att=8.556, loss_ctc=15.01
27epoch:train:1200-1300batch: loss=10.86, loss_att=8.893, loss_ctc=15.44
27epoch:train:1300-1400batch: loss=10.36, loss_att=8.457, loss_ctc=14.81
27epoch:train:1400-1500batch: loss=10.33, loss_att=8.456, loss_ctc=14.7
27epoch:train:1500-1600batch: loss=10.1, loss_att=8.298, loss_ctc=14.3
27epoch:train:1600-1700batch: loss=10.19, loss_att=8.38, loss_ctc=14.42
27epoch:train:1700-1800batch: loss=10.44, loss_att=8.539, loss_ctc=14.88
27epoch:train:1800-1900batch: loss=10.56, loss_att=8.715, loss_ctc=14.86
27epoch:train:1900-2000batch: loss=10.5, loss_att=8.633, loss_ctc=14.87
Validation average objf: -6.003736 over 5505.0 utts
27epoch:train:2000-2100batch: loss=10.69, loss_att=8.789, loss_ctc=15.13
27epoch:train:2100-2200batch: loss=10.76, loss_att=8.876, loss_ctc=15.17
27epoch:train:2200-2300batch: loss=10.94, loss_att=8.987, loss_ctc=15.49
27epoch:train:2300-2400batch: loss=11.19, loss_att=9.203, loss_ctc=15.83
27epoch:train:2400-2500batch: loss=11.05, loss_att=9.051, loss_ctc=15.71
27epoch:train:2500-2600batch: loss=11.12, loss_att=9.131, loss_ctc=15.77
27epoch:train:2600-2700batch: loss=10.4, loss_att=8.531, loss_ctc=14.77
27epoch:train:2700-2800batch: loss=11.59, loss_att=9.553, loss_ctc=16.35
27epoch:train:2800-2900batch: loss=10.72, loss_att=8.92, loss_ctc=14.92
27epoch:train:2900-3000batch: loss=10.94, loss_att=8.981, loss_ctc=15.51
Validation average objf: -5.934589 over 5505.0 utts
27epoch:train:3000-3100batch: loss=10.78, loss_att=8.801, loss_ctc=15.41
27epoch:train:3100-3200batch: loss=11.26, loss_att=9.288, loss_ctc=15.86
27epoch:train:3200-3300batch: loss=10.81, loss_att=8.911, loss_ctc=15.24
27epoch:train:3300-3400batch: loss=11.12, loss_att=9.11, loss_ctc=15.81
27epoch:train:3400-3500batch: loss=11.3, loss_att=9.385, loss_ctc=15.76
27epoch:train:3500-3600batch: loss=10.87, loss_att=9.015, loss_ctc=15.19
27epoch:train:3600-3700batch: loss=11.36, loss_att=9.415, loss_ctc=15.88
27epoch:train:3700-3800batch: loss=10.97, loss_att=8.998, loss_ctc=15.57
27epoch:train:3800-3900batch: loss=10.99, loss_att=9.093, loss_ctc=15.42
27epoch:train:3900-4000batch: loss=10.7, loss_att=8.863, loss_ctc=15.0
Validation average objf: -5.918985 over 5505.0 utts
27epoch:train:4000-4100batch: loss=11.02, loss_att=9.123, loss_ctc=15.46
27epoch:train:4100-4200batch: loss=10.76, loss_att=8.826, loss_ctc=15.26
27epoch:train:4200-4300batch: loss=11.0, loss_att=9.032, loss_ctc=15.59
27epoch:train:4300-4400batch: loss=11.58, loss_att=9.569, loss_ctc=16.27
27epoch:train:4400-4500batch: loss=11.49, loss_att=9.487, loss_ctc=16.16
27epoch:train:4500-4600batch: loss=11.32, loss_att=9.241, loss_ctc=16.16
27epoch:train:4600-4700batch: loss=10.99, loss_att=9.071, loss_ctc=15.46
27epoch:train:4700-4800batch: loss=11.34, loss_att=9.371, loss_ctc=15.95
27epoch:train:4800-4900batch: loss=11.37, loss_att=9.317, loss_ctc=16.16
27epoch:train:4900-5000batch: loss=11.5, loss_att=9.524, loss_ctc=16.1
Validation average objf: -5.950000 over 5505.0 utts
27epoch:train:5000-5100batch: loss=12.03, loss_att=9.835, loss_ctc=17.16
27epoch:train:5100-5200batch: loss=11.56, loss_att=9.611, loss_ctc=16.12
27epoch:train:5200-5300batch: loss=11.9, loss_att=9.871, loss_ctc=16.63
27epoch:train:5300-5400batch: loss=11.13, loss_att=9.204, loss_ctc=15.61
27epoch:train:5400-5500batch: loss=11.31, loss_att=9.351, loss_ctc=15.89
27epoch:train:5500-5600batch: loss=11.39, loss_att=9.453, loss_ctc=15.93
27epoch:train:5600-5700batch: loss=11.29, loss_att=9.312, loss_ctc=15.92
27epoch:train:5700-5800batch: loss=11.26, loss_att=9.337, loss_ctc=15.74
27epoch:train:5800-5900batch: loss=10.79, loss_att=8.872, loss_ctc=15.25
27epoch:train:5900-6000batch: loss=11.12, loss_att=9.238, loss_ctc=15.5
Validation average objf: -6.040279 over 5505.0 utts
27epoch:train:6000-6100batch: loss=11.24, loss_att=9.249, loss_ctc=15.87
27epoch:train:6100-6200batch: loss=10.61, loss_att=8.913, loss_ctc=14.58
27epoch:train:6200-6300batch: loss=11.1, loss_att=9.155, loss_ctc=15.63
27epoch:train:6300-6400batch: loss=11.12, loss_att=9.156, loss_ctc=15.69
27epoch:train:6400-6500batch: loss=10.96, loss_att=8.922, loss_ctc=15.7
27epoch:train:6500-6600batch: loss=10.87, loss_att=9.023, loss_ctc=15.17
27epoch:train:6600-6700batch: loss=11.24, loss_att=9.336, loss_ctc=15.67
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-27.pt: epoch=27, learning_rate=0.0010535091177148554, objf=-0.7100690324803071, valid_objf=-6.040278723887375
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-27-info
epoch 28, learning rate 0.0010338164923520217
28epoch:train:0-0batch: loss=8.382, loss_att=6.751, loss_ctc=12.19
28epoch:train:0-100batch: loss=10.68, loss_att=8.758, loss_ctc=15.18
28epoch:train:100-200batch: loss=10.51, loss_att=8.626, loss_ctc=14.9
28epoch:train:200-300batch: loss=10.47, loss_att=8.579, loss_ctc=14.87
28epoch:train:300-400batch: loss=10.72, loss_att=8.769, loss_ctc=15.28
28epoch:train:400-500batch: loss=10.61, loss_att=8.671, loss_ctc=15.12
28epoch:train:500-600batch: loss=10.34, loss_att=8.463, loss_ctc=14.73
28epoch:train:600-700batch: loss=9.927, loss_att=8.1, loss_ctc=14.19
28epoch:train:700-800batch: loss=10.31, loss_att=8.455, loss_ctc=14.65
28epoch:train:800-900batch: loss=10.6, loss_att=8.646, loss_ctc=15.16
28epoch:train:900-1000batch: loss=10.3, loss_att=8.4, loss_ctc=14.74
Validation average objf: -5.994058 over 5505.0 utts
28epoch:train:1000-1100batch: loss=9.816, loss_att=8.037, loss_ctc=13.97
28epoch:train:1100-1200batch: loss=11.05, loss_att=9.043, loss_ctc=15.75
28epoch:train:1200-1300batch: loss=9.756, loss_att=7.933, loss_ctc=14.01
28epoch:train:1300-1400batch: loss=10.35, loss_att=8.496, loss_ctc=14.67
28epoch:train:1400-1500batch: loss=10.61, loss_att=8.672, loss_ctc=15.14
28epoch:train:1500-1600batch: loss=10.63, loss_att=8.743, loss_ctc=15.05
28epoch:train:1600-1700batch: loss=9.761, loss_att=8.077, loss_ctc=13.69
28epoch:train:1700-1800batch: loss=10.78, loss_att=8.825, loss_ctc=15.33
28epoch:train:1800-1900batch: loss=10.34, loss_att=8.554, loss_ctc=14.51
28epoch:train:1900-2000batch: loss=10.3, loss_att=8.444, loss_ctc=14.62
Validation average objf: -5.900680 over 5505.0 utts
28epoch:train:2000-2100batch: loss=10.74, loss_att=8.735, loss_ctc=15.42
28epoch:train:2100-2200batch: loss=10.54, loss_att=8.706, loss_ctc=14.82
28epoch:train:2200-2300batch: loss=10.58, loss_att=8.736, loss_ctc=14.88
28epoch:train:2300-2400batch: loss=9.881, loss_att=8.122, loss_ctc=13.99
28epoch:train:2400-2500batch: loss=10.33, loss_att=8.526, loss_ctc=14.54
28epoch:train:2500-2600batch: loss=9.687, loss_att=7.951, loss_ctc=13.74
28epoch:train:2600-2700batch: loss=11.16, loss_att=9.16, loss_ctc=15.81
28epoch:train:2700-2800batch: loss=11.05, loss_att=9.094, loss_ctc=15.62
28epoch:train:2800-2900batch: loss=10.55, loss_att=8.656, loss_ctc=14.98
28epoch:train:2900-3000batch: loss=11.07, loss_att=9.122, loss_ctc=15.6
Validation average objf: -5.927750 over 5505.0 utts
28epoch:train:3000-3100batch: loss=10.93, loss_att=9.012, loss_ctc=15.42
28epoch:train:3100-3200batch: loss=10.81, loss_att=8.928, loss_ctc=15.19
28epoch:train:3200-3300batch: loss=11.12, loss_att=9.2, loss_ctc=15.6
28epoch:train:3300-3400batch: loss=10.97, loss_att=8.974, loss_ctc=15.62
28epoch:train:3400-3500batch: loss=10.12, loss_att=8.395, loss_ctc=14.14
28epoch:train:3500-3600batch: loss=11.5, loss_att=9.449, loss_ctc=16.27
28epoch:train:3600-3700batch: loss=11.53, loss_att=9.549, loss_ctc=16.16
28epoch:train:3700-3800batch: loss=11.28, loss_att=9.239, loss_ctc=16.04
28epoch:train:3800-3900batch: loss=11.74, loss_att=9.741, loss_ctc=16.42
28epoch:train:3900-4000batch: loss=10.99, loss_att=9.037, loss_ctc=15.56
Validation average objf: -6.031478 over 5505.0 utts
28epoch:train:4000-4100batch: loss=11.11, loss_att=9.18, loss_ctc=15.6
28epoch:train:4100-4200batch: loss=11.21, loss_att=9.284, loss_ctc=15.71
28epoch:train:4200-4300batch: loss=10.63, loss_att=8.775, loss_ctc=14.95
28epoch:train:4300-4400batch: loss=10.91, loss_att=9.019, loss_ctc=15.31
28epoch:train:4400-4500batch: loss=11.13, loss_att=9.2, loss_ctc=15.64
28epoch:train:4500-4600batch: loss=11.19, loss_att=9.281, loss_ctc=15.64
28epoch:train:4600-4700batch: loss=10.61, loss_att=8.77, loss_ctc=14.91
28epoch:train:4700-4800batch: loss=10.96, loss_att=9.049, loss_ctc=15.43
28epoch:train:4800-4900batch: loss=10.96, loss_att=9.126, loss_ctc=15.25
28epoch:train:4900-5000batch: loss=11.48, loss_att=9.437, loss_ctc=16.25
Validation average objf: -5.974120 over 5505.0 utts
28epoch:train:5000-5100batch: loss=11.17, loss_att=9.198, loss_ctc=15.77
28epoch:train:5100-5200batch: loss=10.8, loss_att=8.933, loss_ctc=15.14
28epoch:train:5200-5300batch: loss=11.08, loss_att=9.161, loss_ctc=15.54
28epoch:train:5300-5400batch: loss=11.68, loss_att=9.63, loss_ctc=16.48
28epoch:train:5400-5500batch: loss=11.95, loss_att=9.779, loss_ctc=17.02
28epoch:train:5500-5600batch: loss=11.03, loss_att=9.062, loss_ctc=15.62
28epoch:train:5600-5700batch: loss=11.16, loss_att=9.299, loss_ctc=15.51
28epoch:train:5700-5800batch: loss=10.7, loss_att=8.894, loss_ctc=14.92
28epoch:train:5800-5900batch: loss=11.48, loss_att=9.463, loss_ctc=16.18
28epoch:train:5900-6000batch: loss=10.69, loss_att=8.965, loss_ctc=14.72
Validation average objf: -5.982257 over 5505.0 utts
28epoch:train:6000-6100batch: loss=10.66, loss_att=8.778, loss_ctc=15.06
28epoch:train:6100-6200batch: loss=11.05, loss_att=9.124, loss_ctc=15.55
28epoch:train:6200-6300batch: loss=11.23, loss_att=9.282, loss_ctc=15.76
28epoch:train:6300-6400batch: loss=11.12, loss_att=9.247, loss_ctc=15.48
28epoch:train:6400-6500batch: loss=11.43, loss_att=9.399, loss_ctc=16.18
28epoch:train:6500-6600batch: loss=10.47, loss_att=8.626, loss_ctc=14.76
28epoch:train:6600-6700batch: loss=10.84, loss_att=8.974, loss_ctc=15.2
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-28.pt: epoch=28, learning_rate=0.0010338164923520217, objf=-0.7016373410633808, valid_objf=-5.9822568971389645
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-28-info
epoch 29, learning rate 0.0010151884359423649
29epoch:train:0-0batch: loss=10.62, loss_att=8.67, loss_ctc=15.17
29epoch:train:0-100batch: loss=10.28, loss_att=8.35, loss_ctc=14.77
29epoch:train:100-200batch: loss=10.12, loss_att=8.299, loss_ctc=14.36
29epoch:train:200-300batch: loss=9.714, loss_att=7.977, loss_ctc=13.77
29epoch:train:300-400batch: loss=10.07, loss_att=8.241, loss_ctc=14.33
29epoch:train:400-500batch: loss=10.24, loss_att=8.416, loss_ctc=14.51
29epoch:train:500-600batch: loss=11.01, loss_att=9.048, loss_ctc=15.6
29epoch:train:600-700batch: loss=10.18, loss_att=8.384, loss_ctc=14.36
29epoch:train:700-800batch: loss=9.637, loss_att=7.895, loss_ctc=13.7
29epoch:train:800-900batch: loss=10.36, loss_att=8.457, loss_ctc=14.8
29epoch:train:900-1000batch: loss=10.53, loss_att=8.71, loss_ctc=14.79
Validation average objf: -5.953816 over 5505.0 utts
29epoch:train:1000-1100batch: loss=10.37, loss_att=8.515, loss_ctc=14.69
29epoch:train:1100-1200batch: loss=10.06, loss_att=8.224, loss_ctc=14.34
29epoch:train:1200-1300batch: loss=9.996, loss_att=8.159, loss_ctc=14.28
29epoch:train:1300-1400batch: loss=10.62, loss_att=8.699, loss_ctc=15.09
29epoch:train:1400-1500batch: loss=10.47, loss_att=8.604, loss_ctc=14.83
29epoch:train:1500-1600batch: loss=10.08, loss_att=8.287, loss_ctc=14.25
29epoch:train:1600-1700batch: loss=9.757, loss_att=8.02, loss_ctc=13.81
29epoch:train:1700-1800batch: loss=10.63, loss_att=8.742, loss_ctc=15.03
29epoch:train:1800-1900batch: loss=9.7, loss_att=7.959, loss_ctc=13.76
29epoch:train:1900-2000batch: loss=10.52, loss_att=8.624, loss_ctc=14.95
Validation average objf: -5.806188 over 5505.0 utts
29epoch:train:2000-2100batch: loss=9.822, loss_att=8.076, loss_ctc=13.9
29epoch:train:2100-2200batch: loss=10.51, loss_att=8.608, loss_ctc=14.96
29epoch:train:2200-2300batch: loss=11.28, loss_att=9.286, loss_ctc=15.94
29epoch:train:2300-2400batch: loss=10.33, loss_att=8.475, loss_ctc=14.67
29epoch:train:2400-2500batch: loss=9.997, loss_att=8.276, loss_ctc=14.01
29epoch:train:2500-2600batch: loss=10.69, loss_att=8.757, loss_ctc=15.2
29epoch:train:2600-2700batch: loss=10.83, loss_att=8.975, loss_ctc=15.15
29epoch:train:2700-2800batch: loss=10.6, loss_att=8.662, loss_ctc=15.11
29epoch:train:2800-2900batch: loss=10.6, loss_att=8.674, loss_ctc=15.11
29epoch:train:2900-3000batch: loss=11.24, loss_att=9.196, loss_ctc=16.0
Validation average objf: -5.979956 over 5505.0 utts
29epoch:train:3000-3100batch: loss=10.33, loss_att=8.444, loss_ctc=14.72
29epoch:train:3100-3200batch: loss=10.39, loss_att=8.506, loss_ctc=14.8
29epoch:train:3200-3300batch: loss=10.99, loss_att=9.022, loss_ctc=15.57
29epoch:train:3300-3400batch: loss=11.17, loss_att=9.239, loss_ctc=15.67
29epoch:train:3400-3500batch: loss=10.4, loss_att=8.513, loss_ctc=14.8
29epoch:train:3500-3600batch: loss=10.39, loss_att=8.649, loss_ctc=14.44
29epoch:train:3600-3700batch: loss=10.94, loss_att=9.019, loss_ctc=15.43
29epoch:train:3700-3800batch: loss=10.77, loss_att=8.958, loss_ctc=15.01
29epoch:train:3800-3900batch: loss=10.36, loss_att=8.517, loss_ctc=14.67
29epoch:train:3900-4000batch: loss=11.75, loss_att=9.653, loss_ctc=16.65
Validation average objf: -6.021159 over 5505.0 utts
29epoch:train:4000-4100batch: loss=10.65, loss_att=8.838, loss_ctc=14.89
29epoch:train:4100-4200batch: loss=10.7, loss_att=8.824, loss_ctc=15.09
29epoch:train:4200-4300batch: loss=11.07, loss_att=9.161, loss_ctc=15.52
29epoch:train:4300-4400batch: loss=11.19, loss_att=9.223, loss_ctc=15.79
29epoch:train:4400-4500batch: loss=11.08, loss_att=9.08, loss_ctc=15.73
29epoch:train:4500-4600batch: loss=10.86, loss_att=9.022, loss_ctc=15.15
29epoch:train:4600-4700batch: loss=10.76, loss_att=8.9, loss_ctc=15.1
29epoch:train:4700-4800batch: loss=10.36, loss_att=8.601, loss_ctc=14.48
29epoch:train:4800-4900batch: loss=10.91, loss_att=8.996, loss_ctc=15.38
29epoch:train:4900-5000batch: loss=10.7, loss_att=8.912, loss_ctc=14.86
Validation average objf: -5.944583 over 5505.0 utts
29epoch:train:5000-5100batch: loss=11.53, loss_att=9.487, loss_ctc=16.3
29epoch:train:5100-5200batch: loss=10.59, loss_att=8.741, loss_ctc=14.89
29epoch:train:5200-5300batch: loss=11.46, loss_att=9.509, loss_ctc=16.01
29epoch:train:5300-5400batch: loss=11.28, loss_att=9.285, loss_ctc=15.93
29epoch:train:5400-5500batch: loss=11.19, loss_att=9.233, loss_ctc=15.74
29epoch:train:5500-5600batch: loss=11.65, loss_att=9.64, loss_ctc=16.35
29epoch:train:5600-5700batch: loss=11.43, loss_att=9.518, loss_ctc=15.88
29epoch:train:5700-5800batch: loss=10.52, loss_att=8.729, loss_ctc=14.7
29epoch:train:5800-5900batch: loss=10.43, loss_att=8.726, loss_ctc=14.41
29epoch:train:5900-6000batch: loss=10.51, loss_att=8.695, loss_ctc=14.74
Validation average objf: -6.116623 over 5505.0 utts
29epoch:train:6000-6100batch: loss=11.14, loss_att=9.2, loss_ctc=15.65
29epoch:train:6100-6200batch: loss=12.0, loss_att=9.933, loss_ctc=16.83
29epoch:train:6200-6300batch: loss=10.7, loss_att=8.902, loss_ctc=14.91
29epoch:train:6300-6400batch: loss=10.37, loss_att=8.597, loss_ctc=14.51
29epoch:train:6400-6500batch: loss=10.29, loss_att=8.597, loss_ctc=14.23
29epoch:train:6500-6600batch: loss=10.69, loss_att=8.861, loss_ctc=14.97
29epoch:train:6600-6700batch: loss=10.6, loss_att=8.799, loss_ctc=14.82
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-29.pt: epoch=29, learning_rate=0.0010151884359423649, objf=-0.6911313359900452, valid_objf=-6.116622672570391
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-29-info
epoch 30, learning rate 0.000997532363986211
30epoch:train:0-0batch: loss=11.73, loss_att=10.08, loss_ctc=15.6
30epoch:train:0-100batch: loss=9.949, loss_att=8.136, loss_ctc=14.18
30epoch:train:100-200batch: loss=9.66, loss_att=7.883, loss_ctc=13.81
30epoch:train:200-300batch: loss=10.4, loss_att=8.553, loss_ctc=14.7
30epoch:train:300-400batch: loss=10.04, loss_att=8.229, loss_ctc=14.25
30epoch:train:400-500batch: loss=10.22, loss_att=8.393, loss_ctc=14.5
30epoch:train:500-600batch: loss=10.48, loss_att=8.622, loss_ctc=14.83
30epoch:train:600-700batch: loss=9.786, loss_att=8.023, loss_ctc=13.9
30epoch:train:700-800batch: loss=10.42, loss_att=8.593, loss_ctc=14.68
30epoch:train:800-900batch: loss=9.706, loss_att=7.934, loss_ctc=13.84
30epoch:train:900-1000batch: loss=9.76, loss_att=8.033, loss_ctc=13.79
Validation average objf: -5.894823 over 5505.0 utts
30epoch:train:1000-1100batch: loss=10.34, loss_att=8.421, loss_ctc=14.83
30epoch:train:1100-1200batch: loss=10.14, loss_att=8.292, loss_ctc=14.44
30epoch:train:1200-1300batch: loss=9.418, loss_att=7.734, loss_ctc=13.35
30epoch:train:1300-1400batch: loss=10.21, loss_att=8.427, loss_ctc=14.39
30epoch:train:1400-1500batch: loss=10.63, loss_att=8.677, loss_ctc=15.18
30epoch:train:1500-1600batch: loss=10.04, loss_att=8.232, loss_ctc=14.25
30epoch:train:1600-1700batch: loss=9.811, loss_att=8.04, loss_ctc=13.94
30epoch:train:1700-1800batch: loss=10.43, loss_att=8.549, loss_ctc=14.82
30epoch:train:1800-1900batch: loss=10.65, loss_att=8.738, loss_ctc=15.11
30epoch:train:1900-2000batch: loss=10.13, loss_att=8.361, loss_ctc=14.27
Validation average objf: -5.854326 over 5505.0 utts
30epoch:train:2000-2100batch: loss=10.74, loss_att=8.831, loss_ctc=15.2
30epoch:train:2100-2200batch: loss=10.64, loss_att=8.673, loss_ctc=15.23
30epoch:train:2200-2300batch: loss=10.44, loss_att=8.599, loss_ctc=14.74
30epoch:train:2300-2400batch: loss=10.64, loss_att=8.693, loss_ctc=15.2
30epoch:train:2400-2500batch: loss=10.49, loss_att=8.633, loss_ctc=14.81
30epoch:train:2500-2600batch: loss=10.1, loss_att=8.324, loss_ctc=14.25
30epoch:train:2600-2700batch: loss=11.02, loss_att=9.085, loss_ctc=15.54
30epoch:train:2700-2800batch: loss=10.61, loss_att=8.772, loss_ctc=14.9
30epoch:train:2800-2900batch: loss=10.57, loss_att=8.638, loss_ctc=15.07
30epoch:train:2900-3000batch: loss=10.43, loss_att=8.509, loss_ctc=14.91
Validation average objf: -5.841806 over 5505.0 utts
30epoch:train:3000-3100batch: loss=11.15, loss_att=9.184, loss_ctc=15.75
30epoch:train:3100-3200batch: loss=11.1, loss_att=9.106, loss_ctc=15.77
30epoch:train:3200-3300batch: loss=10.97, loss_att=9.009, loss_ctc=15.53
30epoch:train:3300-3400batch: loss=11.15, loss_att=9.153, loss_ctc=15.81
30epoch:train:3400-3500batch: loss=10.77, loss_att=8.807, loss_ctc=15.34
30epoch:train:3500-3600batch: loss=10.83, loss_att=8.831, loss_ctc=15.49
30epoch:train:3600-3700batch: loss=10.09, loss_att=8.326, loss_ctc=14.21
30epoch:train:3700-3800batch: loss=11.28, loss_att=9.222, loss_ctc=16.09
30epoch:train:3800-3900batch: loss=10.97, loss_att=9.076, loss_ctc=15.4
30epoch:train:3900-4000batch: loss=10.97, loss_att=9.011, loss_ctc=15.54
Validation average objf: -5.973161 over 5505.0 utts
30epoch:train:4000-4100batch: loss=10.84, loss_att=8.954, loss_ctc=15.24
30epoch:train:4100-4200batch: loss=10.69, loss_att=8.834, loss_ctc=15.03
30epoch:train:4200-4300batch: loss=11.07, loss_att=9.191, loss_ctc=15.44
30epoch:train:4300-4400batch: loss=10.85, loss_att=9.027, loss_ctc=15.11
30epoch:train:4400-4500batch: loss=10.76, loss_att=8.847, loss_ctc=15.22
30epoch:train:4500-4600batch: loss=10.36, loss_att=8.596, loss_ctc=14.46
30epoch:train:4600-4700batch: loss=10.48, loss_att=8.74, loss_ctc=14.54
30epoch:train:4700-4800batch: loss=11.19, loss_att=9.168, loss_ctc=15.9
30epoch:train:4800-4900batch: loss=10.39, loss_att=8.56, loss_ctc=14.67
30epoch:train:4900-5000batch: loss=10.48, loss_att=8.682, loss_ctc=14.68
Validation average objf: -6.008778 over 5505.0 utts
30epoch:train:5000-5100batch: loss=10.62, loss_att=8.76, loss_ctc=14.95
30epoch:train:5100-5200batch: loss=11.1, loss_att=9.226, loss_ctc=15.47
30epoch:train:5200-5300batch: loss=10.86, loss_att=8.976, loss_ctc=15.24
30epoch:train:5300-5400batch: loss=11.19, loss_att=9.327, loss_ctc=15.53
30epoch:train:5400-5500batch: loss=10.9, loss_att=9.004, loss_ctc=15.32
30epoch:train:5500-5600batch: loss=11.71, loss_att=9.615, loss_ctc=16.59
30epoch:train:5600-5700batch: loss=11.19, loss_att=9.22, loss_ctc=15.8
30epoch:train:5700-5800batch: loss=10.92, loss_att=9.059, loss_ctc=15.26
30epoch:train:5800-5900batch: loss=11.07, loss_att=9.042, loss_ctc=15.82
30epoch:train:5900-6000batch: loss=10.66, loss_att=8.828, loss_ctc=14.92
Validation average objf: -6.101991 over 5505.0 utts
30epoch:train:6000-6100batch: loss=10.85, loss_att=8.955, loss_ctc=15.26
30epoch:train:6100-6200batch: loss=11.34, loss_att=9.44, loss_ctc=15.78
30epoch:train:6200-6300batch: loss=10.27, loss_att=8.581, loss_ctc=14.23
30epoch:train:6300-6400batch: loss=11.39, loss_att=9.305, loss_ctc=16.25
30epoch:train:6400-6500batch: loss=10.2, loss_att=8.411, loss_ctc=14.38
30epoch:train:6500-6600batch: loss=10.43, loss_att=8.752, loss_ctc=14.34
30epoch:train:6600-6700batch: loss=10.2, loss_att=8.421, loss_ctc=14.34
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-30.pt: epoch=30, learning_rate=0.000997532363986211, objf=-0.6881802678918023, valid_objf=-6.101991087647593
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-30-info
epoch 31, learning rate 0.0009807641711998259
31epoch:train:0-0batch: loss=13.03, loss_att=10.68, loss_ctc=18.5
31epoch:train:0-100batch: loss=10.73, loss_att=8.834, loss_ctc=15.14
31epoch:train:100-200batch: loss=10.38, loss_att=8.459, loss_ctc=14.86
31epoch:train:200-300batch: loss=9.72, loss_att=8.024, loss_ctc=13.68
31epoch:train:300-400batch: loss=10.35, loss_att=8.438, loss_ctc=14.81
31epoch:train:400-500batch: loss=10.08, loss_att=8.206, loss_ctc=14.45
31epoch:train:500-600batch: loss=10.16, loss_att=8.258, loss_ctc=14.6
31epoch:train:600-700batch: loss=10.25, loss_att=8.384, loss_ctc=14.6
31epoch:train:700-800batch: loss=9.356, loss_att=7.639, loss_ctc=13.36
31epoch:train:800-900batch: loss=10.22, loss_att=8.416, loss_ctc=14.43
31epoch:train:900-1000batch: loss=9.916, loss_att=8.108, loss_ctc=14.13
Validation average objf: -5.905464 over 5505.0 utts
31epoch:train:1000-1100batch: loss=10.17, loss_att=8.318, loss_ctc=14.49
31epoch:train:1100-1200batch: loss=10.44, loss_att=8.546, loss_ctc=14.87
31epoch:train:1200-1300batch: loss=9.591, loss_att=7.822, loss_ctc=13.72
31epoch:train:1300-1400batch: loss=9.598, loss_att=7.844, loss_ctc=13.69
31epoch:train:1400-1500batch: loss=10.03, loss_att=8.181, loss_ctc=14.34
31epoch:train:1500-1600batch: loss=10.65, loss_att=8.743, loss_ctc=15.11
31epoch:train:1600-1700batch: loss=10.41, loss_att=8.575, loss_ctc=14.7
31epoch:train:1700-1800batch: loss=10.44, loss_att=8.601, loss_ctc=14.74
31epoch:train:1800-1900batch: loss=10.02, loss_att=8.223, loss_ctc=14.2
31epoch:train:1900-2000batch: loss=9.886, loss_att=8.093, loss_ctc=14.07
Validation average objf: -5.888277 over 5505.0 utts
31epoch:train:2000-2100batch: loss=10.27, loss_att=8.505, loss_ctc=14.38
31epoch:train:2100-2200batch: loss=10.37, loss_att=8.53, loss_ctc=14.66
31epoch:train:2200-2300batch: loss=10.44, loss_att=8.587, loss_ctc=14.77
31epoch:train:2300-2400batch: loss=10.34, loss_att=8.497, loss_ctc=14.63
31epoch:train:2400-2500batch: loss=10.37, loss_att=8.555, loss_ctc=14.61
31epoch:train:2500-2600batch: loss=10.83, loss_att=8.886, loss_ctc=15.37
31epoch:train:2600-2700batch: loss=10.68, loss_att=8.761, loss_ctc=15.16
31epoch:train:2700-2800batch: loss=10.3, loss_att=8.51, loss_ctc=14.49
31epoch:train:2800-2900batch: loss=10.33, loss_att=8.54, loss_ctc=14.51
31epoch:train:2900-3000batch: loss=10.68, loss_att=8.74, loss_ctc=15.21
Validation average objf: -6.020043 over 5505.0 utts
31epoch:train:3000-3100batch: loss=10.46, loss_att=8.629, loss_ctc=14.73
31epoch:train:3100-3200batch: loss=10.51, loss_att=8.729, loss_ctc=14.66
31epoch:train:3200-3300batch: loss=10.86, loss_att=8.941, loss_ctc=15.33
31epoch:train:3300-3400batch: loss=10.53, loss_att=8.672, loss_ctc=14.88
31epoch:train:3400-3500batch: loss=10.77, loss_att=8.873, loss_ctc=15.19
31epoch:train:3500-3600batch: loss=10.25, loss_att=8.441, loss_ctc=14.49
31epoch:train:3600-3700batch: loss=9.98, loss_att=8.223, loss_ctc=14.08
31epoch:train:3700-3800batch: loss=10.47, loss_att=8.627, loss_ctc=14.78
31epoch:train:3800-3900batch: loss=10.64, loss_att=8.777, loss_ctc=14.99
31epoch:train:3900-4000batch: loss=10.5, loss_att=8.643, loss_ctc=14.84
Validation average objf: -6.018024 over 5505.0 utts
31epoch:train:4000-4100batch: loss=10.89, loss_att=9.005, loss_ctc=15.29
31epoch:train:4100-4200batch: loss=10.76, loss_att=8.861, loss_ctc=15.19
31epoch:train:4200-4300batch: loss=10.63, loss_att=8.8, loss_ctc=14.89
31epoch:train:4300-4400batch: loss=10.82, loss_att=9.032, loss_ctc=15.0
31epoch:train:4400-4500batch: loss=11.29, loss_att=9.249, loss_ctc=16.07
31epoch:train:4500-4600batch: loss=10.4, loss_att=8.596, loss_ctc=14.61
31epoch:train:4600-4700batch: loss=10.75, loss_att=8.854, loss_ctc=15.18
31epoch:train:4700-4800batch: loss=11.07, loss_att=9.095, loss_ctc=15.68
31epoch:train:4800-4900batch: loss=11.36, loss_att=9.384, loss_ctc=15.96
31epoch:train:4900-5000batch: loss=10.58, loss_att=8.782, loss_ctc=14.77
Validation average objf: -6.030167 over 5505.0 utts
31epoch:train:5000-5100batch: loss=10.87, loss_att=8.998, loss_ctc=15.25
31epoch:train:5100-5200batch: loss=9.674, loss_att=8.053, loss_ctc=13.46
31epoch:train:5200-5300batch: loss=10.16, loss_att=8.437, loss_ctc=14.19
31epoch:train:5300-5400batch: loss=10.83, loss_att=8.958, loss_ctc=15.18
31epoch:train:5400-5500batch: loss=10.53, loss_att=8.778, loss_ctc=14.6
31epoch:train:5500-5600batch: loss=11.08, loss_att=9.085, loss_ctc=15.73
31epoch:train:5600-5700batch: loss=10.67, loss_att=8.793, loss_ctc=15.04
31epoch:train:5700-5800batch: loss=10.89, loss_att=8.973, loss_ctc=15.36
31epoch:train:5800-5900batch: loss=10.92, loss_att=8.961, loss_ctc=15.48
31epoch:train:5900-6000batch: loss=10.52, loss_att=8.675, loss_ctc=14.83
Validation average objf: -6.082128 over 5505.0 utts
31epoch:train:6000-6100batch: loss=10.52, loss_att=8.734, loss_ctc=14.67
31epoch:train:6100-6200batch: loss=10.39, loss_att=8.631, loss_ctc=14.51
31epoch:train:6200-6300batch: loss=10.61, loss_att=8.712, loss_ctc=15.04
31epoch:train:6300-6400batch: loss=9.955, loss_att=8.353, loss_ctc=13.69
31epoch:train:6400-6500batch: loss=10.58, loss_att=8.723, loss_ctc=14.91
31epoch:train:6500-6600batch: loss=10.15, loss_att=8.426, loss_ctc=14.17
31epoch:train:6600-6700batch: loss=10.19, loss_att=8.452, loss_ctc=14.25
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-31.pt: epoch=31, learning_rate=0.0009807641711998259, objf=-0.6751169826185707, valid_objf=-6.082127753178928
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-31-info
epoch 32, learning rate 0.0009648141140037428
32epoch:train:0-0batch: loss=3.932, loss_att=3.589, loss_ctc=4.73
32epoch:train:0-100batch: loss=10.59, loss_att=8.708, loss_ctc=14.99
32epoch:train:100-200batch: loss=9.374, loss_att=7.705, loss_ctc=13.27
32epoch:train:200-300batch: loss=10.16, loss_att=8.42, loss_ctc=14.21
32epoch:train:300-400batch: loss=10.23, loss_att=8.346, loss_ctc=14.63
32epoch:train:400-500batch: loss=9.575, loss_att=7.888, loss_ctc=13.51
32epoch:train:500-600batch: loss=10.09, loss_att=8.292, loss_ctc=14.27
32epoch:train:600-700batch: loss=10.53, loss_att=8.667, loss_ctc=14.89
32epoch:train:700-800batch: loss=9.763, loss_att=7.999, loss_ctc=13.88
32epoch:train:800-900batch: loss=10.4, loss_att=8.487, loss_ctc=14.88
32epoch:train:900-1000batch: loss=9.727, loss_att=7.955, loss_ctc=13.86
Validation average objf: -6.048792 over 5505.0 utts
32epoch:train:1000-1100batch: loss=9.98, loss_att=8.209, loss_ctc=14.11
32epoch:train:1100-1200batch: loss=10.24, loss_att=8.408, loss_ctc=14.52
32epoch:train:1200-1300batch: loss=9.702, loss_att=7.937, loss_ctc=13.82
32epoch:train:1300-1400batch: loss=10.06, loss_att=8.236, loss_ctc=14.32
32epoch:train:1400-1500batch: loss=9.85, loss_att=8.098, loss_ctc=13.94
32epoch:train:1500-1600batch: loss=9.907, loss_att=8.134, loss_ctc=14.04
32epoch:train:1600-1700batch: loss=9.979, loss_att=8.17, loss_ctc=14.2
32epoch:train:1700-1800batch: loss=10.19, loss_att=8.321, loss_ctc=14.53
32epoch:train:1800-1900batch: loss=10.12, loss_att=8.3, loss_ctc=14.37
32epoch:train:1900-2000batch: loss=10.14, loss_att=8.333, loss_ctc=14.36
Validation average objf: -5.896604 over 5505.0 utts
32epoch:train:2000-2100batch: loss=10.49, loss_att=8.571, loss_ctc=14.96
32epoch:train:2100-2200batch: loss=10.6, loss_att=8.731, loss_ctc=14.96
32epoch:train:2200-2300batch: loss=9.782, loss_att=8.052, loss_ctc=13.82
32epoch:train:2300-2400batch: loss=9.84, loss_att=8.058, loss_ctc=14.0
32epoch:train:2400-2500batch: loss=10.08, loss_att=8.234, loss_ctc=14.4
32epoch:train:2500-2600batch: loss=10.53, loss_att=8.727, loss_ctc=14.75
32epoch:train:2600-2700batch: loss=10.37, loss_att=8.571, loss_ctc=14.55
32epoch:train:2700-2800batch: loss=10.64, loss_att=8.711, loss_ctc=15.14
32epoch:train:2800-2900batch: loss=10.77, loss_att=8.799, loss_ctc=15.38
32epoch:train:2900-3000batch: loss=9.923, loss_att=8.21, loss_ctc=13.92
Validation average objf: -6.008465 over 5505.0 utts
32epoch:train:3000-3100batch: loss=10.14, loss_att=8.336, loss_ctc=14.36
32epoch:train:3100-3200batch: loss=10.71, loss_att=8.792, loss_ctc=15.2
32epoch:train:3200-3300batch: loss=11.1, loss_att=9.152, loss_ctc=15.63
32epoch:train:3300-3400batch: loss=10.57, loss_att=8.658, loss_ctc=15.04
32epoch:train:3400-3500batch: loss=10.33, loss_att=8.497, loss_ctc=14.61
32epoch:train:3500-3600batch: loss=10.1, loss_att=8.315, loss_ctc=14.25
32epoch:train:3600-3700batch: loss=11.16, loss_att=9.182, loss_ctc=15.78
32epoch:train:3700-3800batch: loss=10.99, loss_att=9.119, loss_ctc=15.35
32epoch:train:3800-3900batch: loss=10.29, loss_att=8.469, loss_ctc=14.54
32epoch:train:3900-4000batch: loss=10.96, loss_att=9.013, loss_ctc=15.49
Validation average objf: -6.008465 over 5505.0 utts
32epoch:train:4000-4100batch: loss=10.74, loss_att=8.794, loss_ctc=15.28
32epoch:train:4100-4200batch: loss=11.15, loss_att=9.162, loss_ctc=15.8
32epoch:train:4200-4300batch: loss=11.07, loss_att=9.128, loss_ctc=15.6
32epoch:train:4300-4400batch: loss=10.65, loss_att=8.809, loss_ctc=14.95
32epoch:train:4400-4500batch: loss=10.74, loss_att=8.801, loss_ctc=15.28
32epoch:train:4500-4600batch: loss=11.73, loss_att=9.633, loss_ctc=16.61
32epoch:train:4600-4700batch: loss=11.02, loss_att=9.075, loss_ctc=15.57
32epoch:train:4700-4800batch: loss=10.32, loss_att=8.534, loss_ctc=14.48
32epoch:train:4800-4900batch: loss=10.3, loss_att=8.51, loss_ctc=14.49
32epoch:train:4900-5000batch: loss=10.37, loss_att=8.592, loss_ctc=14.52
Validation average objf: -5.847451 over 5505.0 utts
32epoch:train:5000-5100batch: loss=10.66, loss_att=8.756, loss_ctc=15.11
32epoch:train:5100-5200batch: loss=10.76, loss_att=8.893, loss_ctc=15.1
32epoch:train:5200-5300batch: loss=11.09, loss_att=9.159, loss_ctc=15.61
32epoch:train:5300-5400batch: loss=10.42, loss_att=8.691, loss_ctc=14.44
32epoch:train:5400-5500batch: loss=10.35, loss_att=8.578, loss_ctc=14.49
32epoch:train:5500-5600batch: loss=10.79, loss_att=8.883, loss_ctc=15.24
32epoch:train:5600-5700batch: loss=10.58, loss_att=8.703, loss_ctc=14.96
32epoch:train:5700-5800batch: loss=10.67, loss_att=8.794, loss_ctc=15.06
32epoch:train:5800-5900batch: loss=11.18, loss_att=9.225, loss_ctc=15.74
32epoch:train:5900-6000batch: loss=10.94, loss_att=9.0, loss_ctc=15.46
Validation average objf: -6.079567 over 5505.0 utts
32epoch:train:6000-6100batch: loss=10.89, loss_att=8.959, loss_ctc=15.4
32epoch:train:6100-6200batch: loss=10.03, loss_att=8.347, loss_ctc=13.97
32epoch:train:6200-6300batch: loss=10.2, loss_att=8.485, loss_ctc=14.2
32epoch:train:6300-6400batch: loss=10.65, loss_att=8.795, loss_ctc=14.98
32epoch:train:6400-6500batch: loss=9.975, loss_att=8.259, loss_ctc=13.98
32epoch:train:6500-6600batch: loss=11.25, loss_att=9.319, loss_ctc=15.75
32epoch:train:6600-6700batch: loss=9.816, loss_att=8.16, loss_ctc=13.68
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-32.pt: epoch=32, learning_rate=0.0009648141140037428, objf=-0.6758233387367253, valid_objf=-6.07956687102634
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-32-info
epoch 33, learning rate 0.0009496177562642609
33epoch:train:0-0batch: loss=10.56, loss_att=8.753, loss_ctc=14.77
33epoch:train:0-100batch: loss=10.03, loss_att=8.241, loss_ctc=14.19
33epoch:train:100-200batch: loss=10.09, loss_att=8.273, loss_ctc=14.32
33epoch:train:200-300batch: loss=9.937, loss_att=8.124, loss_ctc=14.17
33epoch:train:300-400batch: loss=9.911, loss_att=8.054, loss_ctc=14.24
33epoch:train:400-500batch: loss=9.805, loss_att=7.995, loss_ctc=14.03
33epoch:train:500-600batch: loss=9.674, loss_att=7.989, loss_ctc=13.61
33epoch:train:600-700batch: loss=10.04, loss_att=8.206, loss_ctc=14.33
33epoch:train:700-800batch: loss=10.17, loss_att=8.341, loss_ctc=14.44
33epoch:train:800-900batch: loss=10.37, loss_att=8.507, loss_ctc=14.73
33epoch:train:900-1000batch: loss=9.385, loss_att=7.647, loss_ctc=13.44
Validation average objf: -5.938334 over 5505.0 utts
33epoch:train:1000-1100batch: loss=9.815, loss_att=8.082, loss_ctc=13.86
33epoch:train:1100-1200batch: loss=9.548, loss_att=7.858, loss_ctc=13.49
33epoch:train:1200-1300batch: loss=9.811, loss_att=7.99, loss_ctc=14.06
33epoch:train:1300-1400batch: loss=10.33, loss_att=8.441, loss_ctc=14.73
33epoch:train:1400-1500batch: loss=10.23, loss_att=8.378, loss_ctc=14.56
33epoch:train:1500-1600batch: loss=10.23, loss_att=8.379, loss_ctc=14.54
33epoch:train:1600-1700batch: loss=10.09, loss_att=8.292, loss_ctc=14.29
33epoch:train:1700-1800batch: loss=9.741, loss_att=7.954, loss_ctc=13.91
33epoch:train:1800-1900batch: loss=10.35, loss_att=8.477, loss_ctc=14.7
33epoch:train:1900-2000batch: loss=10.23, loss_att=8.362, loss_ctc=14.6
Validation average objf: -6.003500 over 5505.0 utts
33epoch:train:2000-2100batch: loss=10.18, loss_att=8.415, loss_ctc=14.3
33epoch:train:2100-2200batch: loss=10.17, loss_att=8.379, loss_ctc=14.35
33epoch:train:2200-2300batch: loss=10.47, loss_att=8.667, loss_ctc=14.69
33epoch:train:2300-2400batch: loss=10.23, loss_att=8.401, loss_ctc=14.48
33epoch:train:2400-2500batch: loss=10.45, loss_att=8.582, loss_ctc=14.81
33epoch:train:2500-2600batch: loss=10.48, loss_att=8.597, loss_ctc=14.86
33epoch:train:2600-2700batch: loss=10.22, loss_att=8.503, loss_ctc=14.22
33epoch:train:2700-2800batch: loss=10.42, loss_att=8.547, loss_ctc=14.78
33epoch:train:2800-2900batch: loss=10.72, loss_att=8.763, loss_ctc=15.28
33epoch:train:2900-3000batch: loss=10.07, loss_att=8.28, loss_ctc=14.26
Validation average objf: -5.870126 over 5505.0 utts
33epoch:train:3000-3100batch: loss=10.23, loss_att=8.4, loss_ctc=14.5
33epoch:train:3100-3200batch: loss=10.36, loss_att=8.623, loss_ctc=14.41
33epoch:train:3200-3300batch: loss=10.15, loss_att=8.352, loss_ctc=14.35
33epoch:train:3300-3400batch: loss=10.58, loss_att=8.687, loss_ctc=15.0
33epoch:train:3400-3500batch: loss=10.33, loss_att=8.512, loss_ctc=14.59
33epoch:train:3500-3600batch: loss=10.04, loss_att=8.279, loss_ctc=14.16
33epoch:train:3600-3700batch: loss=10.45, loss_att=8.536, loss_ctc=14.93
33epoch:train:3700-3800batch: loss=10.6, loss_att=8.726, loss_ctc=14.99
33epoch:train:3800-3900batch: loss=10.7, loss_att=8.806, loss_ctc=15.13
33epoch:train:3900-4000batch: loss=10.76, loss_att=8.801, loss_ctc=15.32
Validation average objf: -6.199729 over 5505.0 utts
33epoch:train:4000-4100batch: loss=10.66, loss_att=8.819, loss_ctc=14.96
33epoch:train:4100-4200batch: loss=10.37, loss_att=8.47, loss_ctc=14.79
33epoch:train:4200-4300batch: loss=10.86, loss_att=8.968, loss_ctc=15.27
33epoch:train:4300-4400batch: loss=10.41, loss_att=8.528, loss_ctc=14.8
33epoch:train:4400-4500batch: loss=10.19, loss_att=8.473, loss_ctc=14.21
33epoch:train:4500-4600batch: loss=10.69, loss_att=8.853, loss_ctc=14.98
33epoch:train:4600-4700batch: loss=10.45, loss_att=8.544, loss_ctc=14.89
33epoch:train:4700-4800batch: loss=10.62, loss_att=8.753, loss_ctc=14.97
33epoch:train:4800-4900batch: loss=10.41, loss_att=8.575, loss_ctc=14.7
33epoch:train:4900-5000batch: loss=10.17, loss_att=8.381, loss_ctc=14.35
Validation average objf: -6.144581 over 5505.0 utts
33epoch:train:5000-5100batch: loss=10.62, loss_att=8.727, loss_ctc=15.05
33epoch:train:5100-5200batch: loss=10.72, loss_att=8.857, loss_ctc=15.08
33epoch:train:5200-5300batch: loss=10.87, loss_att=9.035, loss_ctc=15.14
33epoch:train:5300-5400batch: loss=10.18, loss_att=8.432, loss_ctc=14.25
33epoch:train:5400-5500batch: loss=10.16, loss_att=8.364, loss_ctc=14.35
33epoch:train:5500-5600batch: loss=10.3, loss_att=8.533, loss_ctc=14.42
33epoch:train:5600-5700batch: loss=10.51, loss_att=8.625, loss_ctc=14.92
33epoch:train:5700-5800batch: loss=11.18, loss_att=9.17, loss_ctc=15.85
33epoch:train:5800-5900batch: loss=10.7, loss_att=8.843, loss_ctc=15.03
33epoch:train:5900-6000batch: loss=10.0, loss_att=8.341, loss_ctc=13.87
Validation average objf: -6.127588 over 5505.0 utts
33epoch:train:6000-6100batch: loss=10.65, loss_att=8.832, loss_ctc=14.88
33epoch:train:6100-6200batch: loss=10.42, loss_att=8.629, loss_ctc=14.59
33epoch:train:6200-6300batch: loss=10.73, loss_att=8.896, loss_ctc=15.01
33epoch:train:6300-6400batch: loss=10.72, loss_att=8.811, loss_ctc=15.18
33epoch:train:6400-6500batch: loss=10.36, loss_att=8.571, loss_ctc=14.54
33epoch:train:6500-6600batch: loss=10.62, loss_att=8.783, loss_ctc=14.9
33epoch:train:6600-6700batch: loss=10.44, loss_att=8.606, loss_ctc=14.73
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-33.pt: epoch=33, learning_rate=0.0009496177562642609, objf=-0.6681259993543016, valid_objf=-6.127587846276112
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-33-info
epoch 34, learning rate 0.0009351196434197979
34epoch:train:0-0batch: loss=10.72, loss_att=9.096, loss_ctc=14.51
34epoch:train:0-100batch: loss=10.39, loss_att=8.509, loss_ctc=14.77
34epoch:train:100-200batch: loss=10.11, loss_att=8.296, loss_ctc=14.35
34epoch:train:200-300batch: loss=10.08, loss_att=8.208, loss_ctc=14.46
34epoch:train:300-400batch: loss=9.789, loss_att=8.002, loss_ctc=13.96
34epoch:train:400-500batch: loss=9.359, loss_att=7.679, loss_ctc=13.28
34epoch:train:500-600batch: loss=9.895, loss_att=8.123, loss_ctc=14.03
34epoch:train:600-700batch: loss=9.829, loss_att=8.077, loss_ctc=13.92
34epoch:train:700-800batch: loss=10.17, loss_att=8.32, loss_ctc=14.49
34epoch:train:800-900batch: loss=9.822, loss_att=7.998, loss_ctc=14.08
34epoch:train:900-1000batch: loss=9.475, loss_att=7.795, loss_ctc=13.39
Validation average objf: -6.063668 over 5505.0 utts
34epoch:train:1000-1100batch: loss=9.833, loss_att=8.029, loss_ctc=14.04
34epoch:train:1100-1200batch: loss=9.886, loss_att=8.081, loss_ctc=14.1
34epoch:train:1200-1300batch: loss=9.829, loss_att=8.017, loss_ctc=14.06
34epoch:train:1300-1400batch: loss=9.997, loss_att=8.221, loss_ctc=14.14
34epoch:train:1400-1500batch: loss=10.3, loss_att=8.37, loss_ctc=14.8
34epoch:train:1500-1600batch: loss=10.47, loss_att=8.612, loss_ctc=14.8
34epoch:train:1600-1700batch: loss=10.07, loss_att=8.255, loss_ctc=14.3
34epoch:train:1700-1800batch: loss=9.824, loss_att=8.062, loss_ctc=13.93
34epoch:train:1800-1900batch: loss=9.821, loss_att=8.036, loss_ctc=13.99
34epoch:train:1900-2000batch: loss=9.834, loss_att=7.994, loss_ctc=14.13
Validation average objf: -6.009309 over 5505.0 utts
34epoch:train:2000-2100batch: loss=9.927, loss_att=8.179, loss_ctc=14.01
34epoch:train:2100-2200batch: loss=9.653, loss_att=7.954, loss_ctc=13.62
34epoch:train:2200-2300batch: loss=10.22, loss_att=8.396, loss_ctc=14.49
34epoch:train:2300-2400batch: loss=10.32, loss_att=8.506, loss_ctc=14.55
34epoch:train:2400-2500batch: loss=10.25, loss_att=8.412, loss_ctc=14.53
34epoch:train:2500-2600batch: loss=10.02, loss_att=8.194, loss_ctc=14.28
34epoch:train:2600-2700batch: loss=10.35, loss_att=8.516, loss_ctc=14.63
34epoch:train:2700-2800batch: loss=10.01, loss_att=8.243, loss_ctc=14.12
34epoch:train:2800-2900batch: loss=10.57, loss_att=8.663, loss_ctc=15.02
34epoch:train:2900-3000batch: loss=10.15, loss_att=8.347, loss_ctc=14.37
Validation average objf: -5.842841 over 5505.0 utts
34epoch:train:3000-3100batch: loss=10.57, loss_att=8.641, loss_ctc=15.09
34epoch:train:3100-3200batch: loss=10.02, loss_att=8.297, loss_ctc=14.05
34epoch:train:3200-3300batch: loss=9.8, loss_att=8.15, loss_ctc=13.65
34epoch:train:3300-3400batch: loss=9.319, loss_att=7.712, loss_ctc=13.07
34epoch:train:3400-3500batch: loss=10.88, loss_att=8.958, loss_ctc=15.35
34epoch:train:3500-3600batch: loss=10.5, loss_att=8.651, loss_ctc=14.83
34epoch:train:3600-3700batch: loss=9.894, loss_att=8.14, loss_ctc=13.99
34epoch:train:3700-3800batch: loss=9.881, loss_att=8.141, loss_ctc=13.94
34epoch:train:3800-3900batch: loss=11.01, loss_att=9.149, loss_ctc=15.34
34epoch:train:3900-4000batch: loss=10.47, loss_att=8.461, loss_ctc=15.16
Validation average objf: -6.033125 over 5505.0 utts
34epoch:train:4000-4100batch: loss=10.9, loss_att=9.0, loss_ctc=15.35
34epoch:train:4100-4200batch: loss=10.33, loss_att=8.484, loss_ctc=14.64
34epoch:train:4200-4300batch: loss=10.27, loss_att=8.473, loss_ctc=14.45
34epoch:train:4300-4400batch: loss=10.27, loss_att=8.422, loss_ctc=14.58
34epoch:train:4400-4500batch: loss=10.94, loss_att=8.964, loss_ctc=15.55
34epoch:train:4500-4600batch: loss=10.9, loss_att=8.944, loss_ctc=15.45
34epoch:train:4600-4700batch: loss=10.84, loss_att=8.951, loss_ctc=15.24
34epoch:train:4700-4800batch: loss=10.81, loss_att=8.919, loss_ctc=15.21
34epoch:train:4800-4900batch: loss=10.19, loss_att=8.467, loss_ctc=14.21
34epoch:train:4900-5000batch: loss=10.92, loss_att=9.021, loss_ctc=15.34
Validation average objf: -6.101043 over 5505.0 utts
34epoch:train:5000-5100batch: loss=10.57, loss_att=8.796, loss_ctc=14.7
34epoch:train:5100-5200batch: loss=11.01, loss_att=9.126, loss_ctc=15.41
34epoch:train:5200-5300batch: loss=10.48, loss_att=8.641, loss_ctc=14.76
34epoch:train:5300-5400batch: loss=10.38, loss_att=8.617, loss_ctc=14.48
34epoch:train:5400-5500batch: loss=10.86, loss_att=8.983, loss_ctc=15.24
34epoch:train:5500-5600batch: loss=10.33, loss_att=8.562, loss_ctc=14.47
34epoch:train:5600-5700batch: loss=10.21, loss_att=8.543, loss_ctc=14.1
34epoch:train:5700-5800batch: loss=9.948, loss_att=8.292, loss_ctc=13.81
34epoch:train:5800-5900batch: loss=10.68, loss_att=8.818, loss_ctc=15.02
34epoch:train:5900-6000batch: loss=10.26, loss_att=8.51, loss_ctc=14.34
Validation average objf: -6.190845 over 5505.0 utts
34epoch:train:6000-6100batch: loss=10.42, loss_att=8.611, loss_ctc=14.63
34epoch:train:6100-6200batch: loss=10.36, loss_att=8.525, loss_ctc=14.65
34epoch:train:6200-6300batch: loss=10.27, loss_att=8.44, loss_ctc=14.53
34epoch:train:6300-6400batch: loss=10.68, loss_att=8.86, loss_ctc=14.93
34epoch:train:6400-6500batch: loss=10.13, loss_att=8.356, loss_ctc=14.28
34epoch:train:6500-6600batch: loss=9.99, loss_att=8.204, loss_ctc=14.16
34epoch:train:6600-6700batch: loss=9.514, loss_att=7.917, loss_ctc=13.24
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-34.pt: epoch=34, learning_rate=0.0009351196434197979, objf=-0.6634375124214344, valid_objf=-6.190844970481381
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-34-info
epoch 35, learning rate 0.0009212659203975832
35epoch:train:0-0batch: loss=10.99, loss_att=8.431, loss_ctc=16.95
35epoch:train:0-100batch: loss=10.24, loss_att=8.385, loss_ctc=14.57
35epoch:train:100-200batch: loss=9.734, loss_att=7.945, loss_ctc=13.91
35epoch:train:200-300batch: loss=10.03, loss_att=8.237, loss_ctc=14.2
35epoch:train:300-400batch: loss=9.493, loss_att=7.778, loss_ctc=13.49
35epoch:train:400-500batch: loss=9.795, loss_att=8.088, loss_ctc=13.78
35epoch:train:500-600batch: loss=9.696, loss_att=7.942, loss_ctc=13.79
35epoch:train:600-700batch: loss=9.615, loss_att=7.841, loss_ctc=13.75
35epoch:train:700-800batch: loss=9.873, loss_att=8.092, loss_ctc=14.03
35epoch:train:800-900batch: loss=9.42, loss_att=7.708, loss_ctc=13.41
35epoch:train:900-1000batch: loss=9.092, loss_att=7.43, loss_ctc=12.97
Validation average objf: -6.031755 over 5505.0 utts
35epoch:train:1000-1100batch: loss=9.884, loss_att=8.11, loss_ctc=14.02
35epoch:train:1100-1200batch: loss=9.494, loss_att=7.803, loss_ctc=13.44
35epoch:train:1200-1300batch: loss=9.57, loss_att=7.773, loss_ctc=13.76
35epoch:train:1300-1400batch: loss=9.435, loss_att=7.687, loss_ctc=13.51
35epoch:train:1400-1500batch: loss=9.63, loss_att=7.915, loss_ctc=13.63
35epoch:train:1500-1600batch: loss=10.44, loss_att=8.591, loss_ctc=14.75
35epoch:train:1600-1700batch: loss=10.65, loss_att=8.77, loss_ctc=15.05
35epoch:train:1700-1800batch: loss=9.89, loss_att=8.091, loss_ctc=14.09
35epoch:train:1800-1900batch: loss=9.587, loss_att=7.843, loss_ctc=13.66
35epoch:train:1900-2000batch: loss=10.12, loss_att=8.239, loss_ctc=14.51
Validation average objf: -6.025436 over 5505.0 utts
35epoch:train:2000-2100batch: loss=9.723, loss_att=8.007, loss_ctc=13.73
35epoch:train:2100-2200batch: loss=10.21, loss_att=8.293, loss_ctc=14.67
35epoch:train:2200-2300batch: loss=10.3, loss_att=8.454, loss_ctc=14.62
35epoch:train:2300-2400batch: loss=10.08, loss_att=8.278, loss_ctc=14.28
35epoch:train:2400-2500batch: loss=9.798, loss_att=8.038, loss_ctc=13.9
35epoch:train:2500-2600batch: loss=10.29, loss_att=8.437, loss_ctc=14.62
35epoch:train:2600-2700batch: loss=9.891, loss_att=8.045, loss_ctc=14.2
35epoch:train:2700-2800batch: loss=9.8, loss_att=8.075, loss_ctc=13.83
35epoch:train:2800-2900batch: loss=10.54, loss_att=8.61, loss_ctc=15.05
35epoch:train:2900-3000batch: loss=10.43, loss_att=8.656, loss_ctc=14.58
Validation average objf: -6.137448 over 5505.0 utts
35epoch:train:3000-3100batch: loss=9.962, loss_att=8.228, loss_ctc=14.01
35epoch:train:3100-3200batch: loss=10.28, loss_att=8.501, loss_ctc=14.43
35epoch:train:3200-3300batch: loss=10.35, loss_att=8.546, loss_ctc=14.55
35epoch:train:3300-3400batch: loss=10.11, loss_att=8.324, loss_ctc=14.27
35epoch:train:3400-3500batch: loss=10.21, loss_att=8.475, loss_ctc=14.25
35epoch:train:3500-3600batch: loss=10.74, loss_att=8.813, loss_ctc=15.24
35epoch:train:3600-3700batch: loss=10.42, loss_att=8.552, loss_ctc=14.77
35epoch:train:3700-3800batch: loss=10.3, loss_att=8.476, loss_ctc=14.54
35epoch:train:3800-3900batch: loss=10.62, loss_att=8.805, loss_ctc=14.85
35epoch:train:3900-4000batch: loss=10.37, loss_att=8.544, loss_ctc=14.63
Validation average objf: -5.976336 over 5505.0 utts
35epoch:train:4000-4100batch: loss=10.15, loss_att=8.343, loss_ctc=14.35
35epoch:train:4100-4200batch: loss=10.31, loss_att=8.527, loss_ctc=14.47
35epoch:train:4200-4300batch: loss=9.783, loss_att=8.112, loss_ctc=13.68
35epoch:train:4300-4400batch: loss=10.23, loss_att=8.439, loss_ctc=14.41
35epoch:train:4400-4500batch: loss=9.931, loss_att=8.194, loss_ctc=13.98
35epoch:train:4500-4600batch: loss=10.79, loss_att=8.821, loss_ctc=15.38
35epoch:train:4600-4700batch: loss=10.07, loss_att=8.312, loss_ctc=14.18
35epoch:train:4700-4800batch: loss=10.68, loss_att=8.797, loss_ctc=15.09
35epoch:train:4800-4900batch: loss=10.07, loss_att=8.349, loss_ctc=14.09
35epoch:train:4900-5000batch: loss=10.2, loss_att=8.476, loss_ctc=14.23
Validation average objf: -6.004361 over 5505.0 utts
35epoch:train:5000-5100batch: loss=10.1, loss_att=8.347, loss_ctc=14.19
35epoch:train:5100-5200batch: loss=10.37, loss_att=8.523, loss_ctc=14.67
35epoch:train:5200-5300batch: loss=9.963, loss_att=8.248, loss_ctc=13.97
35epoch:train:5300-5400batch: loss=10.77, loss_att=8.935, loss_ctc=15.06
35epoch:train:5400-5500batch: loss=10.09, loss_att=8.305, loss_ctc=14.24
35epoch:train:5500-5600batch: loss=10.45, loss_att=8.696, loss_ctc=14.54
35epoch:train:5600-5700batch: loss=10.4, loss_att=8.592, loss_ctc=14.63
35epoch:train:5700-5800batch: loss=9.996, loss_att=8.288, loss_ctc=13.98
35epoch:train:5800-5900batch: loss=9.448, loss_att=7.801, loss_ctc=13.29
35epoch:train:5900-6000batch: loss=10.8, loss_att=8.883, loss_ctc=15.26
Validation average objf: -6.141098 over 5505.0 utts
35epoch:train:6000-6100batch: loss=9.707, loss_att=8.06, loss_ctc=13.55
35epoch:train:6100-6200batch: loss=10.55, loss_att=8.742, loss_ctc=14.78
35epoch:train:6200-6300batch: loss=10.76, loss_att=8.921, loss_ctc=15.04
35epoch:train:6300-6400batch: loss=10.58, loss_att=8.722, loss_ctc=14.92
35epoch:train:6400-6500batch: loss=10.14, loss_att=8.352, loss_ctc=14.32
35epoch:train:6500-6600batch: loss=10.48, loss_att=8.71, loss_ctc=14.62
35epoch:train:6600-6700batch: loss=9.858, loss_att=8.165, loss_ctc=13.81
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-35.pt: epoch=35, learning_rate=0.0009212659203975832, objf=-0.6550814741745419, valid_objf=-6.141097581743869
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-35-info
epoch 36, learning rate 0.0009080102245746393
36epoch:train:0-0batch: loss=7.913, loss_att=6.388, loss_ctc=11.47
36epoch:train:0-100batch: loss=10.18, loss_att=8.314, loss_ctc=14.54
36epoch:train:100-200batch: loss=9.87, loss_att=8.061, loss_ctc=14.09
36epoch:train:200-300batch: loss=9.551, loss_att=7.818, loss_ctc=13.59
36epoch:train:300-400batch: loss=10.34, loss_att=8.444, loss_ctc=14.75
36epoch:train:400-500batch: loss=9.828, loss_att=8.094, loss_ctc=13.87
36epoch:train:500-600batch: loss=9.749, loss_att=8.002, loss_ctc=13.82
36epoch:train:600-700batch: loss=8.78, loss_att=7.196, loss_ctc=12.48
36epoch:train:700-800batch: loss=9.08, loss_att=7.447, loss_ctc=12.89
36epoch:train:800-900batch: loss=9.534, loss_att=7.805, loss_ctc=13.57
36epoch:train:900-1000batch: loss=9.753, loss_att=7.951, loss_ctc=13.96
Validation average objf: -6.030072 over 5505.0 utts
36epoch:train:1000-1100batch: loss=8.919, loss_att=7.261, loss_ctc=12.79
36epoch:train:1100-1200batch: loss=9.789, loss_att=7.956, loss_ctc=14.07
36epoch:train:1200-1300batch: loss=9.924, loss_att=8.152, loss_ctc=14.06
36epoch:train:1300-1400batch: loss=10.21, loss_att=8.325, loss_ctc=14.59
36epoch:train:1400-1500batch: loss=10.07, loss_att=8.304, loss_ctc=14.21
36epoch:train:1500-1600batch: loss=9.43, loss_att=7.737, loss_ctc=13.38
36epoch:train:1600-1700batch: loss=9.564, loss_att=7.855, loss_ctc=13.55
36epoch:train:1700-1800batch: loss=9.541, loss_att=7.894, loss_ctc=13.38
36epoch:train:1800-1900batch: loss=9.71, loss_att=8.041, loss_ctc=13.6
36epoch:train:1900-2000batch: loss=9.591, loss_att=7.885, loss_ctc=13.57
Validation average objf: -6.128393 over 5505.0 utts
36epoch:train:2000-2100batch: loss=10.12, loss_att=8.282, loss_ctc=14.41
36epoch:train:2100-2200batch: loss=10.38, loss_att=8.405, loss_ctc=14.98
36epoch:train:2200-2300batch: loss=10.41, loss_att=8.546, loss_ctc=14.75
36epoch:train:2300-2400batch: loss=9.786, loss_att=8.072, loss_ctc=13.79
36epoch:train:2400-2500batch: loss=10.31, loss_att=8.423, loss_ctc=14.73
36epoch:train:2500-2600batch: loss=10.16, loss_att=8.363, loss_ctc=14.36
36epoch:train:2600-2700batch: loss=9.706, loss_att=7.969, loss_ctc=13.76
36epoch:train:2700-2800batch: loss=10.3, loss_att=8.497, loss_ctc=14.49
36epoch:train:2800-2900batch: loss=10.0, loss_att=8.235, loss_ctc=14.13
36epoch:train:2900-3000batch: loss=10.38, loss_att=8.503, loss_ctc=14.75
Validation average objf: -5.976405 over 5505.0 utts
36epoch:train:3000-3100batch: loss=10.32, loss_att=8.428, loss_ctc=14.75
36epoch:train:3100-3200batch: loss=9.899, loss_att=8.267, loss_ctc=13.71
36epoch:train:3200-3300batch: loss=9.943, loss_att=8.201, loss_ctc=14.01
36epoch:train:3300-3400batch: loss=9.866, loss_att=8.139, loss_ctc=13.89
36epoch:train:3400-3500batch: loss=10.78, loss_att=8.874, loss_ctc=15.21
36epoch:train:3500-3600batch: loss=10.06, loss_att=8.313, loss_ctc=14.14
36epoch:train:3600-3700batch: loss=9.902, loss_att=8.15, loss_ctc=13.99
36epoch:train:3700-3800batch: loss=9.72, loss_att=8.015, loss_ctc=13.7
36epoch:train:3800-3900batch: loss=10.35, loss_att=8.525, loss_ctc=14.6
36epoch:train:3900-4000batch: loss=10.04, loss_att=8.228, loss_ctc=14.28
Validation average objf: -6.036725 over 5505.0 utts
36epoch:train:4000-4100batch: loss=10.31, loss_att=8.493, loss_ctc=14.55
36epoch:train:4100-4200batch: loss=10.4, loss_att=8.566, loss_ctc=14.68
36epoch:train:4200-4300batch: loss=9.862, loss_att=8.09, loss_ctc=14.0
36epoch:train:4300-4400batch: loss=10.25, loss_att=8.41, loss_ctc=14.53
36epoch:train:4400-4500batch: loss=10.2, loss_att=8.43, loss_ctc=14.33
36epoch:train:4500-4600batch: loss=9.691, loss_att=8.02, loss_ctc=13.59
36epoch:train:4600-4700batch: loss=10.52, loss_att=8.726, loss_ctc=14.7
36epoch:train:4700-4800batch: loss=10.12, loss_att=8.394, loss_ctc=14.14
36epoch:train:4800-4900batch: loss=10.7, loss_att=8.866, loss_ctc=14.97
36epoch:train:4900-5000batch: loss=10.23, loss_att=8.42, loss_ctc=14.45
Validation average objf: -6.091025 over 5505.0 utts
36epoch:train:5000-5100batch: loss=10.54, loss_att=8.696, loss_ctc=14.83
36epoch:train:5100-5200batch: loss=10.14, loss_att=8.422, loss_ctc=14.15
36epoch:train:5200-5300batch: loss=10.7, loss_att=8.813, loss_ctc=15.1
36epoch:train:5300-5400batch: loss=9.718, loss_att=8.048, loss_ctc=13.62
36epoch:train:5400-5500batch: loss=10.68, loss_att=8.843, loss_ctc=14.95
36epoch:train:5500-5600batch: loss=9.794, loss_att=8.143, loss_ctc=13.65
36epoch:train:5600-5700batch: loss=10.19, loss_att=8.408, loss_ctc=14.36
36epoch:train:5700-5800batch: loss=10.74, loss_att=8.847, loss_ctc=15.15
36epoch:train:5800-5900batch: loss=9.959, loss_att=8.261, loss_ctc=13.92
36epoch:train:5900-6000batch: loss=10.26, loss_att=8.532, loss_ctc=14.29
Validation average objf: -6.077288 over 5505.0 utts
36epoch:train:6000-6100batch: loss=10.07, loss_att=8.338, loss_ctc=14.12
36epoch:train:6100-6200batch: loss=10.57, loss_att=8.637, loss_ctc=15.09
36epoch:train:6200-6300batch: loss=10.39, loss_att=8.651, loss_ctc=14.43
36epoch:train:6300-6400batch: loss=9.864, loss_att=8.104, loss_ctc=13.97
36epoch:train:6400-6500batch: loss=10.25, loss_att=8.537, loss_ctc=14.23
36epoch:train:6500-6600batch: loss=10.55, loss_att=8.71, loss_ctc=14.83
36epoch:train:6600-6700batch: loss=9.46, loss_att=7.89, loss_ctc=13.12
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-36.pt: epoch=36, learning_rate=0.0009080102245746393, objf=-0.6499529597795228, valid_objf=-6.077287693006358
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-36-info
epoch 37, learning rate 0.0008953107328206326
37epoch:train:0-0batch: loss=10.62, loss_att=8.765, loss_ctc=14.94
37epoch:train:0-100batch: loss=10.0, loss_att=8.17, loss_ctc=14.28
37epoch:train:100-200batch: loss=9.895, loss_att=8.062, loss_ctc=14.17
37epoch:train:200-300batch: loss=9.828, loss_att=8.031, loss_ctc=14.02
37epoch:train:300-400batch: loss=9.279, loss_att=7.591, loss_ctc=13.22
37epoch:train:400-500batch: loss=9.327, loss_att=7.615, loss_ctc=13.32
37epoch:train:500-600batch: loss=9.843, loss_att=8.011, loss_ctc=14.12
37epoch:train:600-700batch: loss=9.969, loss_att=8.117, loss_ctc=14.29
37epoch:train:700-800batch: loss=9.961, loss_att=8.165, loss_ctc=14.15
37epoch:train:800-900batch: loss=9.626, loss_att=7.929, loss_ctc=13.59
37epoch:train:900-1000batch: loss=9.8, loss_att=8.031, loss_ctc=13.93
Validation average objf: -5.950793 over 5505.0 utts
37epoch:train:1000-1100batch: loss=9.933, loss_att=8.132, loss_ctc=14.13
37epoch:train:1100-1200batch: loss=9.766, loss_att=7.996, loss_ctc=13.89
37epoch:train:1200-1300batch: loss=9.363, loss_att=7.651, loss_ctc=13.36
37epoch:train:1300-1400batch: loss=8.861, loss_att=7.182, loss_ctc=12.78
37epoch:train:1400-1500batch: loss=9.837, loss_att=8.003, loss_ctc=14.12
37epoch:train:1500-1600batch: loss=9.369, loss_att=7.671, loss_ctc=13.33
37epoch:train:1600-1700batch: loss=9.262, loss_att=7.495, loss_ctc=13.38
37epoch:train:1700-1800batch: loss=10.15, loss_att=8.276, loss_ctc=14.53
37epoch:train:1800-1900batch: loss=9.711, loss_att=7.953, loss_ctc=13.81
37epoch:train:1900-2000batch: loss=9.718, loss_att=7.973, loss_ctc=13.79
Validation average objf: -6.061338 over 5505.0 utts
37epoch:train:2000-2100batch: loss=9.738, loss_att=8.036, loss_ctc=13.71
37epoch:train:2100-2200batch: loss=9.53, loss_att=7.902, loss_ctc=13.33
37epoch:train:2200-2300batch: loss=9.508, loss_att=7.81, loss_ctc=13.47
37epoch:train:2300-2400batch: loss=10.11, loss_att=8.332, loss_ctc=14.27
37epoch:train:2400-2500batch: loss=10.05, loss_att=8.193, loss_ctc=14.37
37epoch:train:2500-2600batch: loss=9.86, loss_att=8.085, loss_ctc=14.0
37epoch:train:2600-2700batch: loss=10.7, loss_att=8.788, loss_ctc=15.17
37epoch:train:2700-2800batch: loss=9.676, loss_att=8.0, loss_ctc=13.59
37epoch:train:2800-2900batch: loss=10.01, loss_att=8.199, loss_ctc=14.22
37epoch:train:2900-3000batch: loss=10.05, loss_att=8.285, loss_ctc=14.16
Validation average objf: -6.010894 over 5505.0 utts
37epoch:train:3000-3100batch: loss=10.09, loss_att=8.31, loss_ctc=14.23
37epoch:train:3100-3200batch: loss=10.16, loss_att=8.385, loss_ctc=14.3
37epoch:train:3200-3300batch: loss=9.797, loss_att=8.114, loss_ctc=13.72
37epoch:train:3300-3400batch: loss=9.776, loss_att=8.149, loss_ctc=13.57
37epoch:train:3400-3500batch: loss=10.92, loss_att=9.025, loss_ctc=15.34
37epoch:train:3500-3600batch: loss=9.802, loss_att=8.108, loss_ctc=13.75
37epoch:train:3600-3700batch: loss=10.64, loss_att=8.709, loss_ctc=15.15
37epoch:train:3700-3800batch: loss=10.14, loss_att=8.435, loss_ctc=14.12
37epoch:train:3800-3900batch: loss=9.951, loss_att=8.261, loss_ctc=13.9
37epoch:train:3900-4000batch: loss=9.791, loss_att=8.071, loss_ctc=13.8
Validation average objf: -6.089931 over 5505.0 utts
37epoch:train:4000-4100batch: loss=9.913, loss_att=8.181, loss_ctc=13.95
37epoch:train:4100-4200batch: loss=10.25, loss_att=8.401, loss_ctc=14.55
37epoch:train:4200-4300batch: loss=9.837, loss_att=8.151, loss_ctc=13.77
37epoch:train:4300-4400batch: loss=10.42, loss_att=8.603, loss_ctc=14.65
37epoch:train:4400-4500batch: loss=9.773, loss_att=8.053, loss_ctc=13.79
37epoch:train:4500-4600batch: loss=9.858, loss_att=8.188, loss_ctc=13.75
37epoch:train:4600-4700batch: loss=10.52, loss_att=8.606, loss_ctc=15.0
37epoch:train:4700-4800batch: loss=10.37, loss_att=8.594, loss_ctc=14.53
37epoch:train:4800-4900batch: loss=9.87, loss_att=8.15, loss_ctc=13.88
37epoch:train:4900-5000batch: loss=10.32, loss_att=8.509, loss_ctc=14.54
Validation average objf: -6.075582 over 5505.0 utts
37epoch:train:5000-5100batch: loss=9.829, loss_att=8.129, loss_ctc=13.8
37epoch:train:5100-5200batch: loss=9.999, loss_att=8.24, loss_ctc=14.1
37epoch:train:5200-5300batch: loss=9.98, loss_att=8.198, loss_ctc=14.14
37epoch:train:5300-5400batch: loss=10.02, loss_att=8.306, loss_ctc=14.01
37epoch:train:5400-5500batch: loss=10.38, loss_att=8.54, loss_ctc=14.68
37epoch:train:5500-5600batch: loss=10.31, loss_att=8.538, loss_ctc=14.43
37epoch:train:5600-5700batch: loss=10.36, loss_att=8.529, loss_ctc=14.65
37epoch:train:5700-5800batch: loss=10.63, loss_att=8.795, loss_ctc=14.91
37epoch:train:5800-5900batch: loss=10.35, loss_att=8.562, loss_ctc=14.53
37epoch:train:5900-6000batch: loss=9.957, loss_att=8.208, loss_ctc=14.04
Validation average objf: -6.074124 over 5505.0 utts
37epoch:train:6000-6100batch: loss=10.4, loss_att=8.582, loss_ctc=14.65
37epoch:train:6100-6200batch: loss=10.07, loss_att=8.335, loss_ctc=14.11
37epoch:train:6200-6300batch: loss=10.09, loss_att=8.33, loss_ctc=14.18
37epoch:train:6300-6400batch: loss=10.25, loss_att=8.491, loss_ctc=14.35
37epoch:train:6400-6500batch: loss=9.845, loss_att=8.148, loss_ctc=13.81
37epoch:train:6500-6600batch: loss=9.843, loss_att=8.099, loss_ctc=13.91
37epoch:train:6600-6700batch: loss=9.407, loss_att=7.819, loss_ctc=13.11
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-37.pt: epoch=37, learning_rate=0.0008953107328206326, objf=-0.6452974808602863, valid_objf=-6.074124375567666
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-37-info
epoch 38, learning rate 0.0008831313688184429
38epoch:train:0-0batch: loss=20.27, loss_att=16.4, loss_ctc=29.29
38epoch:train:0-100batch: loss=9.199, loss_att=7.559, loss_ctc=13.03
38epoch:train:100-200batch: loss=9.623, loss_att=7.91, loss_ctc=13.62
38epoch:train:200-300batch: loss=9.383, loss_att=7.599, loss_ctc=13.55
38epoch:train:300-400batch: loss=9.374, loss_att=7.726, loss_ctc=13.22
38epoch:train:400-500batch: loss=9.251, loss_att=7.499, loss_ctc=13.34
38epoch:train:500-600batch: loss=9.352, loss_att=7.637, loss_ctc=13.35
38epoch:train:600-700batch: loss=9.093, loss_att=7.386, loss_ctc=13.08
38epoch:train:700-800batch: loss=9.848, loss_att=8.036, loss_ctc=14.08
38epoch:train:800-900batch: loss=9.645, loss_att=7.842, loss_ctc=13.85
38epoch:train:900-1000batch: loss=9.784, loss_att=8.065, loss_ctc=13.79
Validation average objf: -5.954672 over 5505.0 utts
38epoch:train:1000-1100batch: loss=9.515, loss_att=7.745, loss_ctc=13.65
38epoch:train:1100-1200batch: loss=9.61, loss_att=7.854, loss_ctc=13.71
38epoch:train:1200-1300batch: loss=9.482, loss_att=7.71, loss_ctc=13.62
38epoch:train:1300-1400batch: loss=9.479, loss_att=7.745, loss_ctc=13.52
38epoch:train:1400-1500batch: loss=9.752, loss_att=8.021, loss_ctc=13.79
38epoch:train:1500-1600batch: loss=9.789, loss_att=7.956, loss_ctc=14.07
38epoch:train:1600-1700batch: loss=9.818, loss_att=8.03, loss_ctc=13.99
38epoch:train:1700-1800batch: loss=9.977, loss_att=8.137, loss_ctc=14.27
38epoch:train:1800-1900batch: loss=9.554, loss_att=7.788, loss_ctc=13.68
38epoch:train:1900-2000batch: loss=9.666, loss_att=7.927, loss_ctc=13.72
Validation average objf: -6.101214 over 5505.0 utts
38epoch:train:2000-2100batch: loss=9.797, loss_att=8.015, loss_ctc=13.95
38epoch:train:2100-2200batch: loss=9.319, loss_att=7.684, loss_ctc=13.13
38epoch:train:2200-2300batch: loss=9.477, loss_att=7.826, loss_ctc=13.33
38epoch:train:2300-2400batch: loss=10.06, loss_att=8.217, loss_ctc=14.34
38epoch:train:2400-2500batch: loss=9.569, loss_att=7.907, loss_ctc=13.45
38epoch:train:2500-2600batch: loss=10.29, loss_att=8.43, loss_ctc=14.62
38epoch:train:2600-2700batch: loss=10.19, loss_att=8.396, loss_ctc=14.38
38epoch:train:2700-2800batch: loss=9.623, loss_att=7.889, loss_ctc=13.67
38epoch:train:2800-2900batch: loss=9.368, loss_att=7.69, loss_ctc=13.28
38epoch:train:2900-3000batch: loss=9.804, loss_att=8.076, loss_ctc=13.84
Validation average objf: -6.019848 over 5505.0 utts
38epoch:train:3000-3100batch: loss=9.359, loss_att=7.691, loss_ctc=13.25
38epoch:train:3100-3200batch: loss=9.785, loss_att=8.046, loss_ctc=13.84
38epoch:train:3200-3300batch: loss=9.801, loss_att=8.084, loss_ctc=13.81
38epoch:train:3300-3400batch: loss=10.24, loss_att=8.42, loss_ctc=14.48
38epoch:train:3400-3500batch: loss=9.866, loss_att=8.099, loss_ctc=13.99
38epoch:train:3500-3600batch: loss=10.75, loss_att=8.896, loss_ctc=15.08
38epoch:train:3600-3700batch: loss=9.577, loss_att=7.891, loss_ctc=13.51
38epoch:train:3700-3800batch: loss=9.985, loss_att=8.337, loss_ctc=13.83
38epoch:train:3800-3900batch: loss=10.38, loss_att=8.526, loss_ctc=14.7
38epoch:train:3900-4000batch: loss=10.19, loss_att=8.404, loss_ctc=14.36
Validation average objf: -6.079474 over 5505.0 utts
38epoch:train:4000-4100batch: loss=9.767, loss_att=8.094, loss_ctc=13.67
38epoch:train:4100-4200batch: loss=9.737, loss_att=8.126, loss_ctc=13.5
38epoch:train:4200-4300batch: loss=10.03, loss_att=8.22, loss_ctc=14.25
38epoch:train:4300-4400batch: loss=10.2, loss_att=8.415, loss_ctc=14.36
38epoch:train:4400-4500batch: loss=10.76, loss_att=8.895, loss_ctc=15.1
38epoch:train:4500-4600batch: loss=10.02, loss_att=8.337, loss_ctc=13.95
38epoch:train:4600-4700batch: loss=9.741, loss_att=8.071, loss_ctc=13.64
38epoch:train:4700-4800batch: loss=10.21, loss_att=8.451, loss_ctc=14.31
38epoch:train:4800-4900batch: loss=9.797, loss_att=8.115, loss_ctc=13.72
38epoch:train:4900-5000batch: loss=10.35, loss_att=8.472, loss_ctc=14.73
Validation average objf: -6.157788 over 5505.0 utts
38epoch:train:5000-5100batch: loss=10.6, loss_att=8.789, loss_ctc=14.82
38epoch:train:5100-5200batch: loss=9.74, loss_att=8.047, loss_ctc=13.69
38epoch:train:5200-5300batch: loss=10.54, loss_att=8.672, loss_ctc=14.91
38epoch:train:5300-5400batch: loss=10.04, loss_att=8.275, loss_ctc=14.14
38epoch:train:5400-5500batch: loss=10.11, loss_att=8.387, loss_ctc=14.12
38epoch:train:5500-5600batch: loss=9.85, loss_att=8.092, loss_ctc=13.95
38epoch:train:5600-5700batch: loss=9.735, loss_att=8.041, loss_ctc=13.69
38epoch:train:5700-5800batch: loss=10.52, loss_att=8.717, loss_ctc=14.72
38epoch:train:5800-5900batch: loss=10.34, loss_att=8.539, loss_ctc=14.53
38epoch:train:5900-6000batch: loss=10.13, loss_att=8.346, loss_ctc=14.29
Validation average objf: -6.177806 over 5505.0 utts
38epoch:train:6000-6100batch: loss=10.5, loss_att=8.7, loss_ctc=14.7
38epoch:train:6100-6200batch: loss=10.14, loss_att=8.324, loss_ctc=14.37
38epoch:train:6200-6300batch: loss=9.873, loss_att=8.12, loss_ctc=13.96
38epoch:train:6300-6400batch: loss=9.321, loss_att=7.681, loss_ctc=13.15
38epoch:train:6400-6500batch: loss=10.63, loss_att=8.78, loss_ctc=14.95
38epoch:train:6500-6600batch: loss=9.624, loss_att=7.983, loss_ctc=13.45
38epoch:train:6600-6700batch: loss=9.449, loss_att=7.866, loss_ctc=13.14
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-38.pt: epoch=38, learning_rate=0.0008831313688184429, objf=-0.6390126525623127, valid_objf=-6.1778063975930975
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-38-info
epoch 39, learning rate 0.0008714325118774557
39epoch:train:0-0batch: loss=8.471, loss_att=6.983, loss_ctc=11.94
39epoch:train:0-100batch: loss=9.232, loss_att=7.608, loss_ctc=13.02
39epoch:train:100-200batch: loss=9.135, loss_att=7.486, loss_ctc=12.98
39epoch:train:200-300batch: loss=9.26, loss_att=7.586, loss_ctc=13.17
39epoch:train:300-400batch: loss=9.273, loss_att=7.6, loss_ctc=13.18
39epoch:train:400-500batch: loss=9.426, loss_att=7.72, loss_ctc=13.41
39epoch:train:500-600batch: loss=9.25, loss_att=7.515, loss_ctc=13.3
39epoch:train:600-700batch: loss=8.877, loss_att=7.212, loss_ctc=12.76
39epoch:train:700-800batch: loss=9.713, loss_att=7.937, loss_ctc=13.86
39epoch:train:800-900batch: loss=9.378, loss_att=7.686, loss_ctc=13.33
39epoch:train:900-1000batch: loss=9.288, loss_att=7.544, loss_ctc=13.36
Validation average objf: -6.132769 over 5505.0 utts
39epoch:train:1000-1100batch: loss=9.824, loss_att=8.017, loss_ctc=14.04
39epoch:train:1100-1200batch: loss=9.624, loss_att=7.83, loss_ctc=13.81
39epoch:train:1200-1300batch: loss=9.465, loss_att=7.68, loss_ctc=13.63
39epoch:train:1300-1400batch: loss=9.839, loss_att=8.11, loss_ctc=13.87
39epoch:train:1400-1500batch: loss=9.359, loss_att=7.702, loss_ctc=13.23
39epoch:train:1500-1600batch: loss=9.655, loss_att=7.908, loss_ctc=13.73
39epoch:train:1600-1700batch: loss=9.715, loss_att=7.924, loss_ctc=13.89
39epoch:train:1700-1800batch: loss=9.413, loss_att=7.749, loss_ctc=13.29
39epoch:train:1800-1900batch: loss=9.766, loss_att=8.062, loss_ctc=13.74
39epoch:train:1900-2000batch: loss=9.771, loss_att=8.016, loss_ctc=13.87
Validation average objf: -6.090972 over 5505.0 utts
39epoch:train:2000-2100batch: loss=9.77, loss_att=8.056, loss_ctc=13.77
39epoch:train:2100-2200batch: loss=10.21, loss_att=8.316, loss_ctc=14.63
39epoch:train:2200-2300batch: loss=9.883, loss_att=8.116, loss_ctc=14.01
39epoch:train:2300-2400batch: loss=9.721, loss_att=7.953, loss_ctc=13.85
39epoch:train:2400-2500batch: loss=9.723, loss_att=7.974, loss_ctc=13.81
39epoch:train:2500-2600batch: loss=9.698, loss_att=7.951, loss_ctc=13.78
39epoch:train:2600-2700batch: loss=10.12, loss_att=8.347, loss_ctc=14.27
39epoch:train:2700-2800batch: loss=9.938, loss_att=8.259, loss_ctc=13.86
39epoch:train:2800-2900batch: loss=9.971, loss_att=8.194, loss_ctc=14.12
39epoch:train:2900-3000batch: loss=10.19, loss_att=8.417, loss_ctc=14.32
Validation average objf: -6.133902 over 5505.0 utts
39epoch:train:3000-3100batch: loss=9.064, loss_att=7.548, loss_ctc=12.6
39epoch:train:3100-3200batch: loss=10.07, loss_att=8.304, loss_ctc=14.19
39epoch:train:3200-3300batch: loss=9.644, loss_att=7.942, loss_ctc=13.61
39epoch:train:3300-3400batch: loss=9.589, loss_att=7.854, loss_ctc=13.64
39epoch:train:3400-3500batch: loss=9.733, loss_att=8.02, loss_ctc=13.73
39epoch:train:3500-3600batch: loss=10.07, loss_att=8.287, loss_ctc=14.22
39epoch:train:3600-3700batch: loss=9.793, loss_att=8.074, loss_ctc=13.81
39epoch:train:3700-3800batch: loss=10.08, loss_att=8.333, loss_ctc=14.15
39epoch:train:3800-3900batch: loss=9.741, loss_att=8.011, loss_ctc=13.78
39epoch:train:3900-4000batch: loss=9.561, loss_att=7.878, loss_ctc=13.49
Validation average objf: -6.139766 over 5505.0 utts
39epoch:train:4000-4100batch: loss=10.34, loss_att=8.538, loss_ctc=14.55
39epoch:train:4100-4200batch: loss=10.11, loss_att=8.309, loss_ctc=14.32
39epoch:train:4200-4300batch: loss=9.723, loss_att=7.966, loss_ctc=13.82
39epoch:train:4300-4400batch: loss=10.14, loss_att=8.348, loss_ctc=14.33
39epoch:train:4400-4500batch: loss=10.74, loss_att=8.848, loss_ctc=15.15
39epoch:train:4500-4600batch: loss=10.4, loss_att=8.587, loss_ctc=14.64
39epoch:train:4600-4700batch: loss=10.28, loss_att=8.467, loss_ctc=14.52
39epoch:train:4700-4800batch: loss=10.29, loss_att=8.516, loss_ctc=14.44
39epoch:train:4800-4900batch: loss=10.13, loss_att=8.357, loss_ctc=14.27
39epoch:train:4900-5000batch: loss=9.888, loss_att=8.139, loss_ctc=13.97
Validation average objf: -6.215293 over 5505.0 utts
39epoch:train:5000-5100batch: loss=9.71, loss_att=8.066, loss_ctc=13.55
39epoch:train:5100-5200batch: loss=10.29, loss_att=8.517, loss_ctc=14.42
39epoch:train:5200-5300batch: loss=9.836, loss_att=8.091, loss_ctc=13.91
39epoch:train:5300-5400batch: loss=10.5, loss_att=8.698, loss_ctc=14.72
39epoch:train:5400-5500batch: loss=9.687, loss_att=8.013, loss_ctc=13.59
39epoch:train:5500-5600batch: loss=10.1, loss_att=8.394, loss_ctc=14.08
39epoch:train:5600-5700batch: loss=9.418, loss_att=7.795, loss_ctc=13.21
39epoch:train:5700-5800batch: loss=9.655, loss_att=8.046, loss_ctc=13.41
39epoch:train:5800-5900batch: loss=10.28, loss_att=8.465, loss_ctc=14.5
39epoch:train:5900-6000batch: loss=10.15, loss_att=8.388, loss_ctc=14.27
Validation average objf: -6.059405 over 5505.0 utts
39epoch:train:6000-6100batch: loss=10.44, loss_att=8.625, loss_ctc=14.66
39epoch:train:6100-6200batch: loss=10.05, loss_att=8.293, loss_ctc=14.14
39epoch:train:6200-6300batch: loss=9.697, loss_att=8.054, loss_ctc=13.53
39epoch:train:6300-6400batch: loss=9.599, loss_att=7.895, loss_ctc=13.57
39epoch:train:6400-6500batch: loss=10.58, loss_att=8.702, loss_ctc=14.98
39epoch:train:6500-6600batch: loss=9.255, loss_att=7.664, loss_ctc=12.97
39epoch:train:6600-6700batch: loss=9.351, loss_att=7.787, loss_ctc=13.0
Save checkpoint to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-39.pt: epoch=39, learning_rate=0.0008714325118774557, objf=-0.6349669094436221, valid_objf=-6.059404802452316
write training info to exp-bpe-lrfactor10.0-conformer-512-8-noam/epoch-39-info
epoch 40, learning rate 0.000860188221958454
40epoch:train:0-0batch: loss=12.9, loss_att=10.63, loss_ctc=18.2
40epoch:train:0-100batch: loss=9.975, loss_att=8.147, loss_ctc=14.24
40epoch:train:100-200batch: loss=9.596, loss_att=7.836, loss_ctc=13.7
40epoch:train:200-300batch: loss=8.958, loss_att=7.276, loss_ctc=12.88
40epoch:train:300-400batch: loss=9.462, loss_att=7.779, loss_ctc=13.39
40epoch:train:400-500batch: loss=9.454, loss_att=7.719, loss_ctc=13.5
40epoch:train:500-600batch: loss=9.266, loss_att=7.585, loss_ctc=13.19
40epoch:train:600-700batch: loss=9.656, loss_att=7.923, loss_ctc=13.7
40epoch:train:700-800batch: loss=9.394, loss_att=7.717, loss_ctc=13.31
40epoch:train:800-900batch: loss=9.324, loss_att=7.658, loss_ctc=13.21
40epoch:train:900-1000batch: loss=9.152, loss_att=7.518, loss_ctc=12.97
Validation average objf: -5.917051 over 5505.0 utts
40epoch:train:1000-1100batch: loss=9.142, loss_att=7.387, loss_ctc=13.24
40epoch:train:1100-1200batch: loss=9.034, loss_att=7.374, loss_ctc=12.91
40epoch:train:1200-1300batch: loss=9.406, loss_att=7.718, loss_ctc=13.35
40epoch:train:1300-1400batch: loss=9.779, loss_att=7.996, loss_ctc=13.94
40epoch:train:1400-1500batch: loss=9.673, loss_att=7.912, loss_ctc=13.78
40epoch:train:1500-1600batch: loss=9.41, loss_att=7.714, loss_ctc=13.37
40epoch:train:1600-1700batch: loss=9.738, loss_att=7.952, loss_ctc=13.91
40epoch:train:1700-1800batch: loss=9.366, loss_att=7.697, loss_ctc=13.26
40epoch:train:1800-1900batch: loss=9.576, loss_att=7.92, loss_ctc=13.44
40epoch:train:1900-2000batch: loss=8.984, loss_att=7.414, loss_ctc=12.65
Validation average objf: -6.104845 over 5505.0 utts
